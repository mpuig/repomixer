This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<files>
This section contains the contents of the repository's files.

<file path="docs/ja/models/index.md">
---
search:
  exclude: true
---
# ãƒ¢ãƒ‡ãƒ«

Agents SDK ã«ã¯ã€æ¨™æº–ã§ 2 ç¨®é¡ã® OpenAI ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒ¼ãƒˆãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

- **æ¨å¥¨**: [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] â€” æ–°ã—ã„ [Responses API](https://platform.openai.com/docs/api-reference/responses) ã‚’åˆ©ç”¨ã—ã¦ OpenAI API ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚  
- [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] â€” [Chat Completions API](https://platform.openai.com/docs/api-reference/chat) ã‚’åˆ©ç”¨ã—ã¦ OpenAI API ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚

## ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›

1 ã¤ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å†…ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã”ã¨ã«ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ãŸã¨ãˆã°ã€æŒ¯ã‚Šåˆ†ã‘ã«ã¯å°ã•ãé«˜é€Ÿãªãƒ¢ãƒ‡ãƒ«ã‚’ã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã«ã¯å¤§ããé«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†ã€ã¨ã„ã£ãŸä½¿ã„åˆ†ã‘ã§ã™ã€‚[`Agent`][agents.Agent] ã‚’è¨­å®šã™ã‚‹éš›ã¯ã€ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã§ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã§ãã¾ã™ã€‚

1. OpenAI ãƒ¢ãƒ‡ãƒ«åã‚’ç›´æ¥æ¸¡ã™  
2. ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«åã¨ã€ãã‚Œã‚’ `Model` ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¸ãƒãƒƒãƒ”ãƒ³ã‚°ã§ãã‚‹ [`ModelProvider`][agents.models.interface.ModelProvider] ã‚’æ¸¡ã™  
3. [`Model`][agents.models.interface.Model] å®Ÿè£…ã‚’ç›´æ¥æ¸¡ã™  

!!!note
    SDK ã¯ [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] ã¨ [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] ã®ä¸¡æ–¹ã®å½¢ã«å¯¾å¿œã—ã¦ã„ã¾ã™ãŒã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã”ã¨ã« 1 ã¤ã®ãƒ¢ãƒ‡ãƒ«å½¢ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚2 ã¤ã®å½¢ã§ã¯ã‚µãƒãƒ¼ãƒˆã™ã‚‹æ©Ÿèƒ½ãƒ»ãƒ„ãƒ¼ãƒ«ãŒç•°ãªã‚‹ãŸã‚ã§ã™ã€‚ã©ã†ã—ã¦ã‚‚æ··åœ¨ã•ã›ã‚‹å ´åˆã¯ã€åˆ©ç”¨ã™ã‚‹ã™ã¹ã¦ã®æ©Ÿèƒ½ãŒä¸¡æ–¹ã§åˆ©ç”¨å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```python
from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
    model="o3-mini", # (1)!
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model=OpenAIChatCompletionsModel( # (2)!
        model="gpt-4o",
        openai_client=AsyncOpenAI()
    ),
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    model="gpt-3.5-turbo",
)

async def main():
    result = await Runner.run(triage_agent, input="Hola, Â¿cÃ³mo estÃ¡s?")
    print(result.final_output)
```

1. OpenAI ãƒ¢ãƒ‡ãƒ«åã‚’ç›´æ¥æŒ‡å®š  
2. [`Model`][agents.models.interface.Model] å®Ÿè£…ã‚’æä¾›  

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ã•ã‚‰ã«ç´°ã‹ãè¨­å®šã—ãŸã„å ´åˆã¯ã€`temperature` ãªã©ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã§ãã‚‹ [`ModelSettings`][agents.models.interface.ModelSettings] ã‚’æ¸¡ã—ã¾ã™ã€‚

```python
from agents import Agent, ModelSettings

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model="gpt-4o",
    model_settings=ModelSettings(temperature=0.1),
)
```

## ä»–ã® LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆ©ç”¨

ä»–ã® LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯ 3 é€šã‚Šã®æ–¹æ³•ã§åˆ©ç”¨ã§ãã¾ã™ï¼ˆã‚³ãƒ¼ãƒ‰ä¾‹ã¯ [ã“ã¡ã‚‰](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)ï¼‰ã€‚

1. [`set_default_openai_client`][agents.set_default_openai_client]  
   OpenAI äº’æ›ã® API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æŒã¤å ´åˆã«ã€`AsyncOpenAI` ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã« LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã—ã¦è¨­å®šã§ãã¾ã™ã€‚`base_url` ã¨ `api_key` ã‚’è¨­å®šã™ã‚‹ã‚±ãƒ¼ã‚¹ã§ã™ã€‚è¨­å®šä¾‹ã¯ [examples/model_providers/custom_example_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py)ã€‚  

2. [`ModelProvider`][agents.models.interface.ModelProvider]  
   `Runner.run` ãƒ¬ãƒ™ãƒ«ã§ã€Œã“ã®å®Ÿè¡Œä¸­ã®ã™ã¹ã¦ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ã†ã€ã¨å®£è¨€ã§ãã¾ã™ã€‚è¨­å®šä¾‹ã¯ [examples/model_providers/custom_example_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py)ã€‚  

3. [`Agent.model`][agents.agent.Agent.model]  
   ç‰¹å®šã® Agent ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã§ãã¾ã™ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã”ã¨ã«ç•°ãªã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ã‚‰ã‚Œã¾ã™ã€‚è¨­å®šä¾‹ã¯ [examples/model_providers/custom_example_agent.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py)ã€‚å¤šãã®ãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«ä½¿ã†æ–¹æ³•ã¨ã—ã¦ [LiteLLM é€£æº](./litellm.md) ãŒã‚ã‚Šã¾ã™ã€‚  

`platform.openai.com` ã® API ã‚­ãƒ¼ã‚’æŒãŸãªã„å ´åˆã¯ã€`set_tracing_disabled()` ã§ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ã™ã‚‹ã‹ã€[åˆ¥ã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼](../tracing.md) ã‚’è¨­å®šã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

!!! note
    ã“ã‚Œã‚‰ã®ä¾‹ã§ã¯ Chat Completions API/ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚å¤šãã® LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒã¾ã  Responses API ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ã§ã™ã€‚ã‚‚ã—ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒ Responses API ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹å ´åˆã¯ã€Responses ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

## ä»–ã® LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã‚ˆãã‚ã‚‹å•é¡Œ

### Tracing ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã® 401 ã‚¨ãƒ©ãƒ¼

ãƒˆãƒ¬ãƒ¼ã‚¹ã¯ OpenAI ã‚µãƒ¼ãƒãƒ¼ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ãŸã‚ã€OpenAI API ã‚­ãƒ¼ãŒãªã„å ´åˆã«ã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã™ã€‚è§£æ±ºç­–ã¯æ¬¡ã® 3 ã¤ã§ã™ã€‚

1. ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’å®Œå…¨ã«ç„¡åŠ¹åŒ–ã™ã‚‹: [`set_tracing_disabled(True)`][agents.set_tracing_disabled]  
2. ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ç”¨ã® OpenAI ã‚­ãƒ¼ã‚’è¨­å®šã™ã‚‹: [`set_tracing_export_api_key(...)`][agents.set_tracing_export_api_key]  
   ã“ã®ã‚­ãƒ¼ã¯ãƒˆãƒ¬ãƒ¼ã‚¹ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«ã®ã¿ä½¿ç”¨ã•ã‚Œã€[platform.openai.com](https://platform.openai.com/) ã®ã‚‚ã®ãŒå¿…è¦ã§ã™ã€‚  
3. OpenAI ä»¥å¤–ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’ä½¿ã†ã€‚è©³ã—ãã¯ [tracing ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](../tracing.md#custom-tracing-processors) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚  

### Responses API ã‚µãƒãƒ¼ãƒˆ

SDK ã¯æ—¢å®šã§ Responses API ã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€å¤šãã® LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯ã¾ã å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚ãã®ãŸã‚ 404 ãªã©ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚å¯¾å‡¦æ–¹æ³•ã¯ 2 ã¤ã§ã™ã€‚

1. [`set_default_openai_api("chat_completions")`][agents.set_default_openai_api] ã‚’å‘¼ã³å‡ºã™  
   ç’°å¢ƒå¤‰æ•° `OPENAI_API_KEY` ã¨ `OPENAI_BASE_URL` ã‚’è¨­å®šã—ã¦ã„ã‚‹å ´åˆã«æ©Ÿèƒ½ã—ã¾ã™ã€‚  
2. [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] ã‚’ä½¿ç”¨ã™ã‚‹  
   ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ [ã“ã¡ã‚‰](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/) ã«ã‚ã‚Šã¾ã™ã€‚  

### structured outputs ã®ã‚µãƒãƒ¼ãƒˆ

ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯ [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚ãã®å ´åˆã€æ¬¡ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

```
BadRequestError: Error code: 400 - {'error': {'message': "'response_format.type' : value is not one of the allowed values ['text','json_object']", 'type': 'invalid_request_error'}}
```

ã“ã‚Œã¯ä¸€éƒ¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆ¶é™ã§ã€JSON å‡ºåŠ›ã¯ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¦ã‚‚ `json_schema` ã‚’æŒ‡å®šã§ãã¾ã›ã‚“ã€‚ç¾åœ¨ä¿®æ­£ã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ãŒã€JSON ã‚¹ã‚­ãƒ¼ãƒå‡ºåŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚ãã†ã§ãªã„å ´åˆã€ä¸æ­£ãª JSON ã«ã‚ˆã‚Šã‚¢ãƒ—ãƒªãŒé »ç¹ã«å£Šã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
</file>

<file path="docs/ja/models/litellm.md">
---
search:
  exclude: true
---
# LiteLLM çµŒç”±ã§ã®ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨

!!! note

    LiteLLM ã¨ã®çµ±åˆã¯ç¾åœ¨ãƒ™ãƒ¼ã‚¿ç‰ˆã§ã™ã€‚ç‰¹ã«å°è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã¯å•é¡ŒãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å•é¡Œã‚’è¦‹ã¤ã‘ãŸå ´åˆã¯ã€[GitHub Issues](https://github.com/openai/openai-agents-python/issues) ã‹ã‚‰ã”å ±å‘Šãã ã•ã„ã€‚è¿…é€Ÿã«å¯¾å¿œã„ãŸã—ã¾ã™ã€‚

[LiteLLM](https://docs.litellm.ai/docs/) ã¯ã€1 ã¤ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ 100 ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã§ãã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚Agents SDK ã§ã¯ LiteLLM ã¨ã®çµ±åˆã«ã‚ˆã‚Šã€ä»»æ„ã® AI ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

`litellm` ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã® `litellm` ä¾å­˜é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã§å¯¾å¿œã§ãã¾ã™ã€‚

```bash
pip install "openai-agents[litellm]"
```

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€ä»»æ„ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ [`LitellmModel`][agents.extensions.models.litellm_model.LitellmModel] ã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚

## ä¾‹

ä»¥ä¸‹ã¯å‹•ä½œã™ã‚‹å®Œå…¨ãªã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚å®Ÿè¡Œã™ã‚‹ã¨ãƒ¢ãƒ‡ãƒ«åã¨ API ã‚­ãƒ¼ã®å…¥åŠ›ã‚’æ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚ä¾‹ãˆã°æ¬¡ã®ã‚ˆã†ã«å…¥åŠ›ã§ãã¾ã™ã€‚

-   `openai/gpt-4.1` ã‚’ãƒ¢ãƒ‡ãƒ«åã«ã€OpenAI API ã‚­ãƒ¼ã‚’å…¥åŠ›  
-   `anthropic/claude-3-5-sonnet-20240620` ã‚’ãƒ¢ãƒ‡ãƒ«åã«ã€Anthropic API ã‚­ãƒ¼ã‚’å…¥åŠ›  
-   ãã®ä»–

LiteLLM ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã®å…¨ãƒªã‚¹ãƒˆã¯ã€[litellm providers docs](https://docs.litellm.ai/docs/providers) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

```python
from __future__ import annotations

import asyncio

from agents import Agent, Runner, function_tool, set_tracing_disabled
from agents.extensions.models.litellm_model import LitellmModel

@function_tool
def get_weather(city: str):
    print(f"[debug] getting weather for {city}")
    return f"The weather in {city} is sunny."


async def main(model: str, api_key: str):
    agent = Agent(
        name="Assistant",
        instructions="You only respond in haikus.",
        model=LitellmModel(model=model, api_key=api_key),
        tools=[get_weather],
    )

    result = await Runner.run(agent, "What's the weather in Tokyo?")
    print(result.final_output)


if __name__ == "__main__":
    # First try to get model/api key from args
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=False)
    parser.add_argument("--api-key", type=str, required=False)
    args = parser.parse_args()

    model = args.model
    if not model:
        model = input("Enter a model name for Litellm: ")

    api_key = args.api_key
    if not api_key:
        api_key = input("Enter an API key for Litellm: ")

    asyncio.run(main(model, api_key))
```
</file>

<file path="docs/ja/voice/pipeline.md">
---
search:
  exclude: true
---
# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

[`VoicePipeline`][agents.voice.pipeline.VoicePipeline] ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’éŸ³å£°ã‚¢ãƒ—ãƒªã«ç°¡å˜ã«å¤‰æ›ã§ãã‚‹ã‚¯ãƒ©ã‚¹ã§ã™ã€‚ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ¸¡ã™ã¨ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒå…¥åŠ›éŸ³å£°ã®æ–‡å­—èµ·ã“ã—ã€éŸ³å£°çµ‚äº†ã®æ¤œçŸ¥ã€é©åˆ‡ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å‘¼ã³å‡ºã—ã€ãã—ã¦ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å‡ºåŠ›ã‚’éŸ³å£°ã¸å¤‰æ›ã™ã‚‹å‡¦ç†ã‚’æ‹…å½“ã—ã¾ã™ã€‚

```mermaid
graph LR
    %% Input
    A["ğŸ¤ Audio Input"]

    %% Voice Pipeline
    subgraph Voice_Pipeline [Voice Pipeline]
        direction TB
        B["Transcribe (speech-to-text)"]
        C["Your Code"]:::highlight
        D["Text-to-speech"]
        B --> C --> D
    end

    %% Output
    E["ğŸ§ Audio Output"]

    %% Flow
    A --> Voice_Pipeline
    Voice_Pipeline --> E

    %% Custom styling
    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;

```

## ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨­å®š

ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã™ã‚‹éš›ã«ã€ä»¥ä¸‹ã‚’è¨­å®šã§ãã¾ã™ã€‚

1. [`workflow`][agents.voice.workflow.VoiceWorkflowBase] â€ æ–°ã—ã„éŸ³å£°ãŒæ–‡å­—èµ·ã“ã—ã•ã‚Œã‚‹ãŸã³ã«å®Ÿè¡Œã•ã‚Œã‚‹ã‚³ãƒ¼ãƒ‰
2. ä½¿ç”¨ã™ã‚‹ [`speech-to-text`][agents.voice.model.STTModel] ãŠã‚ˆã³ [`text-to-speech`][agents.voice.model.TTSModel] ãƒ¢ãƒ‡ãƒ«
3. [`config`][agents.voice.pipeline_config.VoicePipelineConfig] â€ ä»¥ä¸‹ã®ã‚ˆã†ãªå†…å®¹ã‚’è¨­å®šå¯èƒ½
    - ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã€‚ãƒ¢ãƒ‡ãƒ«åã‚’ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã™
    - ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã€‚ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®ç„¡åŠ¹åŒ–ã€éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯å¦ã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åã€ãƒˆãƒ¬ãƒ¼ã‚¹ ID ãªã©
    - TTS ã¨ STT ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€è¨€èªã€ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‹ãªã©

## ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ

ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ [`run()`][agents.voice.pipeline.VoicePipeline.run] ãƒ¡ã‚½ãƒƒãƒ‰ã§å®Ÿè¡Œã§ãã¾ã™ã€‚éŸ³å£°å…¥åŠ›ã¯æ¬¡ã® 2 å½¢å¼ã§æ¸¡ã›ã¾ã™ã€‚

1. [`AudioInput`][agents.voice.input.AudioInput]  
   å®Œå…¨ãªéŸ³å£°ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒã‚ã‚‹å ´åˆã«ä½¿ç”¨ã—ã€ãã®çµæœã ã‘ã‚’ç”Ÿæˆã—ãŸã„ã¨ãã«ä¾¿åˆ©ã§ã™ã€‚è©±è€…ã®ç™ºè©±çµ‚äº†ã‚’æ¤œçŸ¥ã™ã‚‹å¿…è¦ãŒãªã„ã‚±ãƒ¼ã‚¹ã€ãŸã¨ãˆã°éŒ²éŸ³æ¸ˆã¿éŸ³å£°ã‚„ãƒ—ãƒƒã‚·ãƒ¥ãƒˆã‚¥ãƒ¼ãƒˆãƒ¼ã‚¯å‹ã‚¢ãƒ—ãƒªã®ã‚ˆã†ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©±ã—çµ‚ãˆãŸã‚¿ã‚¤ãƒŸãƒ³ã‚°ãŒæ˜ç¢ºãªå ´åˆã«å‘ã„ã¦ã„ã¾ã™ã€‚
2. [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]  
   ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè©±çµ‚äº†æ¤œçŸ¥ãŒå¿…è¦ãªå ´åˆã«ä½¿ç”¨ã—ã¾ã™ã€‚æ¤œå‡ºã•ã‚ŒãŸéŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’é †æ¬¡ãƒ—ãƒƒã‚·ãƒ¥ã§ãã€éŸ³å£°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒã€Œã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¤œçŸ¥ã€ã¨å‘¼ã°ã‚Œã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã‚’é€šã˜ã¦é©åˆ‡ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’è‡ªå‹•çš„ã«å®Ÿè¡Œã—ã¾ã™ã€‚

## çµæœ

éŸ³å£°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã®çµæœã¯ [`StreamedAudioResult`][agents.voice.result.StreamedAudioResult] ã§ã™ã€‚ã“ã‚Œã¯ç™ºç”Ÿã—ãŸã‚¤ãƒ™ãƒ³ãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å—ã‘å–ã‚Œã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚ã„ãã¤ã‹ã® [`VoiceStreamEvent`][agents.voice.events.VoiceStreamEvent] ãŒã‚ã‚Šã€ä¸»ãªã‚‚ã®ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚

1. [`VoiceStreamEventAudio`][agents.voice.events.VoiceStreamEventAudio] â€ éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’å«ã¿ã¾ã™
2. [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle] â€ ã‚¿ãƒ¼ãƒ³ã®é–‹å§‹ã‚„çµ‚äº†ãªã©ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆã‚’é€šçŸ¥ã—ã¾ã™
3. [`VoiceStreamEventError`][agents.voice.events.VoiceStreamEventError] â€ ã‚¨ãƒ©ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆã§ã™

```python

result = await pipeline.run(input)

async for event in result.stream():
    if event.type == "voice_stream_event_audio":
        # play audio
    elif event.type == "voice_stream_event_lifecycle":
        # lifecycle
    elif event.type == "voice_stream_event_error"
        # error
    ...
```

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### å‰²ã‚Šè¾¼ã¿

Agents SDK ã¯ç¾åœ¨ [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput] ã«å¯¾ã—ã¦ã€çµ„ã¿è¾¼ã¿ã®å‰²ã‚Šè¾¼ã¿å‡¦ç†ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚ãã®ãŸã‚ã€æ¤œå‡ºã•ã‚ŒãŸå„ã‚¿ãƒ¼ãƒ³ã”ã¨ã«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒå€‹åˆ¥ã«å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å†…ã§å‰²ã‚Šè¾¼ã¿ã‚’å‡¦ç†ã—ãŸã„å ´åˆã¯ã€[`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle] ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç›£è¦–ã§ãã¾ã™ã€‚`turn_started` ã¯æ–°ã—ã„ã‚¿ãƒ¼ãƒ³ãŒæ–‡å­—èµ·ã“ã—ã•ã‚Œã€å‡¦ç†ãŒé–‹å§‹ã•ã‚ŒãŸã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚`turn_ended` ã¯è©²å½“ã‚¿ãƒ¼ãƒ³ã®ã™ã¹ã¦ã®éŸ³å£°ãŒé€ä¿¡ã•ã‚ŒãŸå¾Œã«ãƒˆãƒªã‚¬ãƒ¼ã•ã‚Œã¾ã™ã€‚ãŸã¨ãˆã°ã€ãƒ¢ãƒ‡ãƒ«ãŒã‚¿ãƒ¼ãƒ³ã‚’é–‹å§‹ã—ãŸéš›ã«ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã®ãƒã‚¤ã‚¯ã‚’ãƒŸãƒ¥ãƒ¼ãƒˆã—ã€ãã®ã‚¿ãƒ¼ãƒ³ã«é–¢é€£ã™ã‚‹éŸ³å£°ã‚’ã™ã¹ã¦é€ä¿¡ã—çµ‚ãˆãŸå¾Œã«ã‚¢ãƒ³ãƒŸãƒ¥ãƒ¼ãƒˆã™ã‚‹ã¨ã„ã£ãŸåˆ¶å¾¡ã«ã€ã“ã‚Œã‚‰ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚
</file>

<file path="docs/ja/voice/quickstart.md">
---
search:
  exclude: true
---
# ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

## å‰ææ¡ä»¶

ã¾ãšã¯Â [ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆæ‰‹é †](../quickstart.md)Â ã«å¾“ã£ã¦Â Agents SDKÂ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã€ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚ãã®å¾Œã€SDKÂ ã®éŸ³å£°é–¢é€£ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™:

```bash
pip install 'openai-agents[voice]'
```

## ã‚³ãƒ³ã‚»ãƒ—ãƒˆ

æŠ¼ã•ãˆã¦ãŠãã¹ãä¸»ãªæ¦‚å¿µã¯ [`VoicePipeline`][agents.voice.pipeline.VoicePipeline] ã§ã™ã€‚ã“ã‚Œã¯æ¬¡ã® 3Â ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰æˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚

1. speech-to-text ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã¦éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚  
2. é€šå¸¸ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçš„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã‚ã‚‹ã‚ãªãŸã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€çµæœã‚’ç”Ÿæˆã—ã¾ã™ã€‚  
3. text-to-speech ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã¦çµæœã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å†ã³éŸ³å£°ã«å¤‰æ›ã—ã¾ã™ã€‚

```mermaid
graph LR
    %% Input
    A["ğŸ¤ Audio Input"]

    %% Voice Pipeline
    subgraph Voice_Pipeline [Voice Pipeline]
        direction TB
        B["Transcribe (speech-to-text)"]
        C["Your Code"]:::highlight
        D["Text-to-speech"]
        B --> C --> D
    end

    %% Output
    E["ğŸ§ Audio Output"]

    %% Flow
    A --> Voice_Pipeline
    Voice_Pipeline --> E

    %% Custom styling
    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;

```

## ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

ã¾ãšã€ã„ãã¤ã‹ã®Â ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆÂ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã—ã‚‡ã†ã€‚ã“ã®Â SDKÂ ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ãŸã“ã¨ãŒã‚ã‚Œã°ã€è¦‹è¦šãˆãŒã‚ã‚‹ã¯ãšã§ã™ã€‚ã“ã“ã§ã¯è¤‡æ•°ã®Â ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒãƒ³ãƒ‰ã‚ªãƒ•ã€ãã—ã¦ãƒ„ãƒ¼ãƒ«ã‚’ç”¨æ„ã—ã¾ã™ã€‚

```python
import asyncio
import random

from agents import (
    Agent,
    function_tool,
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions



@function_tool
def get_weather(city: str) -> str:
    """Get the weather for a given city."""
    print(f"[debug] get_weather called with city: {city}")
    choices = ["sunny", "cloudy", "rainy", "snowy"]
    return f"The weather in {city} is {random.choice(choices)}."


spanish_agent = Agent(
    name="Spanish",
    handoff_description="A spanish speaking agent.",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. Speak in Spanish.",
    ),
    model="gpt-4o-mini",
)

agent = Agent(
    name="Assistant",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.",
    ),
    model="gpt-4o-mini",
    handoffs=[spanish_agent],
    tools=[get_weather],
)
```

## éŸ³å£°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

[`SingleAgentVoiceWorkflow`][agents.voice.workflow.SingleAgentVoiceWorkflow] ã‚’ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ã—ã¦ã€ã‚·ãƒ³ãƒ—ãƒ«ãªéŸ³å£°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

```python
from agents.voice import SingleAgentVoiceWorkflow, VoicePipeline
pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))
```

## ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ

```python
import numpy as np
import sounddevice as sd
from agents.voice import AudioInput

# For simplicity, we'll just create 3 seconds of silence
# In reality, you'd get microphone data
buffer = np.zeros(24000 * 3, dtype=np.int16)
audio_input = AudioInput(buffer=buffer)

result = await pipeline.run(audio_input)

# Create an audio player using `sounddevice`
player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
player.start()

# Play the audio stream as it comes in
async for event in result.stream():
    if event.type == "voice_stream_event_audio":
        player.write(event.data)

```

## ã¾ã¨ã‚ã¦å®Ÿè¡Œ

```python
import asyncio
import random

import numpy as np
import sounddevice as sd

from agents import (
    Agent,
    function_tool,
    set_tracing_disabled,
)
from agents.voice import (
    AudioInput,
    SingleAgentVoiceWorkflow,
    VoicePipeline,
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions


@function_tool
def get_weather(city: str) -> str:
    """Get the weather for a given city."""
    print(f"[debug] get_weather called with city: {city}")
    choices = ["sunny", "cloudy", "rainy", "snowy"]
    return f"The weather in {city} is {random.choice(choices)}."


spanish_agent = Agent(
    name="Spanish",
    handoff_description="A spanish speaking agent.",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. Speak in Spanish.",
    ),
    model="gpt-4o-mini",
)

agent = Agent(
    name="Assistant",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.",
    ),
    model="gpt-4o-mini",
    handoffs=[spanish_agent],
    tools=[get_weather],
)


async def main():
    pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))
    buffer = np.zeros(24000 * 3, dtype=np.int16)
    audio_input = AudioInput(buffer=buffer)

    result = await pipeline.run(audio_input)

    # Create an audio player using `sounddevice`
    player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
    player.start()

    # Play the audio stream as it comes in
    async for event in result.stream():
        if event.type == "voice_stream_event_audio":
            player.write(event.data)


if __name__ == "__main__":
    asyncio.run(main())
```

ã“ã®ä¾‹ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚ãªãŸã«è©±ã—ã‹ã‘ã¾ã™ã€‚å®Ÿéš›ã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ä¼šè©±ã§ãã‚‹ãƒ‡ãƒ¢ã¯ã€[examples/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static) ã‚’ã”è¦§ãã ã•ã„ã€‚
</file>

<file path="docs/ja/voice/tracing.md">
---
search:
  exclude: true
---
# ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°](../tracing.md) ã¨åŒæ§˜ã«ã€éŸ³å£°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚‚è‡ªå‹•çš„ã«ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã•ã‚Œã¾ã™ã€‚

åŸºæœ¬çš„ãªãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°æƒ…å ±ã«ã¤ã„ã¦ã¯ä¸Šè¨˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ã•ã‚‰ã«ã€[`VoicePipelineConfig`][agents.voice.pipeline_config.VoicePipelineConfig] ã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°è¨­å®šã‚’è¡Œãˆã¾ã™ã€‚

ä¸»ãªãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°é–¢é€£ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚

-   [`tracing_disabled`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]ï¼šãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’ç„¡åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã¯æœ‰åŠ¹ã§ã™ã€‚  
-   [`trace_include_sensitive_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_data]ï¼šãƒˆãƒ¬ãƒ¼ã‚¹ã«éŸ³å£°ãƒ†ã‚­ã‚¹ãƒˆãªã©ã®æ©Ÿå¾®ãªãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚ã“ã‚Œã¯éŸ³å£°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å°‚ç”¨ã§ã‚ã‚Šã€Workflow å†…éƒ¨ã§ç™ºç”Ÿã™ã‚‹å‡¦ç†ã«ã¯å½±éŸ¿ã—ã¾ã›ã‚“ã€‚  
-   [`trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data]ï¼šãƒˆãƒ¬ãƒ¼ã‚¹ã«éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚  
-   [`workflow_name`][agents.voice.pipeline_config.VoicePipelineConfig.workflow_name]ï¼šãƒˆãƒ¬ãƒ¼ã‚¹ Workflow ã®åå‰ã§ã™ã€‚  
-   [`group_id`][agents.voice.pipeline_config.VoicePipelineConfig.group_id]ï¼šè¤‡æ•°ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’é–¢é€£ä»˜ã‘ã‚‹ãŸã‚ã® `group_id` ã§ã™ã€‚  
-   [`trace_metadata`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]ï¼šãƒˆãƒ¬ãƒ¼ã‚¹ã«è¿½åŠ ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
</file>

<file path="docs/ja/agents.md">
---
search:
  exclude: true
---
# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã‚¢ãƒ—ãƒªã®ä¸»è¦ãªæ§‹æˆãƒ–ãƒ­ãƒƒã‚¯ã§ã™ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« ( LLM ) ã« instructions ã¨ tools ã‚’è¨­å®šã—ãŸã‚‚ã®ã§ã™ã€‚

## åŸºæœ¬è¨­å®š

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§æœ€ã‚‚ä¸€èˆ¬çš„ã«è¨­å®šã™ã‚‹ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚

-   `instructions`: é–‹ç™ºè€…ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¾ãŸã¯ system prompt ã¨ã‚‚å‘¼ã°ã‚Œã¾ã™ã€‚
-   `model`: ä½¿ç”¨ã™ã‚‹ LLM ã¨ã€temperature ã‚„ top_p ãªã©ã®ãƒ¢ãƒ‡ãƒ«èª¿æ•´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’æŒ‡å®šã™ã‚‹ä»»æ„ã® `model_settings`ã€‚
-   `tools`: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚¿ã‚¹ã‚¯ã‚’é”æˆã™ã‚‹ãŸã‚ã«åˆ©ç”¨ã§ãã‚‹ãƒ„ãƒ¼ãƒ«ã€‚

```python
from agents import Agent, ModelSettings, function_tool

@function_tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Haiku agent",
    instructions="Always respond in haiku form",
    model="o3-mini",
    tools=[get_weather],
)
```

## ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ãã® `context` å‹ã«ã¤ã„ã¦æ±ç”¨çš„ã§ã™ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ä¾å­˜æ€§æ³¨å…¥ã®æ‰‹æ®µã§ã€`Runner.run()` ã«æ¸¡ã™ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚ã“ã‚Œã¯ã™ã¹ã¦ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒ„ãƒ¼ãƒ«ã€ãƒãƒ³ãƒ‰ã‚ªãƒ•ãªã©ã«æ¸¡ã•ã‚Œã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œæ™‚ã®ä¾å­˜é–¢ä¿‚ã‚„çŠ¶æ…‹ã‚’ã¾ã¨ã‚ã¦ä¿æŒã—ã¾ã™ã€‚ä»»æ„ã® Python ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æ¸¡ã›ã¾ã™ã€‚

```python
@dataclass
class UserContext:
    uid: str
    is_pro_user: bool

    async def fetch_purchases() -> list[Purchase]:
        return ...

agent = Agent[UserContext](
    ...,
)
```

## å‡ºåŠ›ã‚¿ã‚¤ãƒ—

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ ( ã¤ã¾ã‚Š `str` ) ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚ç‰¹å®šã®å‹ã§å‡ºåŠ›ã•ã›ãŸã„å ´åˆã¯ `output_type` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä¸€èˆ¬çš„ã«ã¯ [Pydantic](https://docs.pydantic.dev/) ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆ©ç”¨ã—ã¾ã™ãŒã€Pydantic ã® [TypeAdapter](https://docs.pydantic.dev/latest/api/type_adapter/) ã§ãƒ©ãƒƒãƒ—å¯èƒ½ãªå‹ã§ã‚ã‚Œã°ä½•ã§ã‚‚å¯¾å¿œã—ã¾ã™ã€‚ãŸã¨ãˆã° dataclassã€listã€TypedDict ãªã©ã§ã™ã€‚

```python
from pydantic import BaseModel
from agents import Agent


class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

agent = Agent(
    name="Calendar extractor",
    instructions="Extract calendar events from text",
    output_type=CalendarEvent,
)
```

!!! note

    `output_type` ã‚’æ¸¡ã™ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯é€šå¸¸ã®ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã®ä»£ã‚ã‚Šã« [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†æŒ‡ç¤ºã•ã‚Œã¾ã™ã€‚

## ãƒãƒ³ãƒ‰ã‚ªãƒ•

ãƒãƒ³ãƒ‰ã‚ªãƒ•ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå§”è­²ã§ãã‚‹ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚ãƒãƒ³ãƒ‰ã‚ªãƒ•ã®ãƒªã‚¹ãƒˆã‚’æ¸¡ã—ã¦ãŠãã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å¿…è¦ã«å¿œã˜ã¦ãã‚Œã‚‰ã«å‡¦ç†ã‚’å§”è­²ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å˜ä¸€ã®ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç·¨æˆã§ãã‚‹å¼·åŠ›ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå®Ÿç¾ã—ã¾ã™ã€‚è©³ç´°ã¯ [handoffs](handoffs.md) ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã”è¦§ãã ã•ã„ã€‚

```python
from agents import Agent

booking_agent = Agent(...)
refund_agent = Agent(...)

triage_agent = Agent(
    name="Triage agent",
    instructions=(
        "Help the user with their questions."
        "If they ask about booking, handoff to the booking agent."
        "If they ask about refunds, handoff to the refund agent."
    ),
    handoffs=[booking_agent, refund_agent],
)
```

## å‹•çš„ instructions

é€šå¸¸ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆæ™‚ã« instructions ã‚’æŒ‡å®šã—ã¾ã™ãŒã€é–¢æ•°ã‚’ä»‹ã—ã¦å‹•çš„ã« instructions ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ãã®é–¢æ•°ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿”ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚åŒæœŸé–¢æ•°ã¨ `async` é–¢æ•°ã®ä¸¡æ–¹ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

```python
def dynamic_instructions(
    context: RunContextWrapper[UserContext], agent: Agent[UserContext]
) -> str:
    return f"The user's name is {context.context.name}. Help them with their questions."


agent = Agent[UserContext](
    name="Triage agent",
    instructions=dynamic_instructions,
)
```

## ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆ (hooks)

å ´åˆã«ã‚ˆã£ã¦ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚’è¦³å¯Ÿã—ãŸã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ãŸã¨ãˆã°ã€ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã—ãŸã‚Šã€ç‰¹å®šã®ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿæ™‚ã«ãƒ‡ãƒ¼ã‚¿ã‚’äº‹å‰å–å¾—ã—ãŸã‚Šã™ã‚‹å ´åˆã§ã™ã€‚`hooks` ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ä½¿ã£ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã«ãƒ•ãƒƒã‚¯ã§ãã¾ã™ã€‚[`AgentHooks`][agents.lifecycle.AgentHooks] ã‚¯ãƒ©ã‚¹ã‚’ã‚µãƒ–ã‚¯ãƒ©ã‚¹åŒ–ã—ã€é–¢å¿ƒã®ã‚ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã—ã¦ãã ã•ã„ã€‚

## ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«

ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã‚’ä½¿ã†ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œã¨ä¸¦è¡Œã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¯¾ã™ã‚‹ãƒã‚§ãƒƒã‚¯ã‚„ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚ãŸã¨ãˆã°ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›å†…å®¹ãŒé–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚’ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã¾ã™ã€‚è©³ç´°ã¯ [guardrails](guardrails.md) ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã”è¦§ãã ã•ã„ã€‚

## ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¤‡è£½

`clone()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¤‡è£½ã—ã€å¿…è¦ã«å¿œã˜ã¦ä»»æ„ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’å¤‰æ›´ã§ãã¾ã™ã€‚

```python
pirate_agent = Agent(
    name="Pirate",
    instructions="Write like a pirate",
    model="o3-mini",
)

robot_agent = pirate_agent.clone(
    name="Robot",
    instructions="Write like a robot",
)
```

## ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã®å¼·åˆ¶

ãƒ„ãƒ¼ãƒ«ã®ä¸€è¦§ã‚’æ¸¡ã—ã¦ã‚‚ã€LLM ãŒå¿…ãšãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚[`ModelSettings.tool_choice`][agents.model_settings.ModelSettings.tool_choice] ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚’å¼·åˆ¶ã§ãã¾ã™ã€‚æœ‰åŠ¹ãªå€¤ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚

1. `auto` â€” ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã‚’ LLM ãŒåˆ¤æ–­ã—ã¾ã™ã€‚
2. `required` â€” LLM ã«ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚’å¿…é ˆåŒ–ã—ã¾ã™ ( ãŸã ã—ä½¿ç”¨ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã¯è‡ªå‹•é¸æŠ )ã€‚
3. `none` â€” LLM ã«ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ãªã„ã“ã¨ã‚’è¦æ±‚ã—ã¾ã™ã€‚
4. ç‰¹å®šã®æ–‡å­—åˆ— ( ä¾‹: `my_tool` ) â€” ãã®ç‰¹å®šã®ãƒ„ãƒ¼ãƒ«ã‚’ LLM ã«ä½¿ç”¨ã•ã›ã¾ã™ã€‚

!!! note

    ç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é˜²ããŸã‚ã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å¾Œã« `tool_choice` ã‚’è‡ªå‹•çš„ã« "auto" ã«ãƒªã‚»ãƒƒãƒˆã—ã¾ã™ã€‚ã“ã®å‹•ä½œã¯ [`agent.reset_tool_choice`][agents.agent.Agent.reset_tool_choice] ã§è¨­å®šã§ãã¾ã™ã€‚ç„¡é™ãƒ«ãƒ¼ãƒ—ãŒèµ·ã“ã‚‹ç†ç”±ã¯ã€ãƒ„ãƒ¼ãƒ«ã®çµæœãŒ LLM ã«é€ã‚‰ã‚Œã€`tool_choice` ã«ã‚ˆã‚Šå†ã³ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒç”Ÿæˆã•ã‚Œã‚‹ã€ã¨ã„ã†æµã‚ŒãŒç¹°ã‚Šè¿”ã•ã‚Œã‚‹ã‹ã‚‰ã§ã™ã€‚

    ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å¾Œã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Œå…¨ã«åœæ­¢ã•ã›ãŸã„å ´åˆ ( auto ãƒ¢ãƒ¼ãƒ‰ã§ç¶šè¡Œã•ã›ãŸããªã„å ´åˆ ) ã¯ã€[`Agent.tool_use_behavior="stop_on_first_tool"`] ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ„ãƒ¼ãƒ«ã®å‡ºåŠ›ã‚’ LL M ã®è¿½åŠ å‡¦ç†ãªã—ã«ãã®ã¾ã¾æœ€çµ‚å¿œç­”ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚
</file>

<file path="docs/ja/config.md">
---
search:
  exclude: true
---
# SDK ã®è¨­å®š

## API ã‚­ãƒ¼ã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ SDK ã¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸæ™‚ç‚¹ã§ LLM ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹ `OPENAI_API_KEY` ç’°å¢ƒå¤‰æ•°ã‚’æ¢ã—ã¾ã™ã€‚ã‚¢ãƒ—ãƒªèµ·å‹•å‰ã«ã“ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã§ããªã„å ´åˆã¯ã€ [set_default_openai_key()][agents.set_default_openai_key] é–¢æ•°ã‚’åˆ©ç”¨ã—ã¦ã‚­ãƒ¼ã‚’è¨­å®šã§ãã¾ã™ã€‚

```python
from agents import set_default_openai_key

set_default_openai_key("sk-...")
```

ã¾ãŸã€ä½¿ç”¨ã™ã‚‹ OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’æ§‹æˆã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ SDK ã¯ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ä¸Šè¨˜ã§è¨­å®šã—ãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ¼ã‚’ç”¨ã„ã¦ `AsyncOpenAI` ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚ã“ã‚Œã‚’å¤‰æ›´ã™ã‚‹ã«ã¯ã€ [set_default_openai_client()][agents.set_default_openai_client] é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```python
from openai import AsyncOpenAI
from agents import set_default_openai_client

custom_client = AsyncOpenAI(base_url="...", api_key="...")
set_default_openai_client(custom_client)
```

ã•ã‚‰ã«ã€ä½¿ç”¨ã™ã‚‹ OpenAI API ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚æ—¢å®šã§ã¯ OpenAI Responses API ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã‚’ Chat Completions API ã«å¤‰æ›´ã™ã‚‹ã«ã¯ã€ [set_default_openai_api()][agents.set_default_openai_api] é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

```python
from agents import set_default_openai_api

set_default_openai_api("chat_completions")
```

## ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹ã«ãªã£ã¦ã„ã¾ã™ã€‚å‰è¿°ã® OpenAI API ã‚­ãƒ¼ï¼ˆç’°å¢ƒå¤‰æ•°ã¾ãŸã¯è¨­å®šã—ãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ¼ï¼‰ãŒè‡ªå‹•çš„ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã§ä½¿ç”¨ã™ã‚‹ API ã‚­ãƒ¼ã‚’å€‹åˆ¥ã«è¨­å®šã—ãŸã„å ´åˆã¯ã€ [`set_tracing_export_api_key`][agents.set_tracing_export_api_key] é–¢æ•°ã‚’åˆ©ç”¨ã—ã¦ãã ã•ã„ã€‚

```python
from agents import set_tracing_export_api_key

set_tracing_export_api_key("sk-...")
```

ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’å®Œå…¨ã«ç„¡åŠ¹åŒ–ã™ã‚‹ã«ã¯ã€ [`set_tracing_disabled()`][agents.set_tracing_disabled] é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚

```python
from agents import set_tracing_disabled

set_tracing_disabled(True)
```

## ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°

 SDK ã«ã¯ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ Python ãƒ­ã‚¬ãƒ¼ãŒ 2 ã¤ã‚ã‚Šã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€è­¦å‘Šã¨ã‚¨ãƒ©ãƒ¼ã¯ `stdout` ã«å‡ºåŠ›ã•ã‚Œã¾ã™ãŒã€ãã‚Œä»¥å¤–ã®ãƒ­ã‚°ã¯æŠ‘åˆ¶ã•ã‚Œã¾ã™ã€‚

è©³ç´°ãªãƒ­ã‚°ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€ [`enable_verbose_stdout_logging()`][agents.enable_verbose_stdout_logging] é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```python
from agents import enable_verbose_stdout_logging

enable_verbose_stdout_logging()
```

å¿…è¦ã«å¿œã˜ã¦ã€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ãªã©ã‚’è¿½åŠ ã—ã¦ãƒ­ã‚°ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚è©³ã—ãã¯ [Python ãƒ­ã‚®ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰](https://docs.python.org/3/howto/logging.html) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

```python
import logging

logger = logging.getLogger("openai.agents") # or openai.agents.tracing for the Tracing logger

# To make all logs show up
logger.setLevel(logging.DEBUG)
# To make info and above show up
logger.setLevel(logging.INFO)
# To make warning and above show up
logger.setLevel(logging.WARNING)
# etc

# You can customize this as needed, but this will output to `stderr` by default
logger.addHandler(logging.StreamHandler())
```

### ãƒ­ã‚°ã«å«ã¾ã‚Œã‚‹æ©Ÿå¾®æƒ…å ±

ç‰¹å®šã®ãƒ­ã‚°ã«ã¯æ©Ÿå¾®æƒ…å ±ï¼ˆãŸã¨ãˆã° ãƒ¦ãƒ¼ã‚¶ãƒ¼ ãƒ‡ãƒ¼ã‚¿ï¼‰ãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ã“ã®æƒ…å ±ãŒè¨˜éŒ²ã•ã‚Œã‚‹ã®ã‚’é˜²ããŸã„å ´åˆã¯ã€æ¬¡ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚

LLM ã®å…¥åŠ›ãŠã‚ˆã³å‡ºåŠ›ã®ãƒ­ã‚°ã‚’ç„¡åŠ¹ã«ã™ã‚‹:

```bash
export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1
```

ãƒ„ãƒ¼ãƒ«ã®å…¥åŠ›ãŠã‚ˆã³å‡ºåŠ›ã®ãƒ­ã‚°ã‚’ç„¡åŠ¹ã«ã™ã‚‹:

```bash
export OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1
```
</file>

<file path="docs/ja/context.md">
---
search:
  exclude: true
---
# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã„ã†è¨€è‘‰ã«ã¯è¤‡æ•°ã®æ„å‘³ãŒã‚ã‚Šã¾ã™ã€‚ã“ã“ã§ã¯ä¸»ã« 2 ã¤ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

1. ã‚³ãƒ¼ãƒ‰å†…ã§ãƒ­ãƒ¼ã‚«ãƒ«ã«åˆ©ç”¨ã§ãã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: ãƒ„ãƒ¼ãƒ«é–¢æ•°ã®å®Ÿè¡Œæ™‚ã‚„ `on_handoff` ãªã©ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒ•ãƒƒã‚¯ã§å¿…è¦ã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚„ä¾å­˜é–¢ä¿‚ã§ã™ã€‚  
2. LLM ãŒå‚ç…§ã§ãã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: LLM ãŒãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹éš›ã«è¦‹ãˆã‚‹ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚

## ãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

ãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ [`RunContextWrapper`][agents.run_context.RunContextWrapper] ã‚¯ãƒ©ã‚¹ã¨ã€ãã®ä¸­ã® [`context`][agents.run_context.RunContextWrapper.context] ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã§è¡¨ç¾ã•ã‚Œã¾ã™ã€‚ä»•çµ„ã¿ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚

1. ä»»æ„ã® Python ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚ä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã—ã¦ dataclass ã‚„ Pydantic ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚  
2. ãã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å„ç¨® run ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆä¾‹: `Runner.run(..., **context=whatever** )`ï¼‰ã«æ¸¡ã—ã¾ã™ã€‚  
3. ã™ã¹ã¦ã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚„ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒ•ãƒƒã‚¯ã«ã¯ã€ãƒ©ãƒƒãƒ‘ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ `RunContextWrapper[T]` ãŒæ¸¡ã•ã‚Œã¾ã™ã€‚ã“ã“ã§ `T` ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å‹ã§ã€`wrapper.context` ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚

**æœ€é‡è¦ãƒã‚¤ãƒ³ãƒˆ**: ã‚ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œã«ãŠã„ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒ„ãƒ¼ãƒ«é–¢æ•°ãƒ»ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒ•ãƒƒã‚¯ãªã©ã¯ã™ã¹ã¦åŒã˜ _å‹_ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã¯æ¬¡ã®ã‚ˆã†ãªç”¨é€”ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚

-   å®Ÿè¡Œã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹: ãƒ¦ãƒ¼ã‚¶ãƒ¼å / uid ã‚„ãã®ä»–ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ï¼‰
-   ä¾å­˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆä¾‹: ãƒ­ã‚¬ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ãªã©ï¼‰
-   ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°

!!! danger "Note"

    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ LLM ã«ã¯é€ä¿¡ã•ã‚Œã¾ã›ã‚“ã€‚ã‚ãã¾ã§ãƒ­ãƒ¼ã‚«ãƒ«ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã‚ã‚Šã€èª­ã¿æ›¸ãã‚„ãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—ãŒå¯èƒ½ã§ã™ã€‚

```python
import asyncio
from dataclasses import dataclass

from agents import Agent, RunContextWrapper, Runner, function_tool

@dataclass
class UserInfo:  # (1)!
    name: str
    uid: int

@function_tool
async def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str:  # (2)!
    return f"User {wrapper.context.name} is 47 years old"

async def main():
    user_info = UserInfo(name="John", uid=123)

    agent = Agent[UserInfo](  # (3)!
        name="Assistant",
        tools=[fetch_user_age],
    )

    result = await Runner.run(  # (4)!
        starting_agent=agent,
        input="What is the age of the user?",
        context=user_info,
    )

    print(result.final_output)  # (5)!
    # The user John is 47 years old.

if __name__ == "__main__":
    asyncio.run(main())
```

1. ã“ã‚ŒãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚ã“ã“ã§ã¯ dataclass ã‚’ä½¿ã£ã¦ã„ã¾ã™ãŒã€ä»»æ„ã®å‹ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚  
2. ã“ã‚Œã¯ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚`RunContextWrapper[UserInfo]` ã‚’å—ã‘å–ã‚Šã€å®Ÿè£…å†…ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã—ã¦ã„ã¾ã™ã€‚  
3. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚¸ã‚§ãƒãƒªãƒƒã‚¯ `UserInfo` ã‚’ä»˜ä¸ã™ã‚‹ã“ã¨ã§ã€å‹ãƒã‚§ãƒƒã‚«ãƒ¼ãŒèª¤ã‚Šã‚’æ¤œå‡ºã§ãã¾ã™ï¼ˆãŸã¨ãˆã°åˆ¥ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå‹ã‚’å—ã‘å–ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’æ¸¡ã—ãŸå ´åˆãªã©ï¼‰ã€‚  
4. `run` é–¢æ•°ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã—ã¾ã™ã€‚  
5. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ãƒ„ãƒ¼ãƒ«ã‚’æ­£ã—ãå‘¼ã³å‡ºã—ã€å¹´é½¢ã‚’å–å¾—ã—ã¾ã™ã€‚  

## ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ / LLM ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

LLM ãŒå‘¼ã³å‡ºã•ã‚Œã‚‹ã¨ãã€LLM ãŒå‚ç…§ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ä¼šè©±å±¥æ­´ã«å«ã¾ã‚Œã‚‹ã‚‚ã®ã ã‘ã§ã™ã€‚ã—ãŸãŒã£ã¦ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ LLM ã«æ¸¡ã—ãŸã„å ´åˆã¯ã€ãã®ãƒ‡ãƒ¼ã‚¿ã‚’å±¥æ­´ã«å«ã‚ã‚‹å½¢ã§æä¾›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æ–¹æ³•ã¯ã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚

1. Agent ã® `instructions` ã«è¿½åŠ ã™ã‚‹ã€‚ã„ã‚ã‚†ã‚‹ã€Œsystem promptã€ã‚„ã€Œdeveloper messageã€ã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã§ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯é™çš„ãªæ–‡å­—åˆ—ã§ã‚‚ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å—ã‘å–ã£ã¦æ–‡å­—åˆ—ã‚’è¿”ã™å‹•çš„ãªé–¢æ•°ã§ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚„ç¾åœ¨ã®æ—¥ä»˜ãªã©ã€å¸¸ã«æœ‰ç”¨ãªæƒ…å ±ã‚’æ¸¡ã™éš›ã«ã‚ˆãä½¿ã‚ã‚Œã¾ã™ã€‚  
2. `Runner.run` å‘¼ã³å‡ºã—æ™‚ã® `input` ã«è¿½åŠ ã™ã‚‹ã€‚`instructions` ã¨ä¼¼ã¦ã„ã¾ã™ãŒã€[chain of command](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command) ã®ä¸‹ä½ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é…ç½®ã§ãã¾ã™ã€‚  
3. é–¢æ•°ãƒ„ãƒ¼ãƒ«çµŒç”±ã§å…¬é–‹ã™ã‚‹ã€‚ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã§å–å¾—ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«é©ã—ã¦ãŠã‚Šã€LLM ãŒå¿…è¦ã«å¿œã˜ã¦ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ã€‚  
4. retrieval ã‚„ web search ã‚’ä½¿ã†ã€‚ã“ã‚Œã‚‰ã¯ç‰¹åˆ¥ãªãƒ„ãƒ¼ãƒ«ã§ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ï¼ˆretrievalï¼‰ã€ã‚‚ã—ãã¯ Web ã‹ã‚‰å–å¾—ã™ã‚‹ï¼ˆweb searchï¼‰ã“ã¨ãŒã§ãã¾ã™ã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’é–¢é€£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã€Œã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã€ã™ã‚‹ã®ã«æœ‰åŠ¹ã§ã™ã€‚
</file>

<file path="docs/ja/examples.md">
---
search:
  exclude: true
---
# ã‚³ãƒ¼ãƒ‰ä¾‹

ãƒªãƒã‚¸ãƒˆãƒªã® [examples ã‚»ã‚¯ã‚·ãƒ§ãƒ³](https://github.com/openai/openai-agents-python/tree/main/examples) ã«ã¯ã€ SDK ã®ã•ã¾ã–ã¾ãªã‚µãƒ³ãƒ—ãƒ«å®Ÿè£…ãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ä¾‹ã¯ã€ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„æ©Ÿèƒ½ã‚’ç¤ºã™è¤‡æ•°ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«æ•´ç†ã•ã‚Œã¦ã„ã¾ã™ã€‚


## ã‚«ãƒ†ã‚´ãƒªãƒ¼

- **[agent_patterns](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns):**  
  ã“ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ä¾‹ã§ã¯ã€ä¸€èˆ¬çš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚

    - æ±ºå®šè«–çš„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼  
    - ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ  
    - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä¸¦åˆ—å®Ÿè¡Œ  

- **[basic](https://github.com/openai/openai-agents-python/tree/main/examples/basic):**  
  SDK ã®åŸºç¤çš„ãªæ©Ÿèƒ½ã‚’ç¤ºã™ä¾‹ã§ã™ã€‚

    - å‹•çš„ãªã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ  
    - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›  
    - ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆ  

- **[tool examples](https://github.com/openai/openai-agents-python/tree/main/examples/tools):**  
  Web æ¤œç´¢ã‚„ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ãªã©ã€ OpenAI ãŒãƒ›ã‚¹ãƒˆã™ã‚‹ãƒ„ãƒ¼ãƒ«ã®å®Ÿè£…æ–¹æ³•ã¨ã€ãã‚Œã‚‰ã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«çµ±åˆã™ã‚‹æ–¹æ³•ã‚’å­¦ã¹ã¾ã™ã€‚

- **[model providers](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers):**  
  OpenAI ä»¥å¤–ã®ãƒ¢ãƒ‡ãƒ«ã‚’ SDK ã§åˆ©ç”¨ã™ã‚‹æ–¹æ³•ã‚’æ¢ã‚Šã¾ã™ã€‚

- **[handoffs](https://github.com/openai/openai-agents-python/tree/main/examples/handoffs):**  
  ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒãƒ³ãƒ‰ã‚ªãƒ•ã‚’å®Ÿè·µçš„ã«ç¤ºã™ä¾‹ã§ã™ã€‚

- **[mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp):**  
  MCP ã‚’ä½¿ã£ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ§‹ç¯‰æ–¹æ³•ã‚’å­¦ã¹ã¾ã™ã€‚

- **[customer_service](https://github.com/openai/openai-agents-python/tree/main/examples/customer_service)** ã¨ **[research_bot](https://github.com/openai/openai-agents-python/tree/main/examples/research_bot):**  
  ã‚ˆã‚Šå®Ÿè·µçš„ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã‚’ç¤ºã™ã€æ‹¡å¼µã•ã‚ŒãŸ 2 ã¤ã®ä¾‹ã§ã™ã€‚

    - **customer_service**: èˆªç©ºä¼šç¤¾å‘ã‘ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒ¼ãƒ“ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã®ä¾‹  
    - **research_bot**: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ã‚£ãƒ¼ãƒ—ãƒªã‚µãƒ¼ãƒã‚¯ãƒ­ãƒ¼ãƒ³  

- **[voice](https://github.com/openai/openai-agents-python/tree/main/examples/voice):**  
  TTS ã¨ STT ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸéŸ³å£°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä¾‹ã‚’ã”è¦§ãã ã•ã„ã€‚
</file>

<file path="docs/ja/guardrails.md">
---
search:
  exclude: true
---
# ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«

ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã¨ _ä¸¦åˆ—_ ã«å®Ÿè¡Œã•ã‚Œã€ ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ› ã®ãƒã‚§ãƒƒã‚¯ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã¾ã™ã€‚ãŸã¨ãˆã°ã€é¡§å®¢ã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã«éå¸¸ã«è³¢ã„ (ãã®ãŸã‚é…ã / é«˜ä¾¡ãª) ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚ã‚‹ã¨ã—ã¾ã™ã€‚æ‚ªæ„ã®ã‚ã‚‹ ãƒ¦ãƒ¼ã‚¶ãƒ¼ ãŒãƒ¢ãƒ‡ãƒ«ã«æ•°å­¦ã®å®¿é¡Œã‚’æ‰‹ä¼ã‚ã›ã‚ˆã†ã¨ã™ã‚‹ã®ã¯é¿ã‘ãŸã„ã§ã™ã‚ˆã­ã€‚ãã®å ´åˆã€ é«˜é€Ÿ / ä½ã‚³ã‚¹ãƒˆ ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ãŒæ‚ªæ„ã®ã‚ã‚‹åˆ©ç”¨ã‚’æ¤œçŸ¥ã—ãŸå ´åˆã€å³åº§ã«ã‚¨ãƒ©ãƒ¼ã‚’é€å‡ºã—ã¦é«˜ä¾¡ãªãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œã‚’åœæ­¢ã—ã€æ™‚é–“ã¨è²»ç”¨ã‚’ç¯€ç´„ã§ãã¾ã™ã€‚

ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã«ã¯ 2 ç¨®é¡ã‚ã‚Šã¾ã™ã€‚

1. Input ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯æœ€åˆã® ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ› ã«å¯¾ã—ã¦å®Ÿè¡Œã•ã‚Œã¾ã™  
2. Output ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯æœ€çµ‚çš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‡ºåŠ›ã«å¯¾ã—ã¦å®Ÿè¡Œã•ã‚Œã¾ã™  

## Input ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«

Input ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯ 3 ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

1. ã¾ãšã€ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«æ¸¡ã•ã‚ŒãŸã‚‚ã®ã¨åŒã˜å…¥åŠ›ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚  
2. æ¬¡ã«ã€ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•°ãŒå®Ÿè¡Œã•ã‚Œ [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput] ã‚’ç”Ÿæˆã—ã€ãã‚ŒãŒ [`InputGuardrailResult`][agents.guardrail.InputGuardrailResult] ã§ãƒ©ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚  
3. æœ€å¾Œã« [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] ãŒ true ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚true ã®å ´åˆã€[`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered] ä¾‹å¤–ãŒé€å‡ºã•ã‚Œã‚‹ã®ã§ã€ ãƒ¦ãƒ¼ã‚¶ãƒ¼ ã¸ã®é©åˆ‡ãªå¿œç­”ã‚„ä¾‹å¤–å‡¦ç†ã‚’è¡Œãˆã¾ã™ã€‚  

!!! Note

    Input ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯ ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ› ã«å¯¾ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã‚‹ãŸã‚ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã®ã¯ãã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ *æœ€åˆ* ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã‚ã‚‹å ´åˆã ã‘ã§ã™ã€‚ã€Œãªãœ `guardrails` ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ã‚Šã€ `Runner.run` ã«æ¸¡ã•ãªã„ã®ã‹ï¼Ÿã€ã¨æ€ã†ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯å®Ÿéš›ã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã«å¯†æ¥ã«é–¢é€£ã™ã‚‹å ´åˆãŒå¤šãã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã”ã¨ã«ç•°ãªã‚‹ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã€ã‚³ãƒ¼ãƒ‰ã‚’åŒã˜å ´æ‰€ã«ç½®ãã“ã¨ã§å¯èª­æ€§ãŒå‘ä¸Šã™ã‚‹ã‹ã‚‰ã§ã™ã€‚

## Output ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«

Output ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯ 3 ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

1. ã¾ãšã€ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«æ¸¡ã•ã‚ŒãŸã‚‚ã®ã¨åŒã˜å…¥åŠ›ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚  
2. æ¬¡ã«ã€ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•°ãŒå®Ÿè¡Œã•ã‚Œ [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput] ã‚’ç”Ÿæˆã—ã€ãã‚ŒãŒ [`OutputGuardrailResult`][agents.guardrail.OutputGuardrailResult] ã§ãƒ©ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚  
3. æœ€å¾Œã« [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] ãŒ true ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚true ã®å ´åˆã€[`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered] ä¾‹å¤–ãŒé€å‡ºã•ã‚Œã‚‹ã®ã§ã€ ãƒ¦ãƒ¼ã‚¶ãƒ¼ ã¸ã®é©åˆ‡ãªå¿œç­”ã‚„ä¾‹å¤–å‡¦ç†ã‚’è¡Œãˆã¾ã™ã€‚  

!!! Note

    Output ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯æœ€çµ‚çš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‡ºåŠ›ã«å¯¾ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã‚‹ãŸã‚ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã®ã¯ãã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ *æœ€å¾Œ* ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã‚ã‚‹å ´åˆã ã‘ã§ã™ã€‚Input ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«åŒæ§˜ã€ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯å®Ÿéš›ã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã«å¯†æ¥ã«é–¢é€£ã™ã‚‹ãŸã‚ã€ã‚³ãƒ¼ãƒ‰ã‚’åŒã˜å ´æ‰€ã«ç½®ãã“ã¨ã§å¯èª­æ€§ãŒå‘ä¸Šã—ã¾ã™ã€‚

## ãƒˆãƒªãƒƒãƒ—ãƒ¯ã‚¤ãƒ¤

å…¥åŠ›ã¾ãŸã¯å‡ºåŠ›ãŒã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã«å¤±æ•—ã—ãŸå ´åˆã€ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯ãƒˆãƒªãƒƒãƒ—ãƒ¯ã‚¤ãƒ¤ã‚’ç”¨ã„ã¦ãã‚Œã‚’é€šçŸ¥ã§ãã¾ã™ã€‚ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ãŒãƒˆãƒªãƒƒãƒ—ãƒ¯ã‚¤ãƒ¤ã‚’ç™ºç«ã—ãŸã“ã¨ã‚’æ¤œçŸ¥ã™ã‚‹ã¨ã€ãŸã ã¡ã« `{Input,Output}GuardrailTripwireTriggered` ä¾‹å¤–ã‚’é€å‡ºã—ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œã‚’åœæ­¢ã—ã¾ã™ã€‚

## ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã®å®Ÿè£…

å…¥åŠ›ã‚’å—ã‘å–ã‚Šã€[`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput] ã‚’è¿”ã™é–¢æ•°ã‚’ç”¨æ„ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æ¬¡ã®ä¾‹ã§ã¯ã€å†…éƒ¨ã§ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã‚’å®Ÿè¡Œã—ã¦ã“ã‚Œã‚’è¡Œã„ã¾ã™ã€‚

```python
from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    InputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    TResponseInputItem,
    input_guardrail,
)

class MathHomeworkOutput(BaseModel):
    is_math_homework: bool
    reasoning: str

guardrail_agent = Agent( # (1)!
    name="Guardrail check",
    instructions="Check if the user is asking you to do their math homework.",
    output_type=MathHomeworkOutput,
)


@input_guardrail
async def math_guardrail( # (2)!
    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, input, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output, # (3)!
        tripwire_triggered=result.final_output.is_math_homework,
    )


agent = Agent(  # (4)!
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    input_guardrails=[math_guardrail],
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except InputGuardrailTripwireTriggered:
        print("Math homework guardrail tripped")
```

1. ã“ã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã‚’ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•°å†…ã§ä½¿ç”¨ã—ã¾ã™ã€‚  
2. ã“ã‚Œã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å…¥åŠ› / ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€çµæœã‚’è¿”ã™ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•°ã§ã™ã€‚  
3. ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«çµæœã«è¿½åŠ æƒ…å ±ã‚’å«ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚  
4. ã“ã‚Œã¯ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®šç¾©ã™ã‚‹å®Ÿéš›ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚  

Output ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã‚‚åŒæ§˜ã§ã™ã€‚

```python
from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    OutputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    output_guardrail,
)
class MessageOutput(BaseModel): # (1)!
    response: str

class MathOutput(BaseModel): # (2)!
    reasoning: str
    is_math: bool

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the output includes any math.",
    output_type=MathOutput,
)

@output_guardrail
async def math_guardrail(  # (3)!
    ctx: RunContextWrapper, agent: Agent, output: MessageOutput
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, output.response, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output,
        tripwire_triggered=result.final_output.is_math,
    )

agent = Agent( # (4)!
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    output_guardrails=[math_guardrail],
    output_type=MessageOutput,
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except OutputGuardrailTripwireTriggered:
        print("Math output guardrail tripped")
```

1. ã“ã‚Œã¯å®Ÿéš›ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‡ºåŠ›å‹ã§ã™ã€‚  
2. ã“ã‚Œã¯ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã®å‡ºåŠ›å‹ã§ã™ã€‚  
3. ã“ã‚Œã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‡ºåŠ›ã‚’å—ã‘å–ã‚Šã€çµæœã‚’è¿”ã™ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•°ã§ã™ã€‚  
4. ã“ã‚Œã¯ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®šç¾©ã™ã‚‹å®Ÿéš›ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚
</file>

<file path="docs/ja/handoffs.md">
---
search:
  exclude: true
---
# ãƒãƒ³ãƒ‰ã‚ªãƒ•

ãƒãƒ³ãƒ‰ã‚ªãƒ•ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ãŒã‚¿ã‚¹ã‚¯ã‚’åˆ¥ã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã«å§”è­²ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€è¤‡æ•°ã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ãŒãã‚Œãã‚Œç•°ãªã‚‹åˆ†é‡ã‚’å°‚é–€ã¨ã™ã‚‹ã‚·ãƒŠãƒªã‚ªã§ç‰¹ã«ä¾¿åˆ©ã§ã™ã€‚ãŸã¨ãˆã°ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã‚¢ãƒ—ãƒªã§ã¯ã€æ³¨æ–‡çŠ¶æ³ã€è¿”é‡‘ã€ FAQ ãªã©ã®ã‚¿ã‚¹ã‚¯ã‚’å€‹åˆ¥ã«æ‹…å½“ã™ã‚‹ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã‚’ç”¨æ„ã§ãã¾ã™ã€‚

ãƒãƒ³ãƒ‰ã‚ªãƒ•ã¯ LLM ã‹ã‚‰ã¯ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦èªè­˜ã•ã‚Œã¾ã™ã€‚ãã®ãŸã‚ã€`Refund Agent` ã¨ã„ã† ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã¸ã®ãƒãƒ³ãƒ‰ã‚ªãƒ•ã§ã‚ã‚Œã°ã€ãƒ„ãƒ¼ãƒ«åã¯ `transfer_to_refund_agent` ã«ãªã‚Šã¾ã™ã€‚

## ãƒãƒ³ãƒ‰ã‚ªãƒ•ã®ä½œæˆ

ã™ã¹ã¦ã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã«ã¯ [`handoffs`][agents.agent.Agent.handoffs] ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãŒã‚ã‚Šã€ç›´æ¥ `Agent` ã‚’æ¸¡ã™ã“ã¨ã‚‚ã€ãƒãƒ³ãƒ‰ã‚ªãƒ•ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ `Handoff` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

Agents SDK ãŒæä¾›ã™ã‚‹ [`handoff()`][agents.handoffs.handoff] é–¢æ•°ã‚’ä½¿ã£ã¦ãƒãƒ³ãƒ‰ã‚ªãƒ•ã‚’ä½œæˆã§ãã¾ã™ã€‚ã“ã®é–¢æ•°ã§ã¯ã€å¼•ãç¶™ãå…ˆã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã‚’æŒ‡å®šã—ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã‚„å…¥åŠ›ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§è¨­å®šã§ãã¾ã™ã€‚

### åŸºæœ¬çš„ãªä½¿ã„æ–¹

ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒ³ãƒ‰ã‚ªãƒ•ã‚’ä½œæˆã™ã‚‹ä¾‹ã‚’ç¤ºã—ã¾ã™ã€‚

```python
from agents import Agent, handoff

billing_agent = Agent(name="Billing agent")
refund_agent = Agent(name="Refund agent")

# (1)!
triage_agent = Agent(name="Triage agent", handoffs=[billing_agent, handoff(refund_agent)])
```

1. `billing_agent` ã®ã‚ˆã†ã« ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã‚’ç›´æ¥æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚ã€`handoff()` é–¢æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

### `handoff()` é–¢æ•°ã«ã‚ˆã‚‹ãƒãƒ³ãƒ‰ã‚ªãƒ•ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

[`handoff()`][agents.handoffs.handoff] é–¢æ•°ã‚’ä½¿ã†ã¨ã€ãƒãƒ³ãƒ‰ã‚ªãƒ•ã‚’ç´°ã‹ãã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¾ã™ã€‚

-   `agent`: ã“ã“ã§æŒ‡å®šã—ãŸ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã«å‡¦ç†ãŒå¼•ãæ¸¡ã•ã‚Œã¾ã™ã€‚
-   `tool_name_override`: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ `Handoff.default_tool_name()` ãŒä½¿ç”¨ã•ã‚Œã€`transfer_to_<agent_name>` ã¨ã„ã†åå‰ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã‚’ä¸Šæ›¸ãã§ãã¾ã™ã€‚
-   `tool_description_override`: `Handoff.default_tool_description()` ãŒè¿”ã™ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ„ãƒ¼ãƒ«èª¬æ˜ã‚’ä¸Šæ›¸ãã—ã¾ã™ã€‚
-   `on_handoff`: ãƒãƒ³ãƒ‰ã‚ªãƒ•å®Ÿè¡Œæ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã§ã™ã€‚ãƒãƒ³ãƒ‰ã‚ªãƒ•ãŒå‘¼ã°ã‚ŒãŸã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’é–‹å§‹ã™ã‚‹ãªã©ã«ä¾¿åˆ©ã§ã™ã€‚ã“ã®é–¢æ•°ã¯ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ LLM ãŒç”Ÿæˆã—ãŸå…¥åŠ›ã‚‚å—ã‘å–ã‚Œã¾ã™ã€‚æ¸¡ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ `input_type` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã§åˆ¶å¾¡ã—ã¾ã™ã€‚
-   `input_type`: ãƒãƒ³ãƒ‰ã‚ªãƒ•ãŒå—ã‘å–ã‚‹å…¥åŠ›ã®å‹ï¼ˆä»»æ„ï¼‰ã€‚
-   `input_filter`: æ¬¡ã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ãŒå—ã‘å–ã‚‹å…¥åŠ›ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ãã¾ã™ã€‚è©³ç´°ã¯å¾Œè¿°ã—ã¾ã™ã€‚

```python
from agents import Agent, handoff, RunContextWrapper

def on_handoff(ctx: RunContextWrapper[None]):
    print("Handoff called")

agent = Agent(name="My agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    tool_name_override="custom_handoff_tool",
    tool_description_override="Custom description",
)
```

## ãƒãƒ³ãƒ‰ã‚ªãƒ•å…¥åŠ›

å ´åˆã«ã‚ˆã£ã¦ã¯ã€ LLM ãŒãƒãƒ³ãƒ‰ã‚ªãƒ•ã‚’å‘¼ã³å‡ºã™éš›ã«è¿½åŠ ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã—ã¦ã»ã—ã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ãŸã¨ãˆã°ã€ŒEscalation ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ã¸ã®ãƒãƒ³ãƒ‰ã‚ªãƒ•ã§ã¯ã€ãƒ­ã‚°ç”¨ã«ç†ç”±ã‚’æ¸¡ã—ã¦ã‚‚ã‚‰ã„ãŸã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

```python
from pydantic import BaseModel

from agents import Agent, handoff, RunContextWrapper

class EscalationData(BaseModel):
    reason: str

async def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):
    print(f"Escalation agent called with reason: {input_data.reason}")

agent = Agent(name="Escalation agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    input_type=EscalationData,
)
```

## å…¥åŠ›ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼

ãƒãƒ³ãƒ‰ã‚ªãƒ•ãŒç™ºç”Ÿã™ã‚‹ã¨ã€æ–°ã—ã„ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ãŒä¼šè©±ã‚’å¼•ãç¶™ãã€ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´å…¨ä½“ã‚’é–²è¦§ã§ãã‚‹çŠ¶æ…‹ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã‚’å¤‰æ›´ã—ãŸã„å ´åˆã¯ [`input_filter`][agents.handoffs.Handoff.input_filter] ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚å…¥åŠ›ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¯ã€[`HandoffInputData`][agents.handoffs.HandoffInputData] ã¨ã—ã¦æ¸¡ã•ã‚Œã‚‹æ—¢å­˜ã®å…¥åŠ›ã‚’å—ã‘å–ã‚Šã€æ–°ã—ã„ `HandoffInputData` ã‚’è¿”ã™é–¢æ•°ã§ã™ã€‚

ã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãŸã¨ãˆã°å±¥æ­´ã‹ã‚‰ã™ã¹ã¦ã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å‰Šé™¤ã™ã‚‹ãªã©ï¼‰ã¯ [`agents.extensions.handoff_filters`][] ã«å®Ÿè£…æ¸ˆã¿ã§ã™ã€‚

```python
from agents import Agent, handoff
from agents.extensions import handoff_filters

agent = Agent(name="FAQ agent")

handoff_obj = handoff(
    agent=agent,
    input_filter=handoff_filters.remove_all_tools, # (1)!
)
```

1. ã“ã‚Œã«ã‚ˆã‚Š `FAQ agent` ãŒå‘¼ã°ã‚ŒãŸéš›ã«ã€å±¥æ­´ã‹ã‚‰ã™ã¹ã¦ã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒè‡ªå‹•ã§å‰Šé™¤ã•ã‚Œã¾ã™ã€‚

## æ¨å¥¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

LLM ãŒãƒãƒ³ãƒ‰ã‚ªãƒ•ã‚’æ­£ã—ãç†è§£ã§ãã‚‹ã‚ˆã†ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã«ãƒãƒ³ãƒ‰ã‚ªãƒ•ã«é–¢ã™ã‚‹æƒ…å ±ã‚’å«ã‚ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚äº‹å‰ã«ç”¨æ„ã—ãŸãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ [`agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX`][] ã‚’åˆ©ç”¨ã™ã‚‹ã‹ã€[`agents.extensions.handoff_prompt.prompt_with_handoff_instructions`][] ã‚’å‘¼ã³å‡ºã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ¨å¥¨æƒ…å ±ã‚’è‡ªå‹•ã§è¿½åŠ ã§ãã¾ã™ã€‚

```python
from agents import Agent
from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX

billing_agent = Agent(
    name="Billing agent",
    instructions=f"""{RECOMMENDED_PROMPT_PREFIX}
    <Fill in the rest of your prompt here>.""",
)
```
</file>

<file path="docs/ja/index.md">
---
search:
  exclude: true
---
# OpenAI Agents SDK

[OpenAI Agents SDK](https://github.com/openai/openai-agents-python) ã¯ã€æŠ½è±¡åŒ–ã‚’ã»ã¨ã‚“ã©æ’ã—ãŸè»½é‡ã§ä½¿ã„ã‚„ã™ã„ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã«ã‚ˆã‚Šã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã® AI ã‚¢ãƒ—ãƒªã‚’æ§‹ç¯‰ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ä»¥å‰ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‘ã‘å®Ÿé¨“ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã‚ã‚‹ [Swarm](https://github.com/openai/swarm/tree/main) ã‚’ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«ã¸ã¨ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã—ãŸã‚‚ã®ã§ã™ã€‚Agents SDK ã«ã¯ã”ãå°‘æ•°ã®åŸºæœ¬ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒã‚ã‚Šã¾ã™ã€‚

- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**: instructions ã¨ tools ã‚’å‚™ãˆãŸ LLM  
- **ãƒãƒ³ãƒ‰ã‚ªãƒ•**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‰¹å®šã‚¿ã‚¹ã‚¯ã‚’ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸å§”ä»»ã™ã‚‹ã—ãã¿  
- **ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®å…¥åŠ›ã‚’æ¤œè¨¼ã™ã‚‹æ©Ÿèƒ½  

Python ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã“ã‚Œã‚‰ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ãƒ„ãƒ¼ãƒ«ã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®è¤‡é›‘ãªé–¢ä¿‚ã‚’è¡¨ç¾ã§ãã€å­¦ç¿’ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆã¤ã¤å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚ã•ã‚‰ã« SDK ã«ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ•ãƒ­ãƒ¼ã‚’å¯è¦–åŒ–ãƒ»ãƒ‡ãƒãƒƒã‚°ã§ãã‚‹ **ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°** ãŒæ¨™æº–æ­è¼‰ã•ã‚Œã¦ãŠã‚Šã€è©•ä¾¡ã‚„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚‚æ´»ç”¨å¯èƒ½ã§ã™ã€‚

## Agents SDK ã‚’ä½¿ç”¨ã™ã‚‹ç†ç”±

SDK ã«ã¯ 2 ã¤ã®è¨­è¨ˆåŸå‰‡ãŒã‚ã‚Šã¾ã™ã€‚

1. ä½¿ã†ä¾¡å€¤ã®ã‚ã‚‹ååˆ†ãªæ©Ÿèƒ½ã‚’å‚™ãˆã¤ã¤ã€å­¦ç¿’ãŒæ—©ã„ã‚ˆã†ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ•°ã‚’çµã‚‹ã€‚  
2. ã™ãã«ä½¿ã„å§‹ã‚ã‚‰ã‚Œã‚‹åˆæœŸè¨­å®šã§å‹•ä½œã—ã¤ã¤ã€æŒ™å‹•ã‚’ç´°ã‹ãã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã‚‹ã€‚  

ä¸»ãªæ©Ÿèƒ½ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚

- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—: ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã€çµæœã‚’ LLM ã«é€ä¿¡ã€LLM ãŒå®Œäº†ã™ã‚‹ã¾ã§ã®ãƒ«ãƒ¼ãƒ—ã‚’è‡ªå‹•ã§å‡¦ç†ã€‚  
- Python ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆ: æ–°ã—ã„æŠ½è±¡åŒ–ã‚’å­¦ã°ãšã«ã€è¨€èªæ¨™æº–æ©Ÿèƒ½ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€‚  
- ãƒãƒ³ãƒ‰ã‚ªãƒ•: è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®å”èª¿ã¨å§”è­²ã‚’å®Ÿç¾ã™ã‚‹å¼·åŠ›ãªæ©Ÿèƒ½ã€‚  
- ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ä¸¦åˆ—ã§å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã€å¤±æ•—æ™‚ã«æ—©æœŸçµ‚äº†ã€‚  
- é–¢æ•°ãƒ„ãƒ¼ãƒ«: ä»»æ„ã® Python é–¢æ•°ã‚’ãƒ„ãƒ¼ãƒ«åŒ–ã—ã€è‡ªå‹•ã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆã¨ Pydantic ã§ã®æ¤œè¨¼ã‚’æä¾›ã€‚  
- ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°: ãƒ•ãƒ­ãƒ¼ã®å¯è¦–åŒ–ãƒ»ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã«åŠ ãˆã€OpenAI ã®è©•ä¾¡ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»è’¸ç•™ãƒ„ãƒ¼ãƒ«ã‚’åˆ©ç”¨å¯èƒ½ã€‚  

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install openai-agents
```

## Hello World ã®ä¾‹

```python
from agents import Agent, Runner

agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)

# Code within the code,
# Functions calling themselves,
# Infinite loop's dance.
```

(_ã“ã‚Œã‚’å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€`OPENAI_API_KEY` ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„_)

```bash
export OPENAI_API_KEY=sk-...
```
</file>

<file path="docs/ja/mcp.md">
---
search:
  exclude: true
---
# Model context protocol (MCP)

[Model context protocol](https://modelcontextprotocol.io/introduction)ï¼ˆé€šç§° MCPï¼‰ã¯ã€ LLM ã«ãƒ„ãƒ¼ãƒ«ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æä¾›ã™ã‚‹ãŸã‚ã®ä»•çµ„ã¿ã§ã™ã€‚MCP ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯æ¬¡ã®ã‚ˆã†ã«èª¬æ˜ã•ã‚Œã¦ã„ã¾ã™ã€‚

> MCP ã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒ LLM ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æä¾›ã™ã‚‹æ–¹æ³•ã‚’æ¨™æº–åŒ–ã™ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã§ã™ã€‚MCP ã¯ AI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¨ã£ã¦ã® USBâ€‘C ãƒãƒ¼ãƒˆã®ã‚ˆã†ãªã‚‚ã®ã¨è€ƒãˆã¦ãã ã•ã„ã€‚USBâ€‘C ãŒå„ç¨®ãƒ‡ãƒã‚¤ã‚¹ã‚’å‘¨è¾ºæ©Ÿå™¨ã¨æ¥ç¶šã™ã‚‹ãŸã‚ã®æ¨™æº–åŒ–ã•ã‚ŒãŸæ–¹æ³•ã‚’æä¾›ã™ã‚‹ã®ã¨åŒæ§˜ã«ã€MCP ã¯ AI ãƒ¢ãƒ‡ãƒ«ã‚’ã•ã¾ã–ã¾ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚„ãƒ„ãƒ¼ãƒ«ã¨æ¥ç¶šã™ã‚‹ãŸã‚ã®æ¨™æº–åŒ–ã•ã‚ŒãŸæ–¹æ³•ã‚’æä¾›ã—ã¾ã™ã€‚

Agents SDK ã¯ MCP ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€ã“ã‚Œã«ã‚ˆã‚Šå¹…åºƒã„ MCP ã‚µãƒ¼ãƒãƒ¼ã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦è¿½åŠ ã§ãã¾ã™ã€‚

## MCP ã‚µãƒ¼ãƒãƒ¼

ç¾åœ¨ã€MCP ä»•æ§˜ã§ã¯ä½¿ç”¨ã™ã‚‹ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ãƒˆæ–¹å¼ã«åŸºã¥ã 2 ç¨®é¡ã®ã‚µãƒ¼ãƒãƒ¼ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã€‚

1. **stdio** ã‚µãƒ¼ãƒãƒ¼: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ã‹ã™ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã™ã€‚  
2. **HTTP over SSE** ã‚µãƒ¼ãƒãƒ¼: ãƒªãƒ¢ãƒ¼ãƒˆã§å‹•ä½œã—ã€ URL çµŒç”±ã§æ¥ç¶šã—ã¾ã™ã€‚

ã“ã‚Œã‚‰ã®ã‚µãƒ¼ãƒãƒ¼ã¸ã¯ [`MCPServerStdio`][agents.mcp.server.MCPServerStdio] ã¨ [`MCPServerSse`][agents.mcp.server.MCPServerSse] ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦æ¥ç¶šã§ãã¾ã™ã€‚

ãŸã¨ãˆã°ã€[å…¬å¼ MCP filesystem ã‚µãƒ¼ãƒãƒ¼](https://www.npmjs.com/package/@modelcontextprotocol/server-filesystem)ã‚’åˆ©ç”¨ã™ã‚‹å ´åˆã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```python
async with MCPServerStdio(
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", samples_dir],
    }
) as server:
    tools = await server.list_tools()
```

## MCP ã‚µãƒ¼ãƒãƒ¼ã®åˆ©ç”¨

MCP ã‚µãƒ¼ãƒãƒ¼ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«è¿½åŠ ã§ãã¾ã™ã€‚Agents SDK ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œæ™‚ã«æ¯å› MCP ã‚µãƒ¼ãƒãƒ¼ã¸ `list_tools()` ã‚’å‘¼ã³å‡ºã—ã€ LLM ã« MCP ã‚µãƒ¼ãƒãƒ¼ã®ãƒ„ãƒ¼ãƒ«ã‚’èªè­˜ã•ã›ã¾ã™ã€‚LLM ãŒ MCP ã‚µãƒ¼ãƒãƒ¼ã®ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™ã¨ã€SDK ã¯ãã®ã‚µãƒ¼ãƒãƒ¼ã¸ `call_tool()` ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```python

agent=Agent(
    name="Assistant",
    instructions="Use the tools to achieve the task",
    mcp_servers=[mcp_server_1, mcp_server_2]
)
```

## ã‚­ãƒ£ãƒƒã‚·ãƒ¥

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå®Ÿè¡Œã•ã‚Œã‚‹ãŸã³ã«ã€MCP ã‚µãƒ¼ãƒãƒ¼ã¸ `list_tools()` ãŒå‘¼ã³å‡ºã•ã‚Œã¾ã™ã€‚ã‚µãƒ¼ãƒãƒ¼ãŒãƒªãƒ¢ãƒ¼ãƒˆã®å ´åˆã¯ç‰¹ã«ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒç™ºç”Ÿã—ã¾ã™ã€‚ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’è‡ªå‹•ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ãŸã„å ´åˆã¯ã€[`MCPServerStdio`][agents.mcp.server.MCPServerStdio] ã¨ [`MCPServerSse`][agents.mcp.server.MCPServerSse] ã®ä¸¡æ–¹ã« `cache_tools_list=True` ã‚’æ¸¡ã—ã¦ãã ã•ã„ã€‚ãƒ„ãƒ¼ãƒ«ä¸€è¦§ãŒå¤‰æ›´ã•ã‚Œãªã„ã¨ç¢ºä¿¡ã§ãã‚‹å ´åˆã®ã¿ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–ã—ãŸã„å ´åˆã¯ã€ã‚µãƒ¼ãƒãƒ¼ã§ `invalidate_tools_cache()` ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚

## ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ã‚³ãƒ¼ãƒ‰ä¾‹

å®Œå…¨ãªå‹•ä½œä¾‹ã¯ [examples/mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp) ã‚’ã”è¦§ãã ã•ã„ã€‚

## ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

[ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°](./tracing.md) ã¯ MCP ã®æ“ä½œã‚’è‡ªå‹•çš„ã«ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã¾ã™ã€‚å…·ä½“çš„ã«ã¯æ¬¡ã®å†…å®¹ãŒå«ã¾ã‚Œã¾ã™ã€‚

1. ãƒ„ãƒ¼ãƒ«ä¸€è¦§å–å¾—ã®ãŸã‚ã® MCP ã‚µãƒ¼ãƒãƒ¼å‘¼ã³å‡ºã—  
2. é–¢æ•°å‘¼ã³å‡ºã—ã«é–¢ã™ã‚‹ MCP æƒ…å ±  

![MCP Tracing Screenshot](../assets/images/mcp-tracing.jpg)
</file>

<file path="docs/ja/models.md">
# ãƒ¢ãƒ‡ãƒ«

Agents SDK ã«ã¯ã€OpenAI ãƒ¢ãƒ‡ãƒ«ã® 2 ç¨®é¡ã®ã‚µãƒãƒ¼ãƒˆãŒæ¨™æº–ã§ç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚

-   **æ¨å¥¨**: [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] ã¯ã€æ–°ã—ã„ [Responses API](https://platform.openai.com/docs/api-reference/responses) ã‚’ä½¿ã£ã¦ OpenAI API ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚
-   [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] ã¯ã€[Chat Completions API](https://platform.openai.com/docs/api-reference/chat) ã‚’ä½¿ã£ã¦ OpenAI API ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚

## ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›

1 ã¤ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å†…ã§ã€å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã”ã¨ã«ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ãŸã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ãŸã¨ãˆã°ã€ãƒˆãƒªã‚¢ãƒ¼ã‚¸ã«ã¯å°å‹ã§é«˜é€Ÿãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã«ã¯ã‚ˆã‚Šå¤§ããé«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†ã“ã¨ãŒã§ãã¾ã™ã€‚[`Agent`][agents.Agent] ã‚’è¨­å®šã™ã‚‹éš›ã€ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã§ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã¾ã™ã€‚

1. OpenAI ãƒ¢ãƒ‡ãƒ«åã‚’ç›´æ¥æ¸¡ã™ã€‚
2. ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«åã¨ã€ãã®åå‰ã‚’ Model ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã§ãã‚‹ [`ModelProvider`][agents.models.interface.ModelProvider] ã‚’æ¸¡ã™ã€‚
3. [`Model`][agents.models.interface.Model] å®Ÿè£…ã‚’ç›´æ¥æŒ‡å®šã™ã‚‹ã€‚

!!!note

    SDK ã¯ [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] ã¨ [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] ã®ä¸¡æ–¹ã®å½¢çŠ¶ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ãŒã€å„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ 1 ã¤ã®ãƒ¢ãƒ‡ãƒ«å½¢çŠ¶ã®ã¿ã‚’ä½¿ã†ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚ãªãœãªã‚‰ã€2 ã¤ã®å½¢çŠ¶ã¯ã‚µãƒãƒ¼ãƒˆã™ã‚‹æ©Ÿèƒ½ã‚„ãƒ„ãƒ¼ãƒ«ãŒç•°ãªã‚‹ãŸã‚ã§ã™ã€‚ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ãƒ¢ãƒ‡ãƒ«å½¢çŠ¶ã‚’çµ„ã¿åˆã‚ã›ã¦ä½¿ã†å ´åˆã¯ã€åˆ©ç”¨ã™ã‚‹ã™ã¹ã¦ã®æ©Ÿèƒ½ãŒä¸¡æ–¹ã§åˆ©ç”¨å¯èƒ½ã‹ã”ç¢ºèªãã ã•ã„ã€‚

```python
from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
    model="o3-mini", # (1)!
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model=OpenAIChatCompletionsModel( # (2)!
        model="gpt-4o",
        openai_client=AsyncOpenAI()
    ),
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    model="gpt-3.5-turbo",
)

async def main():
    result = await Runner.run(triage_agent, input="Hola, Â¿cÃ³mo estÃ¡s?")
    print(result.final_output)
```

1.  OpenAI ãƒ¢ãƒ‡ãƒ«åã‚’ç›´æ¥è¨­å®šã—ã¾ã™ã€‚
2.  [`Model`][agents.models.interface.Model] å®Ÿè£…ã‚’æŒ‡å®šã—ã¾ã™ã€‚

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ã•ã‚‰ã«ç´°ã‹ãè¨­å®šã—ãŸã„å ´åˆã¯ã€[`ModelSettings`][agents.models.interface.ModelSettings] ã‚’æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€temperature ãªã©ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’æŒ‡å®šã§ãã¾ã™ã€‚

```python
from agents import Agent, ModelSettings

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model="gpt-4o",
    model_settings=ModelSettings(temperature=0.1),
)
```

## ä»–ã® LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆ©ç”¨

ä»–ã® LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯ã€3 ã¤ã®æ–¹æ³•ã§åˆ©ç”¨ã§ãã¾ã™ï¼ˆ[ã“ã¡ã‚‰](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/) ã« code examples ãŒã‚ã‚Šã¾ã™ï¼‰ã€‚

1. [`set_default_openai_client`][agents.set_default_openai_client] ã¯ã€`AsyncOpenAI` ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã—ã¦ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«åˆ©ç”¨ã—ãŸã„å ´åˆã«ä¾¿åˆ©ã§ã™ã€‚ã“ã‚Œã¯ã€LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒ OpenAI äº’æ›ã® API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æŒã¡ã€`base_url` ã¨ `api_key` ã‚’è¨­å®šã§ãã‚‹å ´åˆã«ä½¿ã„ã¾ã™ã€‚[examples/model_providers/custom_example_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py) ã«è¨­å®šä¾‹ãŒã‚ã‚Šã¾ã™ã€‚
2. [`ModelProvider`][agents.models.interface.ModelProvider] ã¯ `Runner.run` ãƒ¬ãƒ™ãƒ«ã§åˆ©ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã€Œã“ã®å®Ÿè¡Œã®ã™ã¹ã¦ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ã†ã€ã¨æŒ‡å®šã§ãã¾ã™ã€‚[examples/model_providers/custom_example_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py) ã«è¨­å®šä¾‹ãŒã‚ã‚Šã¾ã™ã€‚
3. [`Agent.model`][agents.agent.Agent.model] ã§ã€ç‰¹å®šã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã”ã¨ã«ç•°ãªã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ã¦ä½¿ã†ã“ã¨ãŒã§ãã¾ã™ã€‚[examples/model_providers/custom_example_agent.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py) ã«è¨­å®šä¾‹ãŒã‚ã‚Šã¾ã™ã€‚

`platform.openai.com` ã® API ã‚­ãƒ¼ãŒãªã„å ´åˆã¯ã€`set_tracing_disabled()` ã§ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’ç„¡åŠ¹ã«ã™ã‚‹ã‹ã€[åˆ¥ã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼](tracing.md) ã‚’è¨­å®šã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

!!! note

    ã“ã‚Œã‚‰ã® code examples ã§ã¯ Chat Completions API/ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚ãªãœãªã‚‰ã€ã»ã¨ã‚“ã©ã® LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯ã¾ã  Responses API ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ã§ã™ã€‚ã‚‚ã— LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒ Responses API ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹å ´åˆã¯ã€Responses ã®åˆ©ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

## ä»–ã® LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ©ç”¨æ™‚ã®ã‚ˆãã‚ã‚‹å•é¡Œ

### Tracing ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã® 401 ã‚¨ãƒ©ãƒ¼

ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã«é–¢é€£ã™ã‚‹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã€ã“ã‚Œã¯ãƒˆãƒ¬ãƒ¼ã‚¹ãŒ OpenAI ã‚µãƒ¼ãƒãƒ¼ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ãŸã‚ã€OpenAI API ã‚­ãƒ¼ãŒãªã„ã“ã¨ãŒåŸå› ã§ã™ã€‚è§£æ±ºæ–¹æ³•ã¯ 3 ã¤ã‚ã‚Šã¾ã™ã€‚

1. ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’å®Œå…¨ã«ç„¡åŠ¹åŒ–ã™ã‚‹: [`set_tracing_disabled(True)`][agents.set_tracing_disabled]ã€‚
2. ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ç”¨ã® OpenAI ã‚­ãƒ¼ã‚’è¨­å®šã™ã‚‹: [`set_tracing_export_api_key(...)`][agents.set_tracing_export_api_key]ã€‚ã“ã® API ã‚­ãƒ¼ã¯ãƒˆãƒ¬ãƒ¼ã‚¹ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®ã¿ã«ä½¿ã‚ã‚Œã€[platform.openai.com](https://platform.openai.com/) ã®ã‚‚ã®ãŒå¿…è¦ã§ã™ã€‚
3. OpenAI ä»¥å¤–ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’ä½¿ã†ã€‚[ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](tracing.md#custom-tracing-processors) ã‚’ã”è¦§ãã ã•ã„ã€‚

### Responses API ã‚µãƒãƒ¼ãƒˆ

SDK ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ Responses API ã‚’ä½¿ã„ã¾ã™ãŒã€ã»ã¨ã‚“ã©ã®ä»–ã® LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯ã¾ã å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚ãã®ãŸã‚ã€404 ã‚¨ãƒ©ãƒ¼ãªã©ãŒç™ºç”Ÿã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚è§£æ±ºæ–¹æ³•ã¯ 2 ã¤ã‚ã‚Šã¾ã™ã€‚

1. [`set_default_openai_api("chat_completions")`][agents.set_default_openai_api] ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ç’°å¢ƒå¤‰æ•°ã§ `OPENAI_API_KEY` ã¨ `OPENAI_BASE_URL` ã‚’è¨­å®šã—ã¦ã„ã‚‹å ´åˆã«æœ‰åŠ¹ã§ã™ã€‚
2. [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] ã‚’ä½¿ã„ã¾ã™ã€‚[ã“ã¡ã‚‰](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/) ã« code examples ãŒã‚ã‚Šã¾ã™ã€‚

### structured outputs ã‚µãƒãƒ¼ãƒˆ

ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯ [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚ãã®å ´åˆã€æ¬¡ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

```
BadRequestError: Error code: 400 - {'error': {'message': "'response_format.type' : value is not one of the allowed values ['text','json_object']", 'type': 'invalid_request_error'}}
```

ã“ã‚Œã¯ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆ¶é™ã§ã€JSON å‡ºåŠ›ã«ã¯å¯¾å¿œã—ã¦ã„ã¦ã‚‚ã€å‡ºåŠ›ã«ä½¿ã† `json_schema` ã‚’æŒ‡å®šã§ããªã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ç¾åœ¨ã“ã®å•é¡Œã®ä¿®æ­£ã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ãŒã€JSON schema å‡ºåŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆ©ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚ãã†ã§ãªã„å ´åˆã€ä¸æ­£ãª JSON ã«ã‚ˆã‚Šã‚¢ãƒ—ãƒªãŒé »ç¹ã«å‹•ä½œã—ãªããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
</file>

<file path="docs/ja/multi_agent.md">
---
search:
  exclude: true
---
# è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã¯ã€ã‚¢ãƒ—ãƒªå†…ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã©ã®ã‚ˆã†ã«æµã‚Œã‚‹ã‹ã‚’æŒ‡ã—ã¾ã™ã€‚ã©ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã€ã©ã®é †åºã§å®Ÿè¡Œã•ã‚Œã€ãã®å¾Œã©ã†æ±ºå®šã™ã‚‹ã‹ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ä¸»ãªæ–¹æ³•ã¯æ¬¡ã® 2 ã¤ã§ã™ã€‚

1.  LLM ã«åˆ¤æ–­ã•ã›ã‚‹:  LLM ã®çŸ¥èƒ½ã‚’æ´»ç”¨ã—ã€è¨ˆç”»ãƒ»æ¨è«–ã‚’è¡Œã„ã€ãã®çµæœã«åŸºã¥ã„ã¦æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ±ºå®šã—ã¾ã™ã€‚  
2.  ã‚³ãƒ¼ãƒ‰ã§ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹: ã‚³ãƒ¼ãƒ‰å´ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æµã‚Œã‚’å®šç¾©ã—ã¾ã™ã€‚

ã“ã‚Œã‚‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ã§ãã¾ã™ã€‚ãã‚Œãã‚Œã«ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã‚ã‚Šã€ä»¥ä¸‹ã§èª¬æ˜ã—ã¾ã™ã€‚

## LLM ã«ã‚ˆã‚‹ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã¯ã€ instructionsã€ãƒ„ãƒ¼ãƒ«ã€ãƒãƒ³ãƒ‰ã‚ªãƒ•ã‚’å‚™ãˆãŸ  LLM ã§ã™ã€‚ã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ³ãƒ‰ãªã‚¿ã‚¹ã‚¯ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ LLM ã¯ã‚¿ã‚¹ã‚¯ã‚’ã©ã®ã‚ˆã†ã«é€²ã‚ã‚‹ã‹ã‚’è‡ªå¾‹çš„ã«è¨ˆç”»ã—ã€ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚„ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’è¡Œã„ã€ãƒãƒ³ãƒ‰ã‚ªãƒ•ã§ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã‚¿ã‚¹ã‚¯ã‚’å§”è­²ã§ãã¾ã™ã€‚ãŸã¨ãˆã°ã€ãƒªã‚µãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã¯æ¬¡ã®ã‚ˆã†ãªãƒ„ãƒ¼ãƒ«ã‚’è£…å‚™ã§ãã¾ã™ã€‚

-   Web æ¤œç´¢ã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æƒ…å ±ã‚’å–å¾—ã™ã‚‹  
-   ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã§ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã‚„æ¥ç¶šã‚’èª¿ã¹ã‚‹  
-   ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿æ“ä½œã§ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ä¸Šã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹  
-   ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã§ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’è¡Œã†  
-   è¨ˆç”»ç«‹æ¡ˆã‚„ãƒ¬ãƒãƒ¼ãƒˆä½œæˆãªã©ã«é•·ã‘ãŸå°‚é–€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®ãƒãƒ³ãƒ‰ã‚ªãƒ•  

ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã‚¿ã‚¹ã‚¯ãŒã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ³ãƒ‰ã§ã€ LLM ã®çŸ¥èƒ½ã«é ¼ã‚ŠãŸã„å ´åˆã«æœ€é©ã§ã™ã€‚é‡è¦ãªæˆ¦è¡“ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚

1.  è‰¯ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æŠ•è³‡ã™ã‚‹ã€‚åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã€ä½¿ã„æ–¹ã€å®ˆã‚‹ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’æ˜ç¢ºã«ç¤ºã—ã¾ã™ã€‚  
2.  ã‚¢ãƒ—ãƒªã‚’ç›£è¦–ã—ã€æ”¹å–„ã‚’ç¹°ã‚Šè¿”ã™ã€‚å•é¡ŒãŒèµ·ããŸç®‡æ‰€ã‚’ç‰¹å®šã—ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ”¹å–„ã—ã¾ã™ã€‚  
3.  ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å†…çœã¨æ”¹å–„ã‚’è¨±å¯ã™ã‚‹ã€‚ãŸã¨ãˆã°ãƒ«ãƒ¼ãƒ—ã§å®Ÿè¡Œã—è‡ªå·±æ‰¹è©•ã•ã›ãŸã‚Šã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ¸¡ã—ã¦ä¿®æ­£ã•ã›ãŸã‚Šã—ã¾ã™ã€‚  
4.  ä½•ã§ã‚‚ã“ãªã™æ±ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚ˆã‚Šã€ç‰¹å®šã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç”¨æ„ã—ã¾ã™ã€‚  
5.  [evals](https://platform.openai.com/docs/guides/evals) ã«æŠ•è³‡ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´ã—ã€ã‚¿ã‚¹ã‚¯æ€§èƒ½ã‚’å‘ä¸Šã§ãã¾ã™ã€‚  

## ã‚³ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

LLM ã«ã‚ˆã‚‹ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯å¼·åŠ›ã§ã™ãŒã€ã‚³ãƒ¼ãƒ‰ã§ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ã¨é€Ÿåº¦ãƒ»ã‚³ã‚¹ãƒˆãƒ»æ€§èƒ½ã®é¢ã§ã‚ˆã‚Šæ±ºå®šçš„ã‹ã¤äºˆæ¸¬å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ã‚ˆãä½¿ã‚ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚

-   [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) ã‚’ä½¿ã£ã¦ã€ã‚³ãƒ¼ãƒ‰å´ã§æ¤œæŸ»ã§ãã‚‹ é©åˆ‡ãªå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ ã‚’ç”Ÿæˆã™ã‚‹ã€‚ãŸã¨ãˆã°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚¿ã‚¹ã‚¯ã‚’ã„ãã¤ã‹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«åˆ†é¡ã•ã›ã€ãã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«å¿œã˜ã¦æ¬¡ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’é¸æŠã—ã¾ã™ã€‚  
-   ã‚ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‡ºåŠ›ã‚’æ¬¡ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å…¥åŠ›ã«å¤‰æ›ã—ã¦è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒã‚§ãƒ¼ãƒ³ã™ã‚‹ã€‚ãƒ–ãƒ­ã‚°è¨˜äº‹åŸ·ç­†ã‚’ã€Œãƒªã‚µãƒ¼ãƒ â†’ ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ä½œæˆ â†’ è¨˜äº‹åŸ·ç­† â†’ æ‰¹è©• â†’ æ”¹å–„ã€ã¨ã„ã†ä¸€é€£ã®ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†è§£ã§ãã¾ã™ã€‚  
-   ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ `while` ãƒ«ãƒ¼ãƒ—ã§å›ã—ã€è©•ä¾¡ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¡Œã†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨çµ„ã¿åˆã‚ã›ã€è©•ä¾¡è€…ãŒåŸºæº–ã‚’æº€ãŸã—ãŸã¨åˆ¤æ–­ã™ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚  
-   `asyncio.gather` ãªã© Python ã®åŸºæœ¬ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç”¨ã„ã¦è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹ã€‚äº’ã„ã«ä¾å­˜ã—ãªã„è¤‡æ•°ã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆã«é«˜é€ŸåŒ–ã§ãã¾ã™ã€‚  

[`examples/agent_patterns`](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns) ã«ã¯å¤šæ•°ã®ã‚³ãƒ¼ãƒ‰ä¾‹ãŒã‚ã‚Šã¾ã™ã€‚
</file>

<file path="docs/ja/quickstart.md">
---
search:
  exclude: true
---
# ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¨ä»®æƒ³ç’°å¢ƒã®ä½œæˆ

ã“ã‚Œã¯ä¸€åº¦ã ã‘è¡Œãˆã°ååˆ†ã§ã™ã€‚

```bash
mkdir my_project
cd my_project
python -m venv .venv
```

### ä»®æƒ³ç’°å¢ƒã®æœ‰åŠ¹åŒ–

æ–°ã—ã„ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã™ã‚‹ãŸã³ã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

```bash
source .venv/bin/activate
```

### Agents SDK ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install openai-agents # or `uv add openai-agents`, etc
```

### OpenAI API ã‚­ãƒ¼ã®è¨­å®š

ã¾ã ãŠæŒã¡ã§ãªã„å ´åˆã¯ã€[ã“ã¡ã‚‰ã®æ‰‹é †](https://platform.openai.com/docs/quickstart#create-and-export-an-api-key)ã«å¾“ã£ã¦ OpenAI API ã‚­ãƒ¼ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

```bash
export OPENAI_API_KEY=sk-...
```

## æœ€åˆã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆ

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ instructions ã€åå‰ã€`model_config` ãªã©ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šã§å®šç¾©ã—ã¾ã™ã€‚

```python
from agents import Agent

agent = Agent(
    name="Math Tutor",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)
```

## ã•ã‚‰ã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¿½åŠ 

è¿½åŠ ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚‚åŒæ§˜ã®æ–¹æ³•ã§å®šç¾©ã§ãã¾ã™ã€‚`handoff_descriptions` ã¯ãƒãƒ³ãƒ‰ã‚ªãƒ•ã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’åˆ¤æ–­ã™ã‚‹ãŸã‚ã®è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚

```python
from agents import Agent

history_tutor_agent = Agent(
    name="History Tutor",
    handoff_description="Specialist agent for historical questions",
    instructions="You provide assistance with historical queries. Explain important events and context clearly.",
)

math_tutor_agent = Agent(
    name="Math Tutor",
    handoff_description="Specialist agent for math questions",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)
```

## ãƒãƒ³ãƒ‰ã‚ªãƒ•ã®å®šç¾©

å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å¯¾ã—ã¦ã€ã‚¿ã‚¹ã‚¯ã‚’é€²ã‚ã‚‹éš›ã«é¸æŠã§ãã‚‹ãƒãƒ³ãƒ‰ã‚ªãƒ•å…ˆã®ä¸€è¦§ã‚’å®šç¾©ã§ãã¾ã™ã€‚

```python
triage_agent = Agent(
    name="Triage Agent",
    instructions="You determine which agent to use based on the user's homework question",
    handoffs=[history_tutor_agent, math_tutor_agent]
)
```

## ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œ

ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒå®Ÿè¡Œã•ã‚Œã€ãƒˆãƒªã‚¢ãƒ¼ã‚¸ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ 2 ã¤ã®å°‚é–€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã§æ­£ã—ããƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚

```python
from agents import Runner

async def main():
    result = await Runner.run(triage_agent, "What is the capital of France?")
    print(result.final_output)
```

## ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã®è¿½åŠ 

å…¥åŠ›ã¾ãŸã¯å‡ºåŠ›ã«å¯¾ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹ã‚«ã‚¹ã‚¿ãƒ ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã‚’å®šç¾©ã§ãã¾ã™ã€‚

```python
from agents import GuardrailFunctionOutput, Agent, Runner
from pydantic import BaseModel

class HomeworkOutput(BaseModel):
    is_homework: bool
    reasoning: str

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the user is asking about homework.",
    output_type=HomeworkOutput,
)

async def homework_guardrail(ctx, agent, input_data):
    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
    final_output = result.final_output_as(HomeworkOutput)
    return GuardrailFunctionOutput(
        output_info=final_output,
        tripwire_triggered=not final_output.is_homework,
    )
```

## ã™ã¹ã¦ã‚’ã¾ã¨ã‚ã‚‹

ãƒãƒ³ãƒ‰ã‚ªãƒ•ã¨å…¥åŠ›ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦ã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner
from pydantic import BaseModel
import asyncio

class HomeworkOutput(BaseModel):
    is_homework: bool
    reasoning: str

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the user is asking about homework.",
    output_type=HomeworkOutput,
)

math_tutor_agent = Agent(
    name="Math Tutor",
    handoff_description="Specialist agent for math questions",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)

history_tutor_agent = Agent(
    name="History Tutor",
    handoff_description="Specialist agent for historical questions",
    instructions="You provide assistance with historical queries. Explain important events and context clearly.",
)


async def homework_guardrail(ctx, agent, input_data):
    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
    final_output = result.final_output_as(HomeworkOutput)
    return GuardrailFunctionOutput(
        output_info=final_output,
        tripwire_triggered=not final_output.is_homework,
    )

triage_agent = Agent(
    name="Triage Agent",
    instructions="You determine which agent to use based on the user's homework question",
    handoffs=[history_tutor_agent, math_tutor_agent],
    input_guardrails=[
        InputGuardrail(guardrail_function=homework_guardrail),
    ],
)

async def main():
    result = await Runner.run(triage_agent, "who was the first president of the united states?")
    print(result.final_output)

    result = await Runner.run(triage_agent, "what is life")
    print(result.final_output)

if __name__ == "__main__":
    asyncio.run(main())
```

## ãƒˆãƒ¬ãƒ¼ã‚¹ã®è¡¨ç¤º

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œå†…å®¹ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[OpenAI ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã® Trace viewer](https://platform.openai.com/traces) ã«ç§»å‹•ã—ã¦ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’é–²è¦§ã—ã¦ãã ã•ã„ã€‚

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ã‚ˆã‚Šè¤‡é›‘ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ•ãƒ­ãƒ¼ã®æ§‹ç¯‰æ–¹æ³•ã‚’å­¦ã³ã¾ã—ã‚‡ã†ã€‚

-   [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ](agents.md) ã®è¨­å®šæ–¹æ³•ã‚’å­¦ã¶ã€‚
-   [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œ](running_agents.md) ã«ã¤ã„ã¦å­¦ã¶ã€‚
-   [ãƒ„ãƒ¼ãƒ«](tools.md)ã€[ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«](guardrails.md)ã€[ãƒ¢ãƒ‡ãƒ«](models/index.md) ã«ã¤ã„ã¦å­¦ã¶ã€‚
</file>

<file path="docs/ja/results.md">
---
search:
  exclude: true
---
# çµæœ

`Runner.run` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã¨ã€ä»¥ä¸‹ã®ã„ãšã‚Œã‹ãŒè¿”ã•ã‚Œã¾ã™ã€‚

-   `run` ã¾ãŸã¯ `run_sync` ã‚’å‘¼ã³å‡ºã—ãŸå ´åˆã¯ [`RunResult`][agents.result.RunResult]
-   `run_streamed` ã‚’å‘¼ã³å‡ºã—ãŸå ´åˆã¯ [`RunResultStreaming`][agents.result.RunResultStreaming]

ã“ã‚Œã‚‰ã¯ã©ã¡ã‚‰ã‚‚ [`RunResultBase`][agents.result.RunResultBase] ã‚’ç¶™æ‰¿ã—ã¦ãŠã‚Šã€ã»ã¨ã‚“ã©ã®æœ‰ç”¨ãªæƒ…å ±ã¯ã“ã“ã«æ ¼ç´ã•ã‚Œã¦ã„ã¾ã™ã€‚

## æœ€çµ‚å‡ºåŠ›

[`final_output`][agents.result.RunResultBase.final_output] ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã«ã¯ã€æœ€å¾Œã«å®Ÿè¡Œã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ€çµ‚å‡ºåŠ›ãŒæ ¼ç´ã•ã‚Œã¾ã™ã€‚å†…å®¹ã¯ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã§ã™ã€‚

-   `output_type` ãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ `str`
-   `output_type` ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ `last_agent.output_type` å‹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

!!! note

    `final_output` ã®å‹ã¯ `Any` ã§ã™ã€‚ãƒãƒ³ãƒ‰ã‚ªãƒ•ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€é™çš„ã«å‹ä»˜ã‘ã§ãã¾ã›ã‚“ã€‚ãƒãƒ³ãƒ‰ã‚ªãƒ•ãŒç™ºç”Ÿã™ã‚‹ã¨ã€ã©ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã‚‚æœ€å¾Œã«ãªã‚Šå¾—ã‚‹ãŸã‚ã€å¯èƒ½æ€§ã®ã‚ã‚‹å‡ºåŠ›å‹ã‚’é™çš„ã«ç‰¹å®šã§ããªã„ã®ã§ã™ã€‚

## æ¬¡ã®ã‚¿ãƒ¼ãƒ³ã¸ã®å…¥åŠ›

[`result.to_input_list()`][agents.result.RunResultBase.to_input_list] ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œä¸­ã«ç”Ÿæˆã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’å…ƒã®å…¥åŠ›ã«é€£çµã—ãŸå…¥åŠ›ãƒªã‚¹ãƒˆã¸å¤‰æ›ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã®å‡ºåŠ›ã‚’åˆ¥ã®å®Ÿè¡Œã¸æ¸¡ã—ãŸã‚Šã€ãƒ«ãƒ¼ãƒ—ã§å®Ÿè¡Œã—ã¦æ¯å›æ–°ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’è¿½åŠ ã—ãŸã‚Šã™ã‚‹ã“ã¨ãŒå®¹æ˜“ã«ãªã‚Šã¾ã™ã€‚

## æœ€å¾Œã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

[`last_agent`][agents.result.RunResultBase.last_agent] ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã«ã¯ã€æœ€å¾Œã«å®Ÿè¡Œã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ ¼ç´ã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã£ã¦ã¯ã€æ¬¡å›ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã™ã‚‹éš›ã«ã“ã‚ŒãŒå½¹ç«‹ã¤ã“ã¨ãŒã‚ˆãã‚ã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€ãƒ•ãƒ­ãƒ³ãƒˆãƒ©ã‚¤ãƒ³ã®ãƒˆãƒªã‚¢ãƒ¼ã‚¸ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè¨€èªå°‚ç”¨ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãƒãƒ³ãƒ‰ã‚ªãƒ•ã™ã‚‹å ´åˆã€æœ€å¾Œã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¦ãŠãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã£ãŸã¨ãã«å†åˆ©ç”¨ã§ãã¾ã™ã€‚

## æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ 

[`new_items`][agents.result.RunResultBase.new_items] ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã«ã¯ã€å®Ÿè¡Œä¸­ã«ç”Ÿæˆã•ã‚ŒãŸæ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ãŒå«ã¾ã‚Œã¾ã™ã€‚ã“ã‚Œã‚‰ã®ã‚¢ã‚¤ãƒ†ãƒ ã¯ [`RunItem`][agents.items.RunItem] ã§ã™ã€‚RunItem ã¯ã€ LLM ãŒç”Ÿæˆã—ãŸ  raw  ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ©ãƒƒãƒ—ã—ã¾ã™ã€‚

-   [`MessageOutputItem`][agents.items.MessageOutputItem] â€”  LLM ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¤ºã—ã¾ã™ã€‚  raw  ã‚¢ã‚¤ãƒ†ãƒ ã¯ç”Ÿæˆã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã™ã€‚
-   [`HandoffCallItem`][agents.items.HandoffCallItem] â€”  LLM ãŒãƒãƒ³ãƒ‰ã‚ªãƒ• ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã—ãŸã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚  raw  ã‚¢ã‚¤ãƒ†ãƒ ã¯  LLM ã‹ã‚‰ã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚¢ã‚¤ãƒ†ãƒ ã§ã™ã€‚
-   [`HandoffOutputItem`][agents.items.HandoffOutputItem] â€” ãƒãƒ³ãƒ‰ã‚ªãƒ•ãŒç™ºç”Ÿã—ãŸã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚  raw  ã‚¢ã‚¤ãƒ†ãƒ ã¯ãƒãƒ³ãƒ‰ã‚ªãƒ• ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã«å¯¾ã™ã‚‹ãƒ„ãƒ¼ãƒ«å¿œç­”ã§ã™ã€‚ã¾ãŸã€ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰é€ä¿¡å…ƒ / é€ä¿¡å…ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚‚ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚
-   [`ToolCallItem`][agents.items.ToolCallItem] â€”  LLM ãŒãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã—ãŸã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
-   [`ToolCallOutputItem`][agents.items.ToolCallOutputItem] â€” ãƒ„ãƒ¼ãƒ«ãŒå‘¼ã³å‡ºã•ã‚ŒãŸã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚  raw  ã‚¢ã‚¤ãƒ†ãƒ ã¯ãƒ„ãƒ¼ãƒ«å¿œç­”ã§ã™ã€‚ã¾ãŸã€ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰ãƒ„ãƒ¼ãƒ«å‡ºåŠ›ã«ã‚‚ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚
-   [`ReasoningItem`][agents.items.ReasoningItem] â€”  LLM ã‹ã‚‰ã®æ¨è«–ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç¤ºã—ã¾ã™ã€‚  raw  ã‚¢ã‚¤ãƒ†ãƒ ã¯ç”Ÿæˆã•ã‚ŒãŸæ¨è«–å†…å®¹ã§ã™ã€‚

## ãã®ä»–ã®æƒ…å ±

### ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«çµæœ

[`input_guardrail_results`][agents.result.RunResultBase.input_guardrail_results] ã¨ [`output_guardrail_results`][agents.result.RunResultBase.output_guardrail_results] ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã«ã¯ã€ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã®çµæœãŒå­˜åœ¨ã™ã‚‹å ´åˆã«æ ¼ç´ã•ã‚Œã¾ã™ã€‚ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«çµæœã«ã¯ã€ãƒ­ã‚°ã‚„ä¿å­˜ã‚’è¡Œã„ãŸã„æœ‰ç”¨ãªæƒ…å ±ãŒå«ã¾ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€ã“ã‚Œã‚‰ã‚’å‚ç…§ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚

### raw ãƒ¬ã‚¹ãƒãƒ³ã‚¹

[`raw_responses`][agents.result.RunResultBase.raw_responses] ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã«ã¯ã€ LLM ãŒç”Ÿæˆã—ãŸ [`ModelResponse`][agents.items.ModelResponse] ãŒæ ¼ç´ã•ã‚Œã¾ã™ã€‚

### å…ƒã®å…¥åŠ›

[`input`][agents.result.RunResultBase.input] ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã«ã¯ã€`run` ãƒ¡ã‚½ãƒƒãƒ‰ã«æ¸¡ã—ãŸå…ƒã®å…¥åŠ›ãŒæ ¼ç´ã•ã‚Œã¾ã™ã€‚ã»ã¨ã‚“ã©ã®å ´åˆã¯å¿…è¦ã‚ã‚Šã¾ã›ã‚“ãŒã€å¿…è¦ã«å¿œã˜ã¦å‚ç…§ã§ãã‚‹ã‚ˆã†ã«ç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚
</file>

<file path="docs/ja/running_agents.md">
---
search:
  exclude: true
---
# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œ

`Runner` ã‚¯ãƒ©ã‚¹ [`Runner`][agents.run.Runner] ã‚’ä½¿ç”¨ã—ã¦ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚æ–¹æ³•ã¯ 3 ã¤ã‚ã‚Šã¾ã™ã€‚

1. éåŒæœŸã§å®Ÿè¡Œã—ã€[`RunResult`][agents.result.RunResult] ã‚’è¿”ã™ [`Runner.run()`][agents.run.Runner.run]  
2. åŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã§ã€å†…éƒ¨çš„ã«ã¯ `.run()` ã‚’å‘¼ã³å‡ºã™ [`Runner.run_sync()`][agents.run.Runner.run_sync]  
3. éåŒæœŸã§å®Ÿè¡Œã—ã€[`RunResultStreaming`][agents.result.RunResultStreaming] ã‚’è¿”ã™ [`Runner.run_streamed()`][agents.run.Runner.run_streamed]  
   LLM ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§å‘¼ã³å‡ºã—ã€å—ä¿¡ã—ãŸã‚¤ãƒ™ãƒ³ãƒˆã‚’é€æ¬¡ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° ã—ã¾ã™ã€‚

```python
from agents import Agent, Runner

async def main():
    agent = Agent(name="Assistant", instructions="You are a helpful assistant")

    result = await Runner.run(agent, "Write a haiku about recursion in programming.")
    print(result.final_output)
    # Code within the code,
    # Functions calling themselves,
    # Infinite loop's dance.
```

è©³ç´°ã¯ [çµæœã‚¬ã‚¤ãƒ‰](results.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—

`Runner` ã® run ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹éš›ã¯ã€é–‹å§‹ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã¨å…¥åŠ›ã‚’æ¸¡ã—ã¾ã™ã€‚å…¥åŠ›ã¯æ–‡å­—åˆ—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨è¦‹ãªã•ã‚Œã¾ã™ï¼‰ã¾ãŸã¯å…¥åŠ›é …ç›®ã®ãƒªã‚¹ãƒˆï¼ˆOpenAI Responses API ã®é …ç›®ï¼‰ã§ã™ã€‚

Runner ã¯ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

1. ç¾åœ¨ã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã¨ç¾åœ¨ã®å…¥åŠ›ã§ LLM ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚  
2. LLM ãŒå‡ºåŠ›ã‚’ç”Ÿæˆã—ã¾ã™ã€‚  
    1. `final_output` ãŒè¿”ã•ã‚ŒãŸå ´åˆã€ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†ã—ã¦çµæœã‚’è¿”ã—ã¾ã™ã€‚  
    2. ãƒãƒ³ãƒ‰ã‚ªãƒ• ãŒç™ºç”Ÿã—ãŸå ´åˆã€ç¾åœ¨ã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã¨å…¥åŠ›ã‚’æ›´æ–°ã—ã€ãƒ«ãƒ¼ãƒ—ã‚’å†å®Ÿè¡Œã—ã¾ã™ã€‚  
    3. ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã— ãŒã‚ã‚‹å ´åˆã€ãã‚Œã‚‰ã‚’å®Ÿè¡Œã—ã€çµæœã‚’è¿½åŠ ã—ã¦ãƒ«ãƒ¼ãƒ—ã‚’å†å®Ÿè¡Œã—ã¾ã™ã€‚  
3. æŒ‡å®šã—ãŸ `max_turns` ã‚’è¶…ãˆãŸå ´åˆã€[`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded] ä¾‹å¤–ã‚’é€å‡ºã—ã¾ã™ã€‚

!!! note
    LLM ã®å‡ºåŠ›ãŒã€Œæœ€çµ‚å‡ºåŠ›ã€ã¨è¦‹ãªã•ã‚Œã‚‹æ¡ä»¶ã¯ã€æœ›ã¾ã—ã„å‹ã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã§ã‚ã‚Šã€ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒãªã„ã“ã¨ã§ã™ã€‚

## ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°

ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€LLM ã®å®Ÿè¡Œä¸­ã« ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° ã‚¤ãƒ™ãƒ³ãƒˆã‚’å—ã‘å–ã‚Œã¾ã™ã€‚ã‚¹ãƒˆãƒªãƒ¼ãƒ å®Œäº†å¾Œã€[`RunResultStreaming`][agents.result.RunResultStreaming] ã«ã¯å®Ÿè¡Œã«é–¢ã™ã‚‹å®Œå…¨ãªæƒ…å ±ï¼ˆæ–°ã—ãç”Ÿæˆã•ã‚ŒãŸã™ã¹ã¦ã®å‡ºåŠ›ã‚’å«ã‚€ï¼‰ãŒæ ¼ç´ã•ã‚Œã¾ã™ã€‚`.stream_events()` ã‚’å‘¼ã³å‡ºã—ã¦ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° ã‚¤ãƒ™ãƒ³ãƒˆã‚’å–å¾—ã§ãã¾ã™ã€‚è©³ã—ãã¯ [ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¬ã‚¤ãƒ‰](streaming.md) ã‚’ã”è¦§ãã ã•ã„ã€‚

## Run è¨­å®š

`run_config` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã«ã‚ˆã‚Šã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã®ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šã‚’è¡Œãˆã¾ã™ã€‚

- [`model`][agents.run.RunConfig.model]: å„ Agent ã® `model` è¨­å®šã«é–¢ã‚ã‚‰ãšä½¿ç”¨ã™ã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ« LLM ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¾ã™ã€‚  
- [`model_provider`][agents.run.RunConfig.model_provider]: ãƒ¢ãƒ‡ãƒ«åã‚’è§£æ±ºã™ã‚‹ ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ OpenAI ã§ã™ã€‚  
- [`model_settings`][agents.run.RunConfig.model_settings]: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå›ºæœ‰è¨­å®šã‚’ä¸Šæ›¸ãã—ã¾ã™ã€‚ä¾‹ã¨ã—ã¦ã‚°ãƒ­ãƒ¼ãƒãƒ« `temperature` ã‚„ `top_p` ã®è¨­å®šãªã©ã€‚  
- [`input_guardrails`][agents.run.RunConfig.input_guardrails], [`output_guardrails`][agents.run.RunConfig.output_guardrails]: ã™ã¹ã¦ã®å®Ÿè¡Œã«é©ç”¨ã™ã‚‹å…¥åŠ› / å‡ºåŠ› ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ« ã®ãƒªã‚¹ãƒˆã€‚  
- [`handoff_input_filter`][agents.run.RunConfig.handoff_input_filter]: ãƒãƒ³ãƒ‰ã‚ªãƒ• ã«å…¥åŠ›ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã«é©ç”¨ã•ã‚Œã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«å…¥åŠ›ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€‚æ–°ã—ã„ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã¸é€ä¿¡ã•ã‚Œã‚‹å…¥åŠ›ã‚’ç·¨é›†ã§ãã¾ã™ã€‚è©³ç´°ã¯ [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚  
- [`tracing_disabled`][agents.run.RunConfig.tracing_disabled]: å®Ÿè¡Œå…¨ä½“ã® [ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°](tracing.md) ã‚’ç„¡åŠ¹åŒ–ã—ã¾ã™ã€‚  
- [`trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data]: ãƒˆãƒ¬ãƒ¼ã‚¹ã« LLM ã‚„ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®å…¥å‡ºåŠ›ãªã©ã€æ©Ÿå¾®ãªãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹ã‚’è¨­å®šã—ã¾ã™ã€‚  
- [`workflow_name`][agents.run.RunConfig.workflow_name], [`trace_id`][agents.run.RunConfig.trace_id], [`group_id`][agents.run.RunConfig.group_id]: å®Ÿè¡Œã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚° ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åã€ãƒˆãƒ¬ãƒ¼ã‚¹ IDã€ãƒˆãƒ¬ãƒ¼ã‚¹ ã‚°ãƒ«ãƒ¼ãƒ— ID ã‚’è¨­å®šã—ã¾ã™ã€‚å°‘ãªãã¨ã‚‚ `workflow_name` ã®è¨­å®šã‚’æ¨å¥¨ã—ã¾ã™ã€‚`group_id` ã‚’è¨­å®šã™ã‚‹ã¨ã€è¤‡æ•°ã®å®Ÿè¡Œã«ã¾ãŸãŒã‚‹ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ãƒªãƒ³ã‚¯ã§ãã¾ã™ã€‚  
- [`trace_metadata`][agents.run.RunConfig.trace_metadata]: ã™ã¹ã¦ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã«ä»˜ä¸ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€‚  

## ä¼šè©± / ãƒãƒ£ãƒƒãƒˆã‚¹ãƒ¬ãƒƒãƒ‰

ã„ãšã‚Œã‹ã® run ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã¨ã€1 ã¤ä»¥ä¸Šã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ãŒå®Ÿè¡Œã•ã‚Œï¼ˆã¤ã¾ã‚Š 1 ã¤ä»¥ä¸Šã® LLM å‘¼ã³å‡ºã—ãŒè¡Œã‚ã‚Œï¼‰ã€ãƒãƒ£ãƒƒãƒˆä¼šè©±ã® 1 ã¤ã®è«–ç†ã‚¿ãƒ¼ãƒ³ã‚’è¡¨ã—ã¾ã™ã€‚ä¾‹:

1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¿ãƒ¼ãƒ³: ãƒ¦ãƒ¼ã‚¶ãƒ¼ ãŒãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›  
2. Runner å®Ÿè¡Œ: æœ€åˆã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ãŒ LLM ã‚’å‘¼ã³å‡ºã—ã€ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã—ã€2 ç•ªç›®ã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã¸ãƒãƒ³ãƒ‰ã‚ªãƒ•ã€‚2 ç•ªç›®ã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ãŒã•ã‚‰ã«ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã—ã€æœ€çµ‚å‡ºåŠ›ã‚’ç”Ÿæˆã€‚  

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã®çµ‚äº†æ™‚ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ ã«ä½•ã‚’è¡¨ç¤ºã™ã‚‹ã‹ã¯è‡ªç”±ã§ã™ã€‚ãŸã¨ãˆã°ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ãŒç”Ÿæˆã—ãŸã™ã¹ã¦ã®æ–°ã—ã„é …ç›®ã‚’è¡¨ç¤ºã™ã‚‹ã€ã¾ãŸã¯æœ€çµ‚å‡ºåŠ›ã®ã¿ã‚’è¡¨ç¤ºã™ã‚‹ç­‰ã§ã™ã€‚ã„ãšã‚Œã®å ´åˆã§ã‚‚ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ ãŒãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—è³ªå•ã‚’ã—ãŸã‚‰ã€å†åº¦ run ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã›ã¾ã™ã€‚

æ¬¡ã‚¿ãƒ¼ãƒ³ã®å…¥åŠ›ã¯ã€åŸºåº•ã‚¯ãƒ©ã‚¹ [`RunResultBase.to_input_list()`][agents.result.RunResultBase.to_input_list] ã‚’ä½¿ç”¨ã—ã¦å–å¾—ã§ãã¾ã™ã€‚

```python
async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    with trace(workflow_name="Conversation", group_id=thread_id):
        # First turn
        result = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
        print(result.final_output)
        # San Francisco

        # Second turn
        new_input = result.to_input_list() + [{"role": "user", "content": "What state is it in?"}]
        result = await Runner.run(agent, new_input)
        print(result.final_output)
        # California
```

## ä¾‹å¤–

ç‰¹å®šã®çŠ¶æ³ã§ SDK ã¯ä¾‹å¤–ã‚’é€å‡ºã—ã¾ã™ã€‚å®Œå…¨ãªä¸€è¦§ã¯ [`agents.exceptions`][] ã«ã‚ã‚Šã¾ã™ã€‚æ¦‚è¦ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã§ã™ã€‚

- [`AgentsException`][agents.exceptions.AgentsException]: SDK ãŒé€å‡ºã™ã‚‹ã™ã¹ã¦ã®ä¾‹å¤–ã®åŸºåº•ã‚¯ãƒ©ã‚¹  
- [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded]: å®Ÿè¡ŒãŒ `max_turns` ã‚’è¶…ãˆãŸå ´åˆã«é€å‡º  
- [`ModelBehaviorError`][agents.exceptions.ModelBehaviorError]: ãƒ¢ãƒ‡ãƒ«ãŒä¸æ­£ãªå‡ºåŠ›ï¼ˆä¾‹: JSON å½¢å¼é•åã€å­˜åœ¨ã—ãªã„ãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—ãªã©ï¼‰ã‚’ç”Ÿæˆã—ãŸå ´åˆã«é€å‡º  
- [`UserError`][agents.exceptions.UserError]: SDK ã®ä½¿ç”¨æ–¹æ³•ã«èª¤ã‚ŠãŒã‚ã‚‹å ´åˆã«é€å‡º  
- [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered], [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered]: [ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«](guardrails.md) ãŒç™ºç«ã—ãŸå ´åˆã«é€å‡º
</file>

<file path="docs/ja/streaming.md">
---
search:
  exclude: true
---
# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°

ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã®å®Ÿè¡ŒãŒé€²è¡Œã™ã‚‹ã«ã¤ã‚Œã¦ç™ºç”Ÿã™ã‚‹æ›´æ–°ã‚’è³¼èª­ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¨ãƒ³ãƒ‰ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é€²æ—çŠ¶æ³ã‚„éƒ¨åˆ†çš„ãªå¿œç­”ã‚’è¡¨ç¤ºã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚

ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’è¡Œã†ã«ã¯ã€ [`Runner.run_streamed()`][agents.run.Runner.run_streamed] ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Š [`RunResultStreaming`][agents.result.RunResultStreaming] ãŒè¿”ã•ã‚Œã¾ã™ã€‚ç¶šã„ã¦ `result.stream_events()` ã‚’å‘¼ã³å‡ºã™ã¨ã€å¾Œè¿°ã™ã‚‹ [`StreamEvent`][agents.stream_events.StreamEvent] ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®éåŒæœŸã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å–å¾—ã§ãã¾ã™ã€‚

## raw response ã‚¤ãƒ™ãƒ³ãƒˆ

[`RawResponsesStreamEvent`][agents.stream_events.RawResponsesStreamEvent] ã¯ã€ LLM ã‹ã‚‰ç›´æ¥æ¸¡ã•ã‚Œã‚‹ raw ãªã‚¤ãƒ™ãƒ³ãƒˆã§ã™ã€‚ã“ã‚Œã‚‰ã¯ OpenAIÂ ResponsesÂ API å½¢å¼ã§ã‚ã‚Šã€å„ã‚¤ãƒ™ãƒ³ãƒˆã«ã¯ `response.created` ã‚„ `response.output_text.delta` ãªã©ã® type ã¨ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¾ã™ã€‚ç”Ÿæˆã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å³åº§ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã—ãŸã„å ´åˆã«ä¾¿åˆ©ã§ã™ã€‚

ãŸã¨ãˆã°ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ LLM ãŒç”Ÿæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«å‡ºåŠ›ã—ã¾ã™ã€‚

```python
import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner

async def main():
    agent = Agent(
        name="Joker",
        instructions="You are a helpful assistant.",
    )

    result = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            print(event.data.delta, end="", flush=True)


if __name__ == "__main__":
    asyncio.run(main())
```

## Run item ã‚¤ãƒ™ãƒ³ãƒˆã¨ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ã‚¤ãƒ™ãƒ³ãƒˆ

[`RunItemStreamEvent`][agents.stream_events.RunItemStreamEvent] ã¯ã€ã‚ˆã‚Šé«˜ãƒ¬ãƒ™ãƒ«ãªã‚¤ãƒ™ãƒ³ãƒˆã§ã™ã€‚ã‚¢ã‚¤ãƒ†ãƒ ãŒå®Œå…¨ã«ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’é€šçŸ¥ã™ã‚‹ãŸã‚ã€ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã§ã¯ãªãã€Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç”Ÿæˆã•ã‚ŒãŸã€ã€Œãƒ„ãƒ¼ãƒ«ãŒå®Ÿè¡Œã•ã‚ŒãŸã€ã¨ã„ã£ãŸãƒ¬ãƒ™ãƒ«ã§é€²æ—ã‚’ãƒ—ãƒƒã‚·ãƒ¥ã§ãã¾ã™ã€‚åŒæ§˜ã«ã€ [`AgentUpdatedStreamEvent`][agents.stream_events.AgentUpdatedStreamEvent] ã¯ãƒãƒ³ãƒ‰ã‚ªãƒ•ãªã©ã§ç¾åœ¨ã® ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ãŒå¤‰ã‚ã£ãŸéš›ã«æ›´æ–°ã‚’æä¾›ã—ã¾ã™ã€‚

ãŸã¨ãˆã°ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ raw ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç„¡è¦–ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸æ›´æ–°ã®ã¿ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã—ã¾ã™ã€‚

```python
import asyncio
import random
from agents import Agent, ItemHelpers, Runner, function_tool

@function_tool
def how_many_jokes() -> int:
    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="Joker",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],
    )

    result = Runner.run_streamed(
        agent,
        input="Hello",
    )
    print("=== Run starting ===")

    async for event in result.stream_events():
        # We'll ignore the raw responses event deltas
        if event.type == "raw_response_event":
            continue
        # When the agent updates, print that
        elif event.type == "agent_updated_stream_event":
            print(f"Agent updated: {event.new_agent.name}")
            continue
        # When items are generated, print them
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}")
            else:
                pass  # Ignore other event types

    print("=== Run complete ===")


if __name__ == "__main__":
    asyncio.run(main())
```
</file>

<file path="docs/ja/tools.md">
---
search:
  exclude: true
---
# ãƒ„ãƒ¼ãƒ«

ãƒ„ãƒ¼ãƒ«ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚ãŸã¨ãˆã°ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã€ã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡Œã€å¤–éƒ¨ API ã®å‘¼ã³å‡ºã—ã€ã•ã‚‰ã«ã¯ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿æ“ä½œãªã©ã§ã™ã€‚Agents SDK ã«ã¯æ¬¡ã® 3 ç¨®é¡ã®ãƒ„ãƒ¼ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚

-   ãƒ›ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«: ã“ã‚Œã‚‰ã¯ LLM ã‚µãƒ¼ãƒãƒ¼ä¸Šã§ AI ãƒ¢ãƒ‡ãƒ«ã¨ä¸€ç·’ã«å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚OpenAI ã¯ retrievalã€Web æ¤œç´¢ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿æ“ä½œã‚’ãƒ›ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«ã¨ã—ã¦æä¾›ã—ã¦ã„ã¾ã™ã€‚
-   é–¢æ•°å‘¼ã³å‡ºã—: ä»»æ„ã® Python é–¢æ•°ã‚’ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦åˆ©ç”¨ã§ãã¾ã™ã€‚
-   ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: ãƒãƒ³ãƒ‰ã‚ªãƒ•ã›ãšã«ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã™ã“ã¨ãŒã§ãã¾ã™ã€‚

## ãƒ›ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«

OpenAI ã¯ [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] ã‚’ä½¿ç”¨ã™ã‚‹éš›ã«ã€ã„ãã¤ã‹ã®çµ„ã¿è¾¼ã¿ãƒ„ãƒ¼ãƒ«ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

-   [`WebSearchTool`][agents.tool.WebSearchTool] ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã« Web æ¤œç´¢ã‚’è¡Œã‚ã›ã¾ã™ã€‚
-   [`FileSearchTool`][agents.tool.FileSearchTool] ã¯ OpenAI ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚
-   [`ComputerTool`][agents.tool.ComputerTool] ã¯ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿æ“ä½œã‚¿ã‚¹ã‚¯ã‚’è‡ªå‹•åŒ–ã—ã¾ã™ã€‚

```python
from agents import Agent, FileSearchTool, Runner, WebSearchTool

agent = Agent(
    name="Assistant",
    tools=[
        WebSearchTool(),
        FileSearchTool(
            max_num_results=3,
            vector_store_ids=["VECTOR_STORE_ID"],
        ),
    ],
)

async def main():
    result = await Runner.run(agent, "Which coffee shop should I go to, taking into account my preferences and the weather today in SF?")
    print(result.final_output)
```

## é–¢æ•°ãƒ„ãƒ¼ãƒ«

ä»»æ„ã® Python é–¢æ•°ã‚’ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ä½¿ç”¨ã§ãã¾ã™ã€‚Agents SDK ãŒè‡ªå‹•çš„ã«è¨­å®šã‚’è¡Œã„ã¾ã™ã€‚

-   ãƒ„ãƒ¼ãƒ«ã®åå‰ã¯ Python é–¢æ•°ã®åå‰ã«ãªã‚Šã¾ã™ï¼ˆä»»æ„ã§åå‰ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ï¼‰
-   ãƒ„ãƒ¼ãƒ«ã®èª¬æ˜ã¯é–¢æ•°ã® docstring ã‹ã‚‰å–å¾—ã•ã‚Œã¾ã™ï¼ˆä»»æ„ã§èª¬æ˜ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ï¼‰
-   é–¢æ•°ã®å¼•æ•°ã‹ã‚‰è‡ªå‹•çš„ã«å…¥åŠ›ã‚¹ã‚­ãƒ¼ãƒã‚’ç”Ÿæˆã—ã¾ã™
-   å„å…¥åŠ›ã®èª¬æ˜ã¯ã€ç„¡åŠ¹åŒ–ã—ãªã„é™ã‚Š docstring ã‹ã‚‰å–å¾—ã•ã‚Œã¾ã™

Python ã® `inspect` ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦é–¢æ•°ã‚·ã‚°ãƒãƒãƒ£ã‚’æŠ½å‡ºã—ã€[`griffe`](https://mkdocstrings.github.io/griffe/) ã§ docstring ã‚’è§£æã—ã€`pydantic` ã§ã‚¹ã‚­ãƒ¼ãƒã‚’ä½œæˆã—ã¾ã™ã€‚

```python
import json

from typing_extensions import TypedDict, Any

from agents import Agent, FunctionTool, RunContextWrapper, function_tool


class Location(TypedDict):
    lat: float
    long: float

@function_tool  # (1)!
async def fetch_weather(location: Location) -> str:
    # (2)!
    """Fetch the weather for a given location.

    Args:
        location: The location to fetch the weather for.
    """
    # In real life, we'd fetch the weather from a weather API
    return "sunny"


@function_tool(name_override="fetch_data")  # (3)!
def read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -> str:
    """Read the contents of a file.

    Args:
        path: The path to the file to read.
        directory: The directory to read the file from.
    """
    # In real life, we'd read the file from the file system
    return "<file contents>"


agent = Agent(
    name="Assistant",
    tools=[fetch_weather, read_file],  # (4)!
)

for tool in agent.tools:
    if isinstance(tool, FunctionTool):
        print(tool.name)
        print(tool.description)
        print(json.dumps(tool.params_json_schema, indent=2))
        print()

```

1.  é–¢æ•°ã®å¼•æ•°ã«ã¯ä»»æ„ã® Python å‹ã‚’ä½¿ç”¨ã§ãã€åŒæœŸãƒ»éåŒæœŸã©ã¡ã‚‰ã®é–¢æ•°ã‚‚åˆ©ç”¨ã§ãã¾ã™ã€‚
2.  docstring ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ãƒ„ãƒ¼ãƒ«ã¨å¼•æ•°ã®èª¬æ˜ã‚’å–å¾—ã—ã¾ã™ã€‚
3.  é–¢æ•°ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ `context` ã‚’å—ã‘å–ã‚Œã¾ã™ï¼ˆæœ€åˆã®å¼•æ•°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰ã€‚ãƒ„ãƒ¼ãƒ«åã€èª¬æ˜ã€docstring ã®ã‚¹ã‚¿ã‚¤ãƒ«ãªã©ã‚’ä¸Šæ›¸ãè¨­å®šã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚
4.  ãƒ‡ã‚³ãƒ¬ãƒ¼ãƒˆã•ã‚ŒãŸé–¢æ•°ã‚’ãƒ„ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆã«æ¸¡ã—ã¦ãã ã•ã„ã€‚

??? note "å±•é–‹ã—ã¦å‡ºåŠ›ã‚’ç¢ºèª"

    ```
    fetch_weather
    Fetch the weather for a given location.
    {
    "$defs": {
      "Location": {
        "properties": {
          "lat": {
            "title": "Lat",
            "type": "number"
          },
          "long": {
            "title": "Long",
            "type": "number"
          }
        },
        "required": [
          "lat",
          "long"
        ],
        "title": "Location",
        "type": "object"
      }
    },
    "properties": {
      "location": {
        "$ref": "#/$defs/Location",
        "description": "The location to fetch the weather for."
      }
    },
    "required": [
      "location"
    ],
    "title": "fetch_weather_args",
    "type": "object"
    }

    fetch_data
    Read the contents of a file.
    {
    "properties": {
      "path": {
        "description": "The path to the file to read.",
        "title": "Path",
        "type": "string"
      },
      "directory": {
        "anyOf": [
          {
            "type": "string"
          },
          {
            "type": "null"
          }
        ],
        "default": null,
        "description": "The directory to read the file from.",
        "title": "Directory"
      }
    },
    "required": [
      "path"
    ],
    "title": "fetch_data_args",
    "type": "object"
    }
    ```

### ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°ãƒ„ãƒ¼ãƒ«

Python é–¢æ•°ã‚’ãã®ã¾ã¾ãƒ„ãƒ¼ãƒ«ã«ã—ãŸããªã„å ´åˆã¯ã€[`FunctionTool`][agents.tool.FunctionTool] ã‚’ç›´æ¥ä½œæˆã§ãã¾ã™ã€‚æ¬¡ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

-   `name`
-   `description`
-   `params_json_schema`ï¼ˆå¼•æ•°ã® JSON ã‚¹ã‚­ãƒ¼ãƒï¼‰
-   `on_invoke_tool`ï¼ˆcontext ã¨å¼•æ•°ã® JSON æ–‡å­—åˆ—ã‚’å—ã‘å–ã‚Šã€ãƒ„ãƒ¼ãƒ«ã®å‡ºåŠ›ã‚’æ–‡å­—åˆ—ã§è¿”ã™ async é–¢æ•°ï¼‰

```python
from typing import Any

from pydantic import BaseModel

from agents import RunContextWrapper, FunctionTool



def do_some_work(data: str) -> str:
    return "done"


class FunctionArgs(BaseModel):
    username: str
    age: int


async def run_function(ctx: RunContextWrapper[Any], args: str) -> str:
    parsed = FunctionArgs.model_validate_json(args)
    return do_some_work(data=f"{parsed.username} is {parsed.age} years old")


tool = FunctionTool(
    name="process_user",
    description="Processes extracted user data",
    params_json_schema=FunctionArgs.model_json_schema(),
    on_invoke_tool=run_function,
)
```

### å¼•æ•°ã¨ docstring ã®è‡ªå‹•è§£æ

å‰è¿°ã®ã¨ãŠã‚Šã€é–¢æ•°ã‚·ã‚°ãƒãƒãƒ£ã‚’è‡ªå‹•è§£æã—ã¦ãƒ„ãƒ¼ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒã‚’ç”Ÿæˆã—ã€docstring ã‚’è§£æã—ã¦ãƒ„ãƒ¼ãƒ«ãŠã‚ˆã³å€‹åˆ¥å¼•æ•°ã®èª¬æ˜ã‚’æŠ½å‡ºã—ã¾ã™ã€‚ä¸»ãªæ³¨æ„ç‚¹ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚

1. ã‚·ã‚°ãƒãƒãƒ£è§£æã¯ `inspect` ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§è¡Œã„ã¾ã™ã€‚å‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”¨ã„ã¦å¼•æ•°ã®å‹ã‚’èªè­˜ã—ã€Pydantic ãƒ¢ãƒ‡ãƒ«ã‚’å‹•çš„ã«æ§‹ç¯‰ã—ã¦å…¨ä½“ã®ã‚¹ã‚­ãƒ¼ãƒã‚’è¡¨ç¾ã—ã¾ã™ã€‚Python ã®åŸºæœ¬å‹ã€Pydantic ãƒ¢ãƒ‡ãƒ«ã€TypedDict ãªã©ã»ã¨ã‚“ã©ã®å‹ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚
2. `griffe` ã‚’ä½¿ç”¨ã—ã¦ docstring ã‚’è§£æã—ã¾ã™ã€‚å¯¾å¿œã™ã‚‹ docstring å½¢å¼ã¯ `google`ã€`sphinx`ã€`numpy` ã§ã™ã€‚å½¢å¼ã¯è‡ªå‹•æ¤œå‡ºã‚’è©¦ã¿ã¾ã™ãŒã€`function_tool` å‘¼ã³å‡ºã—æ™‚ã«æ˜ç¤ºçš„ã«æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚`use_docstring_info` ã‚’ `False` ã«è¨­å®šã™ã‚‹ã¨ docstring è§£æã‚’ç„¡åŠ¹åŒ–ã§ãã¾ã™ã€‚

ã‚¹ã‚­ãƒ¼ãƒæŠ½å‡ºã®ã‚³ãƒ¼ãƒ‰ã¯ [`agents.function_schema`][] ã«ã‚ã‚Šã¾ã™ã€‚

## ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

ä¸€éƒ¨ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã¯ã€ãƒãƒ³ãƒ‰ã‚ªãƒ•ã›ãšã«ä¸­å¤®ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè¤‡æ•°ã®å°‚é–€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ãã®ã‚ˆã†ãªå ´åˆã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«åŒ–ã§ãã¾ã™ã€‚

```python
from agents import Agent, Runner
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You translate the user's message to Spanish",
)

french_agent = Agent(
    name="French agent",
    instructions="You translate the user's message to French",
)

orchestrator_agent = Agent(
    name="orchestrator_agent",
    instructions=(
        "You are a translation agent. You use the tools given to you to translate."
        "If asked for multiple translations, you call the relevant tools."
    ),
    tools=[
        spanish_agent.as_tool(
            tool_name="translate_to_spanish",
            tool_description="Translate the user's message to Spanish",
        ),
        french_agent.as_tool(
            tool_name="translate_to_french",
            tool_description="Translate the user's message to French",
        ),
    ],
)

async def main():
    result = await Runner.run(orchestrator_agent, input="Say 'Hello, how are you?' in Spanish.")
    print(result.final_output)
```

### ãƒ„ãƒ¼ãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

`agent.as_tool` é–¢æ•°ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç°¡å˜ã«ãƒ„ãƒ¼ãƒ«åŒ–ã™ã‚‹ãŸã‚ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ã§ã™ã€‚ãŸã ã—ã€ã™ã¹ã¦ã®è¨­å®šã«å¯¾å¿œã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆä¾‹: `max_turns` ã¯è¨­å®šä¸å¯ï¼‰ã€‚é«˜åº¦ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã§ã¯ã€ãƒ„ãƒ¼ãƒ«å®Ÿè£…å†…ã§ `Runner.run` ã‚’ç›´æ¥ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

```python
@function_tool
async def run_my_agent() -> str:
  """A tool that runs the agent with custom configs".

    agent = Agent(name="My agent", instructions="...")

    result = await Runner.run(
        agent,
        input="...",
        max_turns=5,
        run_config=...
    )

    return str(result.final_output)
```

## é–¢æ•°ãƒ„ãƒ¼ãƒ«ã§ã®ã‚¨ãƒ©ãƒ¼å‡¦ç†

`@function_tool` ã§é–¢æ•°ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆã™ã‚‹éš›ã€`failure_error_function` ã‚’æ¸¡ã›ã¾ã™ã€‚ã“ã‚Œã¯ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒå¤±æ•—ã—ãŸå ´åˆã« LLM ã¸è¿”ã™ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ã§ã™ã€‚

-   ä½•ã‚‚æŒ‡å®šã—ãªã„å ´åˆã€`default_tool_error_function` ãŒå®Ÿè¡Œã•ã‚Œã€LLM ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã‚’ä¼ãˆã¾ã™ã€‚
-   ç‹¬è‡ªã®ã‚¨ãƒ©ãƒ¼é–¢æ•°ã‚’æ¸¡ã—ãŸå ´åˆã¯ãã¡ã‚‰ãŒå®Ÿè¡Œã•ã‚Œã€ãã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒ LLM ã¸é€ä¿¡ã•ã‚Œã¾ã™ã€‚
-   æ˜ç¤ºçš„ã« `None` ã‚’æ¸¡ã™ã¨ã€ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—æ™‚ã®ã‚¨ãƒ©ãƒ¼ã¯å†é€å‡ºã•ã‚Œã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ãŒç„¡åŠ¹ãª JSON ã‚’ç”Ÿæˆã—ãŸå ´åˆã¯ `ModelBehaviorError`ã€ã‚³ãƒ¼ãƒ‰ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãŸå ´åˆã¯ `UserError` ãªã©ã«ãªã‚Šã¾ã™ã€‚

`FunctionTool` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ‰‹å‹•ã§ä½œæˆã™ã‚‹å ´åˆã¯ã€`on_invoke_tool` é–¢æ•°å†…ã§ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
</file>

<file path="docs/ja/tracing.md">
---
search:
  exclude: true
---
# ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

Agents SDK ã«ã¯ãƒ“ãƒ«ãƒˆã‚¤ãƒ³ã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°æ©Ÿèƒ½ãŒã‚ã‚Šã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œä¸­ã«ç™ºç”Ÿã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆâ€•â€• LLM ç”Ÿæˆã€ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã€ãƒãƒ³ãƒ‰ã‚ªãƒ•ã€ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã€ã•ã‚‰ã«ã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ™ãƒ³ãƒˆã¾ã§â€•â€•ã‚’ç¶²ç¾…çš„ã«è¨˜éŒ²ã—ã¾ã™ã€‚é–‹ç™ºæ™‚ã¨æœ¬ç•ªç’°å¢ƒã®ä¸¡æ–¹ã§ [Traces dashboard](https://platform.openai.com/traces) ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ»å¯è¦–åŒ–ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã§ãã¾ã™ã€‚

!!!note

    ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹ã§ã™ã€‚ç„¡åŠ¹åŒ–ã™ã‚‹æ–¹æ³•ã¯æ¬¡ã® 2 ã¤ã§ã™:

    1. ç’°å¢ƒå¤‰æ•° `OPENAI_AGENTS_DISABLE_TRACING=1` ã‚’è¨­å®šã—ã¦ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ç„¡åŠ¹åŒ–ã™ã‚‹  
    2. å˜ä¸€ã®å®Ÿè¡Œã«å¯¾ã—ã¦ã¯ [`agents.run.RunConfig.tracing_disabled`][] ã‚’ `True` ã«è¨­å®šã™ã‚‹

***OpenAI ã® API ã‚’ Zero Data Retention (ZDR) ãƒãƒªã‚·ãƒ¼ã§åˆ©ç”¨ã—ã¦ã„ã‚‹çµ„ç¹”ã§ã¯ã€ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚***

## ãƒˆãƒ¬ãƒ¼ã‚¹ã¨ã‚¹ãƒ‘ãƒ³

-   **ãƒˆãƒ¬ãƒ¼ã‚¹** ã¯ 1 åº¦ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ã‚’è¡¨ã—ã¾ã™ã€‚è¤‡æ•°ã®ã‚¹ãƒ‘ãƒ³ã§æ§‹æˆã•ã‚Œã€æ¬¡ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’æŒã¡ã¾ã™:
    -   `workflow_name`: è«–ç†çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¾ãŸã¯ã‚¢ãƒ—ãƒªåã€‚ä¾‹: ã€ŒCode generationã€ã‚„ã€ŒCustomer serviceã€
    -   `trace_id`: ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ä¸€æ„ã«è­˜åˆ¥ã™ã‚‹ IDã€‚æŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã™ã€‚å½¢å¼ã¯ `trace_<32_alphanumeric>` ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    -   `group_id`: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ã‚°ãƒ«ãƒ¼ãƒ— IDã€‚ä¼šè©±å†…ã®è¤‡æ•°ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’é–¢é€£ä»˜ã‘ã¾ã™ã€‚ãŸã¨ãˆã°ãƒãƒ£ãƒƒãƒˆã‚¹ãƒ¬ãƒƒãƒ‰ ID ãªã©ã€‚
    -   `disabled`: `True` ã®å ´åˆã€ã“ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã¯è¨˜éŒ²ã•ã‚Œã¾ã›ã‚“ã€‚
    -   `metadata`: ãƒˆãƒ¬ãƒ¼ã‚¹ã«ä»˜éšã™ã‚‹ä»»æ„ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€‚
-   **ã‚¹ãƒ‘ãƒ³** ã¯é–‹å§‹æ™‚åˆ»ã¨çµ‚äº†æ™‚åˆ»ã‚’æŒã¤å€‹ã€…ã®å‡¦ç†ã‚’è¡¨ã—ã¾ã™ã€‚ã‚¹ãƒ‘ãƒ³ã¯ä»¥ä¸‹ã‚’ä¿æŒã—ã¾ã™:
    -   `started_at` ã¨ `ended_at` ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    -   æ‰€å±ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ç¤ºã™ `trace_id`
    -   è¦ªã‚¹ãƒ‘ãƒ³ã‚’æŒ‡ã™ `parent_id` (å­˜åœ¨ã™ã‚‹å ´åˆ)
    -   ã‚¹ãƒ‘ãƒ³ã«é–¢ã™ã‚‹æƒ…å ±ã‚’æ ¼ç´ã™ã‚‹ `span_data`ã€‚ãŸã¨ãˆã° `AgentSpanData` ã«ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæƒ…å ±ãŒã€`GenerationSpanData` ã«ã¯ LLM ç”Ÿæˆæƒ…å ±ãŒå«ã¾ã‚Œã¾ã™ã€‚

## ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ SDK ã¯ä»¥ä¸‹ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã¾ã™:

-   `Runner.{run, run_sync, run_streamed}()` å…¨ä½“ã‚’ `trace()` ã§ãƒ©ãƒƒãƒ—
-   ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå®Ÿè¡Œã•ã‚Œã‚‹ãŸã³ã« `agent_span()` ã§ãƒ©ãƒƒãƒ—
-   LLM ç”Ÿæˆã‚’ `generation_span()` ã§ãƒ©ãƒƒãƒ—
-   é–¢æ•°ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’ `function_span()` ã§ãƒ©ãƒƒãƒ—
-   ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã‚’ `guardrail_span()` ã§ãƒ©ãƒƒãƒ—
-   ãƒãƒ³ãƒ‰ã‚ªãƒ•ã‚’ `handoff_span()` ã§ãƒ©ãƒƒãƒ—
-   éŸ³å£°å…¥åŠ› (speechâ€‘toâ€‘text) ã‚’ `transcription_span()` ã§ãƒ©ãƒƒãƒ—
-   éŸ³å£°å‡ºåŠ› (textâ€‘toâ€‘speech) ã‚’ `speech_span()` ã§ãƒ©ãƒƒãƒ—
-   é–¢é€£ã™ã‚‹éŸ³å£°ã‚¹ãƒ‘ãƒ³ã¯ `speech_group_span()` ã®ä¸‹ã«ãƒã‚¹ãƒˆã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™

ãƒˆãƒ¬ãƒ¼ã‚¹åã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã€ŒAgent traceã€ã§ã™ã€‚`trace` ã‚’ä½¿ç”¨ã—ã¦æŒ‡å®šã—ãŸã‚Šã€[`RunConfig`][agents.run.RunConfig] ã§åå‰ã‚„ãã®ä»–ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’è¨­å®šã§ãã¾ã™ã€‚

ã•ã‚‰ã« [ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼](#custom-tracing-processors) ã‚’è¨­å®šã—ã¦ã€ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’åˆ¥ã®é€ä¿¡å…ˆã«å‡ºåŠ›ï¼ˆç½®ãæ›ãˆã¾ãŸã¯è¿½åŠ ï¼‰ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚

## ä¸Šä½ãƒ¬ãƒ™ãƒ«ã®ãƒˆãƒ¬ãƒ¼ã‚¹

è¤‡æ•°å›ã® `run()` å‘¼ã³å‡ºã—ã‚’ 1 ã¤ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã«ã¾ã¨ã‚ãŸã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ãã®å ´åˆã€ã‚³ãƒ¼ãƒ‰å…¨ä½“ã‚’ `trace()` ã§ãƒ©ãƒƒãƒ—ã—ã¾ã™ã€‚

```python
from agents import Agent, Runner, trace

async def main():
    agent = Agent(name="Joke generator", instructions="Tell funny jokes.")

    with trace("Joke workflow"): # (1)!
        first_result = await Runner.run(agent, "Tell me a joke")
        second_result = await Runner.run(agent, f"Rate this joke: {first_result.final_output}")
        print(f"Joke: {first_result.final_output}")
        print(f"Rating: {second_result.final_output}")
```

1. `with trace()` ã§ 2 ã¤ã® `Runner.run` å‘¼ã³å‡ºã—ã‚’ãƒ©ãƒƒãƒ—ã—ã¦ã„ã‚‹ãŸã‚ã€ãã‚Œãã‚ŒãŒå€‹åˆ¥ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ä½œæˆã›ãšã€å…¨ä½“ã§ 1 ã¤ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã«ãªã‚Šã¾ã™ã€‚

## ãƒˆãƒ¬ãƒ¼ã‚¹ã®ä½œæˆ

[`trace()`][agents.tracing.trace] é–¢æ•°ã‚’ä½¿ã£ã¦ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ä½œæˆã§ãã¾ã™ã€‚é–‹å§‹ã¨çµ‚äº†ãŒå¿…è¦ã§ã€æ–¹æ³•ã¯ 2 ã¤ã‚ã‚Šã¾ã™ã€‚

1. **æ¨å¥¨**: `with trace(...) as my_trace` ã®ã‚ˆã†ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã€‚é–‹å§‹ã¨çµ‚äº†ãŒè‡ªå‹•ã§è¡Œã‚ã‚Œã¾ã™ã€‚  
2. [`trace.start()`][agents.tracing.Trace.start] ã¨ [`trace.finish()`][agents.tracing.Trace.finish] ã‚’æ‰‹å‹•ã§å‘¼ã³å‡ºã™ã€‚

ç¾åœ¨ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã¯ Python ã® [`contextvar`](https://docs.python.org/3/library/contextvars.html) ã§ç®¡ç†ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ä¸¦è¡Œå‡¦ç†ã§ã‚‚è‡ªå‹•ã§æ©Ÿèƒ½ã—ã¾ã™ã€‚æ‰‹å‹•ã§é–‹å§‹ï¼çµ‚äº†ã™ã‚‹å ´åˆã¯ `start()`ï¼`finish()` ã« `mark_as_current` ã¨ `reset_current` ã‚’æ¸¡ã—ã¦ç¾åœ¨ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚

## ã‚¹ãƒ‘ãƒ³ã®ä½œæˆ

å„ç¨® [`*_span()`][agents.tracing.create] ãƒ¡ã‚½ãƒƒãƒ‰ã§ã‚¹ãƒ‘ãƒ³ã‚’ä½œæˆã§ãã¾ã™ã€‚ä¸€èˆ¬çš„ã«ã¯æ‰‹å‹•ã§ä½œæˆã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚«ã‚¹ã‚¿ãƒ æƒ…å ±ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã® [`custom_span()`][agents.tracing.custom_span] ã‚‚åˆ©ç”¨ã§ãã¾ã™ã€‚

ã‚¹ãƒ‘ãƒ³ã¯è‡ªå‹•çš„ã«ç¾åœ¨ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã®ä¸€éƒ¨ã¨ãªã‚Šã€æœ€ã‚‚è¿‘ã„ç¾åœ¨ã®ã‚¹ãƒ‘ãƒ³ã®ä¸‹ã«ãƒã‚¹ãƒˆã•ã‚Œã¾ã™ã€‚ã“ã‚Œã‚‚ Python ã® [`contextvar`](https://docs.python.org/3/library/contextvars.html) ã§ç®¡ç†ã•ã‚Œã¦ã„ã¾ã™ã€‚

## æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿

ä¸€éƒ¨ã®ã‚¹ãƒ‘ãƒ³ã§ã¯æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ãŒåé›†ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

`generation_span()` ã«ã¯ LLM ã®å…¥åŠ›ã¨å‡ºåŠ›ã€`function_span()` ã«ã¯é–¢æ•°å‘¼ã³å‡ºã—ã®å…¥åŠ›ã¨å‡ºåŠ›ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã‚‰ã«æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹å ´åˆã€[`RunConfig.trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data] ã‚’ä½¿ç”¨ã—ã¦è¨˜éŒ²ã‚’ç„¡åŠ¹åŒ–ã§ãã¾ã™ã€‚

åŒæ§˜ã«ã€éŸ³å£°ã‚¹ãƒ‘ãƒ³ã«ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ base64 ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸ PCM éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¾ã™ã€‚[`VoicePipelineConfig.trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data] ã‚’è¨­å®šã—ã¦éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²ã‚’ç„¡åŠ¹åŒ–ã§ãã¾ã™ã€‚

## ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼

ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®é«˜ãƒ¬ãƒ™ãƒ«æ§‹æˆã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚

-   åˆæœŸåŒ–æ™‚ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«ãª [`TraceProvider`][agents.tracing.setup.TraceProvider] ã‚’ä½œæˆã—ã€ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ç”Ÿæˆã€‚
-   `TraceProvider` ã¯ [`BatchTraceProcessor`][agents.tracing.processors.BatchTraceProcessor] ã‚’ç”¨ã„ã¦ã‚¹ãƒ‘ãƒ³ï¼ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ãƒãƒƒãƒé€ä¿¡ã—ã€[`BackendSpanExporter`][agents.tracing.processors.BackendSpanExporter] ãŒ OpenAI ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¸ãƒãƒƒãƒã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’å¤‰æ›´ã—ã¦åˆ¥ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¸é€ä¿¡ã—ãŸã‚Šã€ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã®æŒ™å‹•ã‚’ä¿®æ­£ã™ã‚‹ã«ã¯æ¬¡ã® 2 é€šã‚ŠãŒã‚ã‚Šã¾ã™ã€‚

1. [`add_trace_processor()`][agents.tracing.add_trace_processor]  
   æ—¢å®šã®é€ä¿¡ã«åŠ ãˆã€**è¿½åŠ ** ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’ç™»éŒ²ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Š OpenAI ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¸ã®é€ä¿¡ã«åŠ ãˆã¦ç‹¬è‡ªå‡¦ç†ãŒå¯èƒ½ã§ã™ã€‚  
2. [`set_trace_processors()`][agents.tracing.set_trace_processors]  
   æ—¢å®šã®ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’ç½®ãæ›ãˆã€**ç‹¬è‡ª** ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã ã‘ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚OpenAI ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¸é€ä¿¡ã™ã‚‹å ´åˆã¯ã€ãã®æ©Ÿèƒ½ã‚’æŒã¤ `TracingProcessor` ã‚’å«ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

## å¤–éƒ¨ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ä¸€è¦§

-   [Weights & Biases](https://weave-docs.wandb.ai/guides/integrations/openai_agents)
-   [Arizeâ€‘Phoenix](https://docs.arize.com/phoenix/tracing/integrations-tracing/openai-agents-sdk)
-   [MLflow (selfâ€‘hosted/OSS](https://mlflow.org/docs/latest/tracing/integrations/openai-agent)
-   [MLflow (Databricks hosted](https://docs.databricks.com/aws/en/mlflow/mlflow-tracing#-automatic-tracing)
-   [Braintrust](https://braintrust.dev/docs/guides/traces/integrations#openai-agents-sdk)
-   [Pydantic Logfire](https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents)
-   [AgentOps](https://docs.agentops.ai/v1/integrations/agentssdk)
-   [Scorecard](https://docs.scorecard.io/docs/documentation/features/tracing#openai-agents-sdk-integration)
-   [Keywords AI](https://docs.keywordsai.co/integration/development-frameworks/openai-agent)
-   [LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
-   [Maxim AI](https://www.getmaxim.ai/docs/observe/integrations/openai-agents-sdk)
-   [Comet Opik](https://www.comet.com/docs/opik/tracing/integrations/openai_agents)
-   [Langfuse](https://langfuse.com/docs/integrations/openaiagentssdk/openai-agents)
-   [Langtrace](https://docs.langtrace.ai/supported-integrations/llm-frameworks/openai-agents-sdk)
-   [Okahuâ€‘Monocle](https://github.com/monocle2ai/monocle)
</file>

<file path="docs/ja/visualization.md">
---
search:
  exclude: true
---
# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å¯è¦–åŒ–

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å¯è¦–åŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ ** Graphviz ** ã‚’ç”¨ã„ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ãã®é–¢ä¿‚ã‚’æ§‹é€ åŒ–ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«è¡¨ç¾ã¨ã—ã¦ç”Ÿæˆã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å†…ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒ„ãƒ¼ãƒ«ã€handoffs ãŒã©ã®ã‚ˆã†ã«ç›¸äº’ä½œç”¨ã™ã‚‹ã‹ã‚’ç†è§£ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã® `viz` ä¾å­˜é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™:

```bash
pip install "openai-agents[viz]"
```

## ã‚°ãƒ©ãƒ•ã®ç”Ÿæˆ

`draw_graph` é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å¯è¦–åŒ–ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚ã“ã®é–¢æ•°ã¯æœ‰å‘ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã€ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ç¾ã—ã¾ã™ã€‚

- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ** ã¯é»„è‰²ã®ãƒœãƒƒã‚¯ã‚¹ã§è¡¨ã•ã‚Œã¾ã™ã€‚
- **ãƒ„ãƒ¼ãƒ«** ã¯ç·‘è‰²ã®æ¥•å††ã§è¡¨ã•ã‚Œã¾ã™ã€‚
- **handoffs** ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®æœ‰å‘ã‚¨ãƒƒã‚¸ã§ç¤ºã•ã‚Œã¾ã™ã€‚

### ä½¿ç”¨ä¾‹

```python
from agents import Agent, function_tool
from agents.extensions.visualization import draw_graph

@function_tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    tools=[get_weather],
)

draw_graph(triage_agent)
```

![Agent Graph](../assets/images/graph.png)

ã“ã‚Œã«ã‚ˆã‚Šã€ **triage agent** ã®æ§‹é€ ã¨ã€ãã‚ŒãŒã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚„ãƒ„ãƒ¼ãƒ«ã¨ã©ã®ã‚ˆã†ã«ã¤ãªãŒã£ã¦ã„ã‚‹ã‹ã‚’è¦–è¦šçš„ã«è¡¨ã™ã‚°ãƒ©ãƒ•ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚

## å¯è¦–åŒ–ã®ç†è§£

ç”Ÿæˆã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã«ã¯æ¬¡ã®è¦ç´ ãŒå«ã¾ã‚Œã¾ã™ã€‚

- ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’ç¤ºã™ **start node** (`__start__`)
- é»„è‰²ã®å¡—ã‚Šã¤ã¶ã—ã‚’æŒã¤ **çŸ©å½¢** ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
- ç·‘è‰²ã®å¡—ã‚Šã¤ã¶ã—ã‚’æŒã¤ **æ¥•å††** ã®ãƒ„ãƒ¼ãƒ«
- ç›¸äº’ä½œç”¨ã‚’ç¤ºã™æœ‰å‘ã‚¨ãƒƒã‚¸
  - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã® handoffs ã«ã¯ **å®Ÿç·šã®çŸ¢å°**
  - ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã«ã¯ **ç ´ç·šã®çŸ¢å°**
- å®Ÿè¡ŒãŒçµ‚äº†ã™ã‚‹ä½ç½®ã‚’ç¤ºã™ **end node** (`__end__`)

## ã‚°ãƒ©ãƒ•ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### ã‚°ãƒ©ãƒ•ã®è¡¨ç¤º
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€`draw_graph` ã¯ã‚°ãƒ©ãƒ•ã‚’ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã§è¡¨ç¤ºã—ã¾ã™ã€‚åˆ¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯ã€æ¬¡ã®ã‚ˆã†ã«è¨˜è¿°ã—ã¾ã™ã€‚

```python
draw_graph(triage_agent).view()
```

### ã‚°ãƒ©ãƒ•ã®ä¿å­˜
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€`draw_graph` ã¯ã‚°ãƒ©ãƒ•ã‚’ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã§è¡¨ç¤ºã—ã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã«ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŒ‡å®šã—ã¾ã™:

```python
draw_graph(triage_agent, filename="agent_graph.png")
```

ã“ã‚Œã«ã‚ˆã‚Šã€ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« `agent_graph.png` ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚
</file>

<file path="docs/models/index.md">
# Models

The Agents SDK comes with out-of-the-box support for OpenAI models in two flavors:

-   **Recommended**: the [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel], which calls OpenAI APIs using the new [Responses API](https://platform.openai.com/docs/api-reference/responses).
-   The [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel], which calls OpenAI APIs using the [Chat Completions API](https://platform.openai.com/docs/api-reference/chat).

## Mixing and matching models

Within a single workflow, you may want to use different models for each agent. For example, you could use a smaller, faster model for triage, while using a larger, more capable model for complex tasks. When configuring an [`Agent`][agents.Agent], you can select a specific model by either:

1. Passing the name of an OpenAI model.
2. Passing any model name + a [`ModelProvider`][agents.models.interface.ModelProvider] that can map that name to a Model instance.
3. Directly providing a [`Model`][agents.models.interface.Model] implementation.

!!!note

    While our SDK supports both the [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] and the [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] shapes, we recommend using a single model shape for each workflow because the two shapes support a different set of features and tools. If your workflow requires mixing and matching model shapes, make sure that all the features you're using are available on both.

```python
from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
    model="o3-mini", # (1)!
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model=OpenAIChatCompletionsModel( # (2)!
        model="gpt-4o",
        openai_client=AsyncOpenAI()
    ),
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    model="gpt-3.5-turbo",
)

async def main():
    result = await Runner.run(triage_agent, input="Hola, Â¿cÃ³mo estÃ¡s?")
    print(result.final_output)
```

1.  Sets the name of an OpenAI model directly.
2.  Provides a [`Model`][agents.models.interface.Model] implementation.

When you want to further configure the model used for an agent, you can pass [`ModelSettings`][agents.models.interface.ModelSettings], which provides optional model configuration parameters such as temperature.

```python
from agents import Agent, ModelSettings

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model="gpt-4o",
    model_settings=ModelSettings(temperature=0.1),
)
```

## Using other LLM providers

You can use other LLM providers in 3 ways (examples [here](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)):

1. [`set_default_openai_client`][agents.set_default_openai_client] is useful in cases where you want to globally use an instance of `AsyncOpenAI` as the LLM client. This is for cases where the LLM provider has an OpenAI compatible API endpoint, and you can set the `base_url` and `api_key`. See a configurable example in [examples/model_providers/custom_example_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py).
2. [`ModelProvider`][agents.models.interface.ModelProvider] is at the `Runner.run` level. This lets you say "use a custom model provider for all agents in this run". See a configurable example in [examples/model_providers/custom_example_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py).
3. [`Agent.model`][agents.agent.Agent.model] lets you specify the model on a specific Agent instance. This enables you to mix and match different providers for different agents. See a configurable example in [examples/model_providers/custom_example_agent.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py). An easy way to use most available models is via the [LiteLLM integration](./litellm.md).

In cases where you do not have an API key from `platform.openai.com`, we recommend disabling tracing via `set_tracing_disabled()`, or setting up a [different tracing processor](../tracing.md).

!!! note

    In these examples, we use the Chat Completions API/model, because most LLM providers don't yet support the Responses API. If your LLM provider does support it, we recommend using Responses.

## Common issues with using other LLM providers

### Tracing client error 401

If you get errors related to tracing, this is because traces are uploaded to OpenAI servers, and you don't have an OpenAI API key. You have three options to resolve this:

1. Disable tracing entirely: [`set_tracing_disabled(True)`][agents.set_tracing_disabled].
2. Set an OpenAI key for tracing: [`set_tracing_export_api_key(...)`][agents.set_tracing_export_api_key]. This API key will only be used for uploading traces, and must be from [platform.openai.com](https://platform.openai.com/).
3. Use a non-OpenAI trace processor. See the [tracing docs](../tracing.md#custom-tracing-processors).

### Responses API support

The SDK uses the Responses API by default, but most other LLM providers don't yet support it. You may see 404s or similar issues as a result. To resolve, you have two options:

1. Call [`set_default_openai_api("chat_completions")`][agents.set_default_openai_api]. This works if you are setting `OPENAI_API_KEY` and `OPENAI_BASE_URL` via environment vars.
2. Use [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel]. There are examples [here](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/).

### Structured outputs support

Some model providers don't have support for [structured outputs](https://platform.openai.com/docs/guides/structured-outputs). This sometimes results in an error that looks something like this:

```
BadRequestError: Error code: 400 - {'error': {'message': "'response_format.type' : value is not one of the allowed values ['text','json_object']", 'type': 'invalid_request_error'}}
```

This is a shortcoming of some model providers - they support JSON outputs, but don't allow you to specify the `json_schema` to use for the output. We are working on a fix for this, but we suggest relying on providers that do have support for JSON schema output, because otherwise your app will often break because of malformed JSON.
</file>

<file path="docs/models/litellm.md">
# Using any model via LiteLLM

!!! note

    The LiteLLM integration is in beta. You may run into issues with some model providers, especially smaller ones. Please report any issues via [Github issues](https://github.com/openai/openai-agents-python/issues) and we'll fix quickly.

[LiteLLM](https://docs.litellm.ai/docs/) is a library that allows you to use 100+ models via a single interface. We've added a LiteLLM integration to allow you to use any AI model in the Agents SDK.

## Setup

You'll need to ensure `litellm` is available. You can do this by installing the optional `litellm` dependency group:

```bash
pip install "openai-agents[litellm]"
```

Once done, you can use [`LitellmModel`][agents.extensions.models.litellm_model.LitellmModel] in any agent.

## Example

This is a fully working example. When you run it, you'll be prompted for a model name and API key. For example, you could enter:

-   `openai/gpt-4.1` for the model, and your OpenAI API key
-   `anthropic/claude-3-5-sonnet-20240620` for the model, and your Anthropic API key
-   etc

For a full list of models supported in LiteLLM, see the [litellm providers docs](https://docs.litellm.ai/docs/providers).

```python
from __future__ import annotations

import asyncio

from agents import Agent, Runner, function_tool, set_tracing_disabled
from agents.extensions.models.litellm_model import LitellmModel

@function_tool
def get_weather(city: str):
    print(f"[debug] getting weather for {city}")
    return f"The weather in {city} is sunny."


async def main(model: str, api_key: str):
    agent = Agent(
        name="Assistant",
        instructions="You only respond in haikus.",
        model=LitellmModel(model=model, api_key=api_key),
        tools=[get_weather],
    )

    result = await Runner.run(agent, "What's the weather in Tokyo?")
    print(result.final_output)


if __name__ == "__main__":
    # First try to get model/api key from args
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=False)
    parser.add_argument("--api-key", type=str, required=False)
    args = parser.parse_args()

    model = args.model
    if not model:
        model = input("Enter a model name for Litellm: ")

    api_key = args.api_key
    if not api_key:
        api_key = input("Enter an API key for Litellm: ")

    asyncio.run(main(model, api_key))
```
</file>

<file path="docs/ref/extensions/handoff_filters.md">
# `Handoff filters`

::: agents.extensions.handoff_filters
</file>

<file path="docs/ref/extensions/handoff_prompt.md">
# `Handoff prompt`

::: agents.extensions.handoff_prompt

    options:
        members:
            - RECOMMENDED_PROMPT_PREFIX
            - prompt_with_handoff_instructions
</file>

<file path="docs/ref/extensions/litellm.md">
# `LiteLLM Models`

::: agents.extensions.models.litellm_model
</file>

<file path="docs/ref/mcp/server.md">
# `MCP Servers`

::: agents.mcp.server
</file>

<file path="docs/ref/mcp/util.md">
# `MCP Util`

::: agents.mcp.util
</file>

<file path="docs/ref/models/interface.md">
# `Model interface`

::: agents.models.interface
</file>

<file path="docs/ref/models/openai_chatcompletions.md">
# `OpenAI Chat Completions model`

::: agents.models.openai_chatcompletions
</file>

<file path="docs/ref/models/openai_responses.md">
# `OpenAI Responses model`

::: agents.models.openai_responses
</file>

<file path="docs/ref/tracing/create.md">
# `Creating traces/spans`

::: agents.tracing.create
</file>

<file path="docs/ref/tracing/index.md">
# Tracing module

::: agents.tracing
</file>

<file path="docs/ref/tracing/processor_interface.md">
# `Processor interface`

::: agents.tracing.processor_interface
</file>

<file path="docs/ref/tracing/processors.md">
# `Processors`

::: agents.tracing.processors
</file>

<file path="docs/ref/tracing/scope.md">
# `Scope`

::: agents.tracing.scope
</file>

<file path="docs/ref/tracing/setup.md">
# `Setup`

::: agents.tracing.setup
</file>

<file path="docs/ref/tracing/span_data.md">
# `Span data`

::: agents.tracing.span_data
</file>

<file path="docs/ref/tracing/spans.md">
# `Spans`

::: agents.tracing.spans

    options:
        members:
            - Span
            - NoOpSpan
            - SpanImpl
</file>

<file path="docs/ref/tracing/traces.md">
# `Traces`

::: agents.tracing.traces
</file>

<file path="docs/ref/tracing/util.md">
# `Util`

::: agents.tracing.util
</file>

<file path="docs/ref/voice/models/openai_provider.md">
# `OpenAIVoiceModelProvider`

::: agents.voice.models.openai_model_provider
</file>

<file path="docs/ref/voice/models/openai_stt.md">
# `OpenAI STT`

::: agents.voice.models.openai_stt
</file>

<file path="docs/ref/voice/models/openai_tts.md">
# `OpenAI TTS`

::: agents.voice.models.openai_tts
</file>

<file path="docs/ref/voice/events.md">
# `Events`

::: agents.voice.events
</file>

<file path="docs/ref/voice/exceptions.md">
# `Exceptions`

::: agents.voice.exceptions
</file>

<file path="docs/ref/voice/input.md">
# `Input`

::: agents.voice.input
</file>

<file path="docs/ref/voice/model.md">
# `Model`

::: agents.voice.model
</file>

<file path="docs/ref/voice/pipeline_config.md">
# `Pipeline Config`

::: agents.voice.pipeline_config
</file>

<file path="docs/ref/voice/pipeline.md">
# `Pipeline`

::: agents.voice.pipeline
</file>

<file path="docs/ref/voice/result.md">
# `Result`

::: agents.voice.result
</file>

<file path="docs/ref/voice/utils.md">
# `Utils`

::: agents.voice.utils
</file>

<file path="docs/ref/voice/workflow.md">
# `Workflow`

::: agents.voice.workflow
</file>

<file path="docs/ref/agent_output.md">
# `Agent output`

::: agents.agent_output
</file>

<file path="docs/ref/agent.md">
# `Agents`

::: agents.agent
</file>

<file path="docs/ref/exceptions.md">
# `Exceptions`

::: agents.exceptions
</file>

<file path="docs/ref/function_schema.md">
# `Function schema`

::: agents.function_schema
</file>

<file path="docs/ref/guardrail.md">
# `Guardrails`

::: agents.guardrail
</file>

<file path="docs/ref/handoffs.md">
# `Handoffs`

::: agents.handoffs
</file>

<file path="docs/ref/index.md">
# Agents module

::: agents

    options:
        members:
            - set_default_openai_key
            - set_default_openai_client
            - set_default_openai_api
            - set_tracing_export_api_key
            - set_tracing_disabled
            - set_trace_processors
            - enable_verbose_stdout_logging
</file>

<file path="docs/ref/items.md">
# `Items`

::: agents.items
</file>

<file path="docs/ref/lifecycle.md">
# `Lifecycle`

::: agents.lifecycle

    options:
        show_source: false
</file>

<file path="docs/ref/model_settings.md">
# `Model settings`

::: agents.model_settings
</file>

<file path="docs/ref/result.md">
# `Results`

::: agents.result
</file>

<file path="docs/ref/run_context.md">
# `Run context`

::: agents.run_context
</file>

<file path="docs/ref/run.md">
# `Runner`

::: agents.run

    options:
        members:
            - Runner
            - RunConfig
</file>

<file path="docs/ref/stream_events.md">
# `Streaming events`

::: agents.stream_events
</file>

<file path="docs/ref/tool.md">
# `Tools`

::: agents.tool
</file>

<file path="docs/ref/usage.md">
# `Usage`

::: agents.usage
</file>

<file path="docs/voice/pipeline.md">
# Pipelines and workflows

[`VoicePipeline`][agents.voice.pipeline.VoicePipeline] is a class that makes it easy to turn your agentic workflows into a voice app. You pass in a workflow to run, and the pipeline takes care of transcribing input audio, detecting when the audio ends, calling your workflow at the right time, and turning the workflow output back into audio.

```mermaid
graph LR
    %% Input
    A["ğŸ¤ Audio Input"]

    %% Voice Pipeline
    subgraph Voice_Pipeline [Voice Pipeline]
        direction TB
        B["Transcribe (speech-to-text)"]
        C["Your Code"]:::highlight
        D["Text-to-speech"]
        B --> C --> D
    end

    %% Output
    E["ğŸ§ Audio Output"]

    %% Flow
    A --> Voice_Pipeline
    Voice_Pipeline --> E

    %% Custom styling
    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;

```

## Configuring a pipeline

When you create a pipeline, you can set a few things:

1. The [`workflow`][agents.voice.workflow.VoiceWorkflowBase], which is the code that runs each time new audio is transcribed.
2. The [`speech-to-text`][agents.voice.model.STTModel] and [`text-to-speech`][agents.voice.model.TTSModel] models used
3. The [`config`][agents.voice.pipeline_config.VoicePipelineConfig], which lets you configure things like:
    - A model provider, which can map model names to models
    - Tracing, including whether to disable tracing, whether audio files are uploaded, the workflow name, trace IDs etc.
    - Settings on the TTS and STT models, like the prompt, language and data types used.

## Running a pipeline

You can run a pipeline via the [`run()`][agents.voice.pipeline.VoicePipeline.run] method, which lets you pass in audio input in two forms:

1. [`AudioInput`][agents.voice.input.AudioInput] is used when you have a full audio transcript, and just want to produce a result for it. This is useful in cases where you don't need to detect when a speaker is done speaking; for example, when you have pre-recorded audio or in push-to-talk apps where it's clear when the user is done speaking.
2. [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput] is used when you might need to detect when a user is done speaking. It allows you to push audio chunks as they are detected, and the voice pipeline will automatically run the agent workflow at the right time, via a process called "activity detection".

## Results

The result of a voice pipeline run is a [`StreamedAudioResult`][agents.voice.result.StreamedAudioResult]. This is an object that lets you stream events as they occur. There are a few kinds of [`VoiceStreamEvent`][agents.voice.events.VoiceStreamEvent], including:

1. [`VoiceStreamEventAudio`][agents.voice.events.VoiceStreamEventAudio], which contains a chunk of audio.
2. [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle], which informs you of lifecycle events like a turn starting or ending.
3. [`VoiceStreamEventError`][agents.voice.events.VoiceStreamEventError], is an error event.

```python

result = await pipeline.run(input)

async for event in result.stream():
    if event.type == "voice_stream_event_audio":
        # play audio
    elif event.type == "voice_stream_event_lifecycle":
        # lifecycle
    elif event.type == "voice_stream_event_error"
        # error
    ...
```

## Best practices

### Interruptions

The Agents SDK currently does not support any built-in interruptions support for [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]. Instead for every detected turn it will trigger a separate run of your workflow. If you want to handle interruptions inside your application you can listen to the [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle] events. `turn_started` will indicate that a new turn was transcribed and processing is beginning. `turn_ended` will trigger after all the audio was dispatched for a respective turn. You could use these events to mute the microphone of the speaker when the model starts a turn and unmute it after you flushed all the related audio for a turn.
</file>

<file path="docs/voice/quickstart.md">
# Quickstart

## Prerequisites

Make sure you've followed the base [quickstart instructions](../quickstart.md) for the Agents SDK, and set up a virtual environment. Then, install the optional voice dependencies from the SDK:

```bash
pip install 'openai-agents[voice]'
```

## Concepts

The main concept to know about is a [`VoicePipeline`][agents.voice.pipeline.VoicePipeline], which is a 3 step process:

1. Run a speech-to-text model to turn audio into text.
2. Run your code, which is usually an agentic workflow, to produce a result.
3. Run a text-to-speech model to turn the result text back into audio.

```mermaid
graph LR
    %% Input
    A["ğŸ¤ Audio Input"]

    %% Voice Pipeline
    subgraph Voice_Pipeline [Voice Pipeline]
        direction TB
        B["Transcribe (speech-to-text)"]
        C["Your Code"]:::highlight
        D["Text-to-speech"]
        B --> C --> D
    end

    %% Output
    E["ğŸ§ Audio Output"]

    %% Flow
    A --> Voice_Pipeline
    Voice_Pipeline --> E

    %% Custom styling
    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;

```

## Agents

First, let's set up some Agents. This should feel familiar to you if you've built any agents with this SDK. We'll have a couple of Agents, a handoff, and a tool.

```python
import asyncio
import random

from agents import (
    Agent,
    function_tool,
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions



@function_tool
def get_weather(city: str) -> str:
    """Get the weather for a given city."""
    print(f"[debug] get_weather called with city: {city}")
    choices = ["sunny", "cloudy", "rainy", "snowy"]
    return f"The weather in {city} is {random.choice(choices)}."


spanish_agent = Agent(
    name="Spanish",
    handoff_description="A spanish speaking agent.",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. Speak in Spanish.",
    ),
    model="gpt-4o-mini",
)

agent = Agent(
    name="Assistant",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.",
    ),
    model="gpt-4o-mini",
    handoffs=[spanish_agent],
    tools=[get_weather],
)
```

## Voice pipeline

We'll set up a simple voice pipeline, using [`SingleAgentVoiceWorkflow`][agents.voice.workflow.SingleAgentVoiceWorkflow] as the workflow.

```python
from agents.voice import SingleAgentVoiceWorkflow, VoicePipeline
pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))
```

## Run the pipeline

```python
import numpy as np
import sounddevice as sd
from agents.voice import AudioInput

# For simplicity, we'll just create 3 seconds of silence
# In reality, you'd get microphone data
buffer = np.zeros(24000 * 3, dtype=np.int16)
audio_input = AudioInput(buffer=buffer)

result = await pipeline.run(audio_input)

# Create an audio player using `sounddevice`
player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
player.start()

# Play the audio stream as it comes in
async for event in result.stream():
    if event.type == "voice_stream_event_audio":
        player.write(event.data)

```

## Put it all together

```python
import asyncio
import random

import numpy as np
import sounddevice as sd

from agents import (
    Agent,
    function_tool,
    set_tracing_disabled,
)
from agents.voice import (
    AudioInput,
    SingleAgentVoiceWorkflow,
    VoicePipeline,
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions


@function_tool
def get_weather(city: str) -> str:
    """Get the weather for a given city."""
    print(f"[debug] get_weather called with city: {city}")
    choices = ["sunny", "cloudy", "rainy", "snowy"]
    return f"The weather in {city} is {random.choice(choices)}."


spanish_agent = Agent(
    name="Spanish",
    handoff_description="A spanish speaking agent.",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. Speak in Spanish.",
    ),
    model="gpt-4o-mini",
)

agent = Agent(
    name="Assistant",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.",
    ),
    model="gpt-4o-mini",
    handoffs=[spanish_agent],
    tools=[get_weather],
)


async def main():
    pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))
    buffer = np.zeros(24000 * 3, dtype=np.int16)
    audio_input = AudioInput(buffer=buffer)

    result = await pipeline.run(audio_input)

    # Create an audio player using `sounddevice`
    player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
    player.start()

    # Play the audio stream as it comes in
    async for event in result.stream():
        if event.type == "voice_stream_event_audio":
            player.write(event.data)


if __name__ == "__main__":
    asyncio.run(main())
```

If you run this example, the agent will speak to you! Check out the example in [examples/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static) to see a demo where you can speak to the agent yourself.
</file>

<file path="docs/voice/tracing.md">
# Tracing

Just like the way [agents are traced](../tracing.md), voice pipelines are also automatically traced.

You can read the tracing doc above for basic tracing information, but you can additionally configure tracing of a pipeline via [`VoicePipelineConfig`][agents.voice.pipeline_config.VoicePipelineConfig].

Key tracing related fields are:

-   [`tracing_disabled`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]: controls whether tracing is disabled. By default, tracing is enabled.
-   [`trace_include_sensitive_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_data]: controls whether traces include potentially sensitive data, like audio transcripts. This is specifically for the voice pipeline, and not for anything that goes on inside your Workflow.
-   [`trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data]: controls whether traces include audio data.
-   [`workflow_name`][agents.voice.pipeline_config.VoicePipelineConfig.workflow_name]: The name of the trace workflow.
-   [`group_id`][agents.voice.pipeline_config.VoicePipelineConfig.group_id]: The `group_id` of the trace, which lets you link multiple traces.
-   [`trace_metadata`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]: Additional metadata to include with the trace.
</file>

<file path="docs/agents.md">
# Agents

Agents are the core building block in your apps. An agent is a large language model (LLM), configured with instructions and tools.

## Basic configuration

The most common properties of an agent you'll configure are:

-   `instructions`: also known as a developer message or system prompt.
-   `model`: which LLM to use, and optional `model_settings` to configure model tuning parameters like temperature, top_p, etc.
-   `tools`: Tools that the agent can use to achieve its tasks.

```python
from agents import Agent, ModelSettings, function_tool

@function_tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Haiku agent",
    instructions="Always respond in haiku form",
    model="o3-mini",
    tools=[get_weather],
)
```

## Context

Agents are generic on their `context` type. Context is a dependency-injection tool: it's an object you create and pass to `Runner.run()`, that is passed to every agent, tool, handoff etc, and it serves as a grab bag of dependencies and state for the agent run. You can provide any Python object as the context.

```python
@dataclass
class UserContext:
    uid: str
    is_pro_user: bool

    async def fetch_purchases() -> list[Purchase]:
        return ...

agent = Agent[UserContext](
    ...,
)
```

## Output types

By default, agents produce plain text (i.e. `str`) outputs. If you want the agent to produce a particular type of output, you can use the `output_type` parameter. A common choice is to use [Pydantic](https://docs.pydantic.dev/) objects, but we support any type that can be wrapped in a Pydantic [TypeAdapter](https://docs.pydantic.dev/latest/api/type_adapter/) - dataclasses, lists, TypedDict, etc.

```python
from pydantic import BaseModel
from agents import Agent


class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

agent = Agent(
    name="Calendar extractor",
    instructions="Extract calendar events from text",
    output_type=CalendarEvent,
)
```

!!! note

    When you pass an `output_type`, that tells the model to use [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) instead of regular plain text responses.

## Handoffs

Handoffs are sub-agents that the agent can delegate to. You provide a list of handoffs, and the agent can choose to delegate to them if relevant. This is a powerful pattern that allows orchestrating modular, specialized agents that excel at a single task. Read more in the [handoffs](handoffs.md) documentation.

```python
from agents import Agent

booking_agent = Agent(...)
refund_agent = Agent(...)

triage_agent = Agent(
    name="Triage agent",
    instructions=(
        "Help the user with their questions."
        "If they ask about booking, handoff to the booking agent."
        "If they ask about refunds, handoff to the refund agent."
    ),
    handoffs=[booking_agent, refund_agent],
)
```

## Dynamic instructions

In most cases, you can provide instructions when you create the agent. However, you can also provide dynamic instructions via a function. The function will receive the agent and context, and must return the prompt. Both regular and `async` functions are accepted.

```python
def dynamic_instructions(
    context: RunContextWrapper[UserContext], agent: Agent[UserContext]
) -> str:
    return f"The user's name is {context.context.name}. Help them with their questions."


agent = Agent[UserContext](
    name="Triage agent",
    instructions=dynamic_instructions,
)
```

## Lifecycle events (hooks)

Sometimes, you want to observe the lifecycle of an agent. For example, you may want to log events, or pre-fetch data when certain events occur. You can hook into the agent lifecycle with the `hooks` property. Subclass the [`AgentHooks`][agents.lifecycle.AgentHooks] class, and override the methods you're interested in.

## Guardrails

Guardrails allow you to run checks/validations on user input, in parallel to the agent running. For example, you could screen the user's input for relevance. Read more in the [guardrails](guardrails.md) documentation.

## Cloning/copying agents

By using the `clone()` method on an agent, you can duplicate an Agent, and optionally change any properties you like.

```python
pirate_agent = Agent(
    name="Pirate",
    instructions="Write like a pirate",
    model="o3-mini",
)

robot_agent = pirate_agent.clone(
    name="Robot",
    instructions="Write like a robot",
)
```

## Forcing tool use

Supplying a list of tools doesn't always mean the LLM will use a tool. You can force tool use by setting [`ModelSettings.tool_choice`][agents.model_settings.ModelSettings.tool_choice]. Valid values are:

1. `auto`, which allows the LLM to decide whether or not to use a tool.
2. `required`, which requires the LLM to use a tool (but it can intelligently decide which tool).
3. `none`, which requires the LLM to _not_ use a tool.
4. Setting a specific string e.g. `my_tool`, which requires the LLM to use that specific tool.

!!! note

    To prevent infinite loops, the framework automatically resets `tool_choice` to "auto" after a tool call. This behavior is configurable via [`agent.reset_tool_choice`][agents.agent.Agent.reset_tool_choice]. The infinite loop is because tool results are sent to the LLM, which then generates another tool call because of `tool_choice`, ad infinitum.

    If you want the Agent to completely stop after a tool call (rather than continuing with auto mode), you can set [`Agent.tool_use_behavior="stop_on_first_tool"`] which will directly use the tool output as the final response without further LLM processing.
</file>

<file path="docs/config.md">
# Configuring the SDK

## API keys and clients

By default, the SDK looks for the `OPENAI_API_KEY` environment variable for LLM requests and tracing, as soon as it is imported. If you are unable to set that environment variable before your app starts, you can use the [set_default_openai_key()][agents.set_default_openai_key] function to set the key.

```python
from agents import set_default_openai_key

set_default_openai_key("sk-...")
```

Alternatively, you can also configure an OpenAI client to be used. By default, the SDK creates an `AsyncOpenAI` instance, using the API key from the environment variable or the default key set above. You can change this by using the [set_default_openai_client()][agents.set_default_openai_client] function.

```python
from openai import AsyncOpenAI
from agents import set_default_openai_client

custom_client = AsyncOpenAI(base_url="...", api_key="...")
set_default_openai_client(custom_client)
```

Finally, you can also customize the OpenAI API that is used. By default, we use the OpenAI Responses API. You can override this to use the Chat Completions API by using the [set_default_openai_api()][agents.set_default_openai_api] function.

```python
from agents import set_default_openai_api

set_default_openai_api("chat_completions")
```

## Tracing

Tracing is enabled by default. It uses the OpenAI API keys from the section above by default (i.e. the environment variable or the default key you set). You can specifically set the API key used for tracing by using the [`set_tracing_export_api_key`][agents.set_tracing_export_api_key] function.

```python
from agents import set_tracing_export_api_key

set_tracing_export_api_key("sk-...")
```

You can also disable tracing entirely by using the [`set_tracing_disabled()`][agents.set_tracing_disabled] function.

```python
from agents import set_tracing_disabled

set_tracing_disabled(True)
```

## Debug logging

The SDK has two Python loggers without any handlers set. By default, this means that warnings and errors are sent to `stdout`, but other logs are suppressed.

To enable verbose logging, use the [`enable_verbose_stdout_logging()`][agents.enable_verbose_stdout_logging] function.

```python
from agents import enable_verbose_stdout_logging

enable_verbose_stdout_logging()
```

Alternatively, you can customize the logs by adding handlers, filters, formatters, etc. You can read more in the [Python logging guide](https://docs.python.org/3/howto/logging.html).

```python
import logging

logger = logging.getLogger("openai.agents") # or openai.agents.tracing for the Tracing logger

# To make all logs show up
logger.setLevel(logging.DEBUG)
# To make info and above show up
logger.setLevel(logging.INFO)
# To make warning and above show up
logger.setLevel(logging.WARNING)
# etc

# You can customize this as needed, but this will output to `stderr` by default
logger.addHandler(logging.StreamHandler())
```

### Sensitive data in logs

Certain logs may contain sensitive data (for example, user data). If you want to disable this data from being logged, set the following environment variables.

To disable logging LLM inputs and outputs:

```bash
export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1
```

To disable logging tool inputs and outputs:

```bash
export OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1
```
</file>

<file path="docs/context.md">
# Context management

Context is an overloaded term. There are two main classes of context you might care about:

1. Context available locally to your code: this is data and dependencies you might need when tool functions run, during callbacks like `on_handoff`, in lifecycle hooks, etc.
2. Context available to LLMs: this is data the LLM sees when generating a response.

## Local context

This is represented via the [`RunContextWrapper`][agents.run_context.RunContextWrapper] class and the [`context`][agents.run_context.RunContextWrapper.context] property within it. The way this works is:

1. You create any Python object you want. A common pattern is to use a dataclass or a Pydantic object.
2. You pass that object to the various run methods (e.g. `Runner.run(..., **context=whatever**))`.
3. All your tool calls, lifecycle hooks etc will be passed a wrapper object, `RunContextWrapper[T]`, where `T` represents your context object type which you can access via `wrapper.context`.

The **most important** thing to be aware of: every agent, tool function, lifecycle etc for a given agent run must use the same _type_ of context.

You can use the context for things like:

-   Contextual data for your run (e.g. things like a username/uid or other information about the user)
-   Dependencies (e.g. logger objects, data fetchers, etc)
-   Helper functions

!!! danger "Note"

    The context object is **not** sent to the LLM. It is purely a local object that you can read from, write to and call methods on it.

```python
import asyncio
from dataclasses import dataclass

from agents import Agent, RunContextWrapper, Runner, function_tool

@dataclass
class UserInfo:  # (1)!
    name: str
    uid: int

@function_tool
async def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str:  # (2)!
    return f"User {wrapper.context.name} is 47 years old"

async def main():
    user_info = UserInfo(name="John", uid=123)

    agent = Agent[UserInfo](  # (3)!
        name="Assistant",
        tools=[fetch_user_age],
    )

    result = await Runner.run(  # (4)!
        starting_agent=agent,
        input="What is the age of the user?",
        context=user_info,
    )

    print(result.final_output)  # (5)!
    # The user John is 47 years old.

if __name__ == "__main__":
    asyncio.run(main())
```

1. This is the context object. We've used a dataclass here, but you can use any type.
2. This is a tool. You can see it takes a `RunContextWrapper[UserInfo]`. The tool implementation reads from the context.
3. We mark the agent with the generic `UserInfo`, so that the typechecker can catch errors (for example, if we tried to pass a tool that took a different context type).
4. The context is passed to the `run` function.
5. The agent correctly calls the tool and gets the age.

## Agent/LLM context

When an LLM is called, the **only** data it can see is from the conversation history. This means that if you want to make some new data available to the LLM, you must do it in a way that makes it available in that history. There are a few ways to do this:

1. You can add it to the Agent `instructions`. This is also known as a "system prompt" or "developer message". System prompts can be static strings, or they can be dynamic functions that receive the context and output a string. This is a common tactic for information that is always useful (for example, the user's name or the current date).
2. Add it to the `input` when calling the `Runner.run` functions. This is similar to the `instructions` tactic, but allows you to have messages that are lower in the [chain of command](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command).
3. Expose it via function tools. This is useful for _on-demand_ context - the LLM decides when it needs some data, and can call the tool to fetch that data.
4. Use retrieval or web search. These are special tools that are able to fetch relevant data from files or databases (retrieval), or from the web (web search). This is useful for "grounding" the response in relevant contextual data.
</file>

<file path="docs/examples.md">
# Examples

Check out a variety of sample implementations of the SDK in the examples section of the [repo](https://github.com/openai/openai-agents-python/tree/main/examples). The examples are organized into several categories that demonstrate different patterns and capabilities.


## Categories

- **[agent_patterns](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns):**
  Examples in this category illustrate common agent design patterns, such as

    - Deterministic workflows
    - Agents as tools
    - Parallel agent execution

- **[basic](https://github.com/openai/openai-agents-python/tree/main/examples/basic):**
  These examples showcase foundational capabilities of the SDK, such as

    - Dynamic system prompts
    - Streaming outputs
    - Lifecycle events

- **[tool examples](https://github.com/openai/openai-agents-python/tree/main/examples/tools):**
  Learn how to implement OAI hosted tools such as web search and file search,
   and integrate them into your agents.

- **[model providers](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers):**
  Explore how to use non-OpenAI models with the SDK.

- **[handoffs](https://github.com/openai/openai-agents-python/tree/main/examples/handoffs):**
  See practical examples of agent handoffs.

- **[mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp):**
  Learn how to build agents with MCP.

- **[customer_service](https://github.com/openai/openai-agents-python/tree/main/examples/customer_service)** and **[research_bot](https://github.com/openai/openai-agents-python/tree/main/examples/research_bot):**
  Two more built-out examples that illustrate real-world applications

    - **customer_service**: Example customer service system for an airline.
    - **research_bot**: Simple deep research clone.

- **[voice](https://github.com/openai/openai-agents-python/tree/main/examples/voice):**
  See examples of voice agents, using our TTS and STT models.
</file>

<file path="docs/guardrails.md">
# Guardrails

Guardrails run _in parallel_ to your agents, enabling you to do checks and validations of user input. For example, imagine you have an agent that uses a very smart (and hence slow/expensive) model to help with customer requests. You wouldn't want malicious users to ask the model to help them with their math homework. So, you can run a guardrail with a fast/cheap model. If the guardrail detects malicious usage, it can immediately raise an error, which stops the expensive model from running and saves you time/money.

There are two kinds of guardrails:

1. Input guardrails run on the initial user input
2. Output guardrails run on the final agent output

## Input guardrails

Input guardrails run in 3 steps:

1. First, the guardrail receives the same input passed to the agent.
2. Next, the guardrail function runs to produce a [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput], which is then wrapped in an [`InputGuardrailResult`][agents.guardrail.InputGuardrailResult]
3. Finally, we check if [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] is true. If true, an [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered] exception is raised, so you can appropriately respond to the user or handle the exception.

!!! Note

    Input guardrails are intended to run on user input, so an agent's guardrails only run if the agent is the *first* agent. You might wonder, why is the `guardrails` property on the agent instead of passed to `Runner.run`? It's because guardrails tend to be related to the actual Agent - you'd run different guardrails for different agents, so colocating the code is useful for readability.

## Output guardrails

Output guardrails run in 3 steps:

1. First, the guardrail receives the same input passed to the agent.
2. Next, the guardrail function runs to produce a [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput], which is then wrapped in an [`OutputGuardrailResult`][agents.guardrail.OutputGuardrailResult]
3. Finally, we check if [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] is true. If true, an [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered] exception is raised, so you can appropriately respond to the user or handle the exception.

!!! Note

    Output guardrails are intended to run on the final agent output, so an agent's guardrails only run if the agent is the *last* agent. Similar to the input guardrails, we do this because guardrails tend to be related to the actual Agent - you'd run different guardrails for different agents, so colocating the code is useful for readability.

## Tripwires

If the input or output fails the guardrail, the Guardrail can signal this with a tripwire. As soon as we see a guardrail that has triggered the tripwires, we immediately raise a `{Input,Output}GuardrailTripwireTriggered` exception and halt the Agent execution.

## Implementing a guardrail

You need to provide a function that receives input, and returns a [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]. In this example, we'll do this by running an Agent under the hood.

```python
from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    InputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    TResponseInputItem,
    input_guardrail,
)

class MathHomeworkOutput(BaseModel):
    is_math_homework: bool
    reasoning: str

guardrail_agent = Agent( # (1)!
    name="Guardrail check",
    instructions="Check if the user is asking you to do their math homework.",
    output_type=MathHomeworkOutput,
)


@input_guardrail
async def math_guardrail( # (2)!
    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, input, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output, # (3)!
        tripwire_triggered=result.final_output.is_math_homework,
    )


agent = Agent(  # (4)!
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    input_guardrails=[math_guardrail],
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except InputGuardrailTripwireTriggered:
        print("Math homework guardrail tripped")
```

1. We'll use this agent in our guardrail function.
2. This is the guardrail function that receives the agent's input/context, and returns the result.
3. We can include extra information in the guardrail result.
4. This is the actual agent that defines the workflow.

Output guardrails are similar.

```python
from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    OutputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    output_guardrail,
)
class MessageOutput(BaseModel): # (1)!
    response: str

class MathOutput(BaseModel): # (2)!
    reasoning: str
    is_math: bool

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the output includes any math.",
    output_type=MathOutput,
)

@output_guardrail
async def math_guardrail(  # (3)!
    ctx: RunContextWrapper, agent: Agent, output: MessageOutput
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, output.response, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output,
        tripwire_triggered=result.final_output.is_math,
    )

agent = Agent( # (4)!
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    output_guardrails=[math_guardrail],
    output_type=MessageOutput,
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except OutputGuardrailTripwireTriggered:
        print("Math output guardrail tripped")
```

1. This is the actual agent's output type.
2. This is the guardrail's output type.
3. This is the guardrail function that receives the agent's output, and returns the result.
4. This is the actual agent that defines the workflow.
</file>

<file path="docs/handoffs.md">
# Handoffs

Handoffs allow an agent to delegate tasks to another agent. This is particularly useful in scenarios where different agents specialize in distinct areas. For example, a customer support app might have agents that each specifically handle tasks like order status, refunds, FAQs, etc.

Handoffs are represented as tools to the LLM. So if there's a handoff to an agent named `Refund Agent`, the tool would be called `transfer_to_refund_agent`.

## Creating a handoff

All agents have a [`handoffs`][agents.agent.Agent.handoffs] param, which can either take an `Agent` directly, or a `Handoff` object that customizes the Handoff.

You can create a handoff using the [`handoff()`][agents.handoffs.handoff] function provided by the Agents SDK. This function allows you to specify the agent to hand off to, along with optional overrides and input filters.

### Basic Usage

Here's how you can create a simple handoff:

```python
from agents import Agent, handoff

billing_agent = Agent(name="Billing agent")
refund_agent = Agent(name="Refund agent")

# (1)!
triage_agent = Agent(name="Triage agent", handoffs=[billing_agent, handoff(refund_agent)])
```

1. You can use the agent directly (as in `billing_agent`), or you can use the `handoff()` function.

### Customizing handoffs via the `handoff()` function

The [`handoff()`][agents.handoffs.handoff] function lets you customize things.

-   `agent`: This is the agent to which things will be handed off.
-   `tool_name_override`: By default, the `Handoff.default_tool_name()` function is used, which resolves to `transfer_to_<agent_name>`. You can override this.
-   `tool_description_override`: Override the default tool description from `Handoff.default_tool_description()`
-   `on_handoff`: A callback function executed when the handoff is invoked. This is useful for things like kicking off some data fetching as soon as you know a handoff is being invoked. This function receives the agent context, and can optionally also receive LLM generated input. The input data is controlled by the `input_type` param.
-   `input_type`: The type of input expected by the handoff (optional).
-   `input_filter`: This lets you filter the input received by the next agent. See below for more.

```python
from agents import Agent, handoff, RunContextWrapper

def on_handoff(ctx: RunContextWrapper[None]):
    print("Handoff called")

agent = Agent(name="My agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    tool_name_override="custom_handoff_tool",
    tool_description_override="Custom description",
)
```

## Handoff inputs

In certain situations, you want the LLM to provide some data when it calls a handoff. For example, imagine a handoff to an "Escalation agent". You might want a reason to be provided, so you can log it.

```python
from pydantic import BaseModel

from agents import Agent, handoff, RunContextWrapper

class EscalationData(BaseModel):
    reason: str

async def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):
    print(f"Escalation agent called with reason: {input_data.reason}")

agent = Agent(name="Escalation agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    input_type=EscalationData,
)
```

## Input filters

When a handoff occurs, it's as though the new agent takes over the conversation, and gets to see the entire previous conversation history. If you want to change this, you can set an [`input_filter`][agents.handoffs.Handoff.input_filter]. An input filter is a function that receives the existing input via a [`HandoffInputData`][agents.handoffs.HandoffInputData], and must return a new `HandoffInputData`.

There are some common patterns (for example removing all tool calls from the history), which are implemented for you in [`agents.extensions.handoff_filters`][]

```python
from agents import Agent, handoff
from agents.extensions import handoff_filters

agent = Agent(name="FAQ agent")

handoff_obj = handoff(
    agent=agent,
    input_filter=handoff_filters.remove_all_tools, # (1)!
)
```

1. This will automatically remove all tools from the history when `FAQ agent` is called.

## Recommended prompts

To make sure that LLMs understand handoffs properly, we recommend including information about handoffs in your agents. We have a suggested prefix in [`agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX`][], or you can call [`agents.extensions.handoff_prompt.prompt_with_handoff_instructions`][] to automatically add recommended data to your prompts.

```python
from agents import Agent
from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX

billing_agent = Agent(
    name="Billing agent",
    instructions=f"""{RECOMMENDED_PROMPT_PREFIX}
    <Fill in the rest of your prompt here>.""",
)
```
</file>

<file path="docs/index.md">
# OpenAI Agents SDK

The [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It's a production-ready upgrade of our previous experimentation for agents, [Swarm](https://github.com/openai/swarm/tree/main). The Agents SDK has a very small set of primitives:

-   **Agents**, which are LLMs equipped with instructions and tools
-   **Handoffs**, which allow agents to delegate to other agents for specific tasks
-   **Guardrails**, which enable the inputs to agents to be validated

In combination with Python, these primitives are powerful enough to express complex relationships between tools and agents, and allow you to build real-world applications without a steep learning curve. In addition, the SDK comes with built-in **tracing** that lets you visualize and debug your agentic flows, as well as evaluate them and even fine-tune models for your application.

## Why use the Agents SDK

The SDK has two driving design principles:

1. Enough features to be worth using, but few enough primitives to make it quick to learn.
2. Works great out of the box, but you can customize exactly what happens.

Here are the main features of the SDK:

-   Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.
-   Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.
-   Handoffs: A powerful feature to coordinate and delegate between multiple agents.
-   Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.
-   Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.
-   Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.

## Installation

```bash
pip install openai-agents
```

## Hello world example

```python
from agents import Agent, Runner

agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)

# Code within the code,
# Functions calling themselves,
# Infinite loop's dance.
```

(_If running this, ensure you set the `OPENAI_API_KEY` environment variable_)

```bash
export OPENAI_API_KEY=sk-...
```
</file>

<file path="docs/mcp.md">
# Model context protocol (MCP)

The [Model context protocol](https://modelcontextprotocol.io/introduction) (aka MCP) is a way to provide tools and context to the LLM. From the MCP docs:

> MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.

The Agents SDK has support for MCP. This enables you to use a wide range of MCP servers to provide tools to your Agents.

## MCP servers

Currently, the MCP spec defines two kinds of servers, based on the transport mechanism they use:

1. **stdio** servers run as a subprocess of your application. You can think of them as running "locally".
2. **HTTP over SSE** servers run remotely. You connect to them via a URL.

You can use the [`MCPServerStdio`][agents.mcp.server.MCPServerStdio] and [`MCPServerSse`][agents.mcp.server.MCPServerSse] classes to connect to these servers.

For example, this is how you'd use the [official MCP filesystem server](https://www.npmjs.com/package/@modelcontextprotocol/server-filesystem).

```python
async with MCPServerStdio(
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", samples_dir],
    }
) as server:
    tools = await server.list_tools()
```

## Using MCP servers

MCP servers can be added to Agents. The Agents SDK will call `list_tools()` on the MCP servers each time the Agent is run. This makes the LLM aware of the MCP server's tools. When the LLM calls a tool from an MCP server, the SDK calls `call_tool()` on that server.

```python

agent=Agent(
    name="Assistant",
    instructions="Use the tools to achieve the task",
    mcp_servers=[mcp_server_1, mcp_server_2]
)
```

## Caching

Every time an Agent runs, it calls `list_tools()` on the MCP server. This can be a latency hit, especially if the server is a remote server. To automatically cache the list of tools, you can pass `cache_tools_list=True` to both [`MCPServerStdio`][agents.mcp.server.MCPServerStdio] and [`MCPServerSse`][agents.mcp.server.MCPServerSse]. You should only do this if you're certain the tool list will not change.

If you want to invalidate the cache, you can call `invalidate_tools_cache()` on the servers.

## End-to-end examples

View complete working examples at [examples/mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp).

## Tracing

[Tracing](./tracing.md) automatically captures MCP operations, including:

1. Calls to the MCP server to list tools
2. MCP-related info on function calls

![MCP Tracing Screenshot](./assets/images/mcp-tracing.jpg)
</file>

<file path="docs/multi_agent.md">
# Orchestrating multiple agents

Orchestration refers to the flow of agents in your app. Which agents run, in what order, and how do they decide what happens next? There are two main ways to orchestrate agents:

1. Allowing the LLM to make decisions: this uses the intelligence of an LLM to plan, reason, and decide on what steps to take based on that.
2. Orchestrating via code: determining the flow of agents via your code.

You can mix and match these patterns. Each has their own tradeoffs, described below.

## Orchestrating via LLM

An agent is an LLM equipped with instructions, tools and handoffs. This means that given an open-ended task, the LLM can autonomously plan how it will tackle the task, using tools to take actions and acquire data, and using handoffs to delegate tasks to sub-agents. For example, a research agent could be equipped with tools like:

-   Web search to find information online
-   File search and retrieval to search through proprietary data and connections
-   Computer use to take actions on a computer
-   Code execution to do data analysis
-   Handoffs to specialized agents that are great at planning, report writing and more.

This pattern is great when the task is open-ended and you want to rely on the intelligence of an LLM. The most important tactics here are:

1. Invest in good prompts. Make it clear what tools are available, how to use them, and what parameters it must operate within.
2. Monitor your app and iterate on it. See where things go wrong, and iterate on your prompts.
3. Allow the agent to introspect and improve. For example, run it in a loop, and let it critique itself; or, provide error messages and let it improve.
4. Have specialized agents that excel in one task, rather than having a general purpose agent that is expected to be good at anything.
5. Invest in [evals](https://platform.openai.com/docs/guides/evals). This lets you train your agents to improve and get better at tasks.

## Orchestrating via code

While orchestrating via LLM is powerful, orchestrating via code makes tasks more deterministic and predictable, in terms of speed, cost and performance. Common patterns here are:

-   Using [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) to generate well formed data that you can inspect with your code. For example, you might ask an agent to classify the task into a few categories, and then pick the next agent based on the category.
-   Chaining multiple agents by transforming the output of one into the input of the next. You can decompose a task like writing a blog post into a series of steps - do research, write an outline, write the blog post, critique it, and then improve it.
-   Running the agent that performs the task in a `while` loop with an agent that evaluates and provides feedback, until the evaluator says the output passes certain criteria.
-   Running multiple agents in parallel, e.g. via Python primitives like `asyncio.gather`. This is useful for speed when you have multiple tasks that don't depend on each other.

We have a number of examples in [`examples/agent_patterns`](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns).
</file>

<file path="docs/quickstart.md">
# Quickstart

## Create a project and virtual environment

You'll only need to do this once.

```bash
mkdir my_project
cd my_project
python -m venv .venv
```

### Activate the virtual environment

Do this every time you start a new terminal session.

```bash
source .venv/bin/activate
```

### Install the Agents SDK

```bash
pip install openai-agents # or `uv add openai-agents`, etc
```

### Set an OpenAI API key

If you don't have one, follow [these instructions](https://platform.openai.com/docs/quickstart#create-and-export-an-api-key) to create an OpenAI API key.

```bash
export OPENAI_API_KEY=sk-...
```

## Create your first agent

Agents are defined with instructions, a name, and optional config (such as `model_config`)

```python
from agents import Agent

agent = Agent(
    name="Math Tutor",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)
```

## Add a few more agents

Additional agents can be defined in the same way. `handoff_descriptions` provide additional context for determining handoff routing

```python
from agents import Agent

history_tutor_agent = Agent(
    name="History Tutor",
    handoff_description="Specialist agent for historical questions",
    instructions="You provide assistance with historical queries. Explain important events and context clearly.",
)

math_tutor_agent = Agent(
    name="Math Tutor",
    handoff_description="Specialist agent for math questions",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)
```

## Define your handoffs

On each agent, you can define an inventory of outgoing handoff options that the agent can choose from to decide how to make progress on their task.

```python
triage_agent = Agent(
    name="Triage Agent",
    instructions="You determine which agent to use based on the user's homework question",
    handoffs=[history_tutor_agent, math_tutor_agent]
)
```

## Run the agent orchestration

Let's check that the workflow runs and the triage agent correctly routes between the two specialist agents.

```python
from agents import Runner

async def main():
    result = await Runner.run(triage_agent, "What is the capital of France?")
    print(result.final_output)
```

## Add a guardrail

You can define custom guardrails to run on the input or output.

```python
from agents import GuardrailFunctionOutput, Agent, Runner
from pydantic import BaseModel

class HomeworkOutput(BaseModel):
    is_homework: bool
    reasoning: str

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the user is asking about homework.",
    output_type=HomeworkOutput,
)

async def homework_guardrail(ctx, agent, input_data):
    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
    final_output = result.final_output_as(HomeworkOutput)
    return GuardrailFunctionOutput(
        output_info=final_output,
        tripwire_triggered=not final_output.is_homework,
    )
```

## Put it all together

Let's put it all together and run the entire workflow, using handoffs and the input guardrail.

```python
from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner
from pydantic import BaseModel
import asyncio

class HomeworkOutput(BaseModel):
    is_homework: bool
    reasoning: str

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the user is asking about homework.",
    output_type=HomeworkOutput,
)

math_tutor_agent = Agent(
    name="Math Tutor",
    handoff_description="Specialist agent for math questions",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)

history_tutor_agent = Agent(
    name="History Tutor",
    handoff_description="Specialist agent for historical questions",
    instructions="You provide assistance with historical queries. Explain important events and context clearly.",
)


async def homework_guardrail(ctx, agent, input_data):
    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
    final_output = result.final_output_as(HomeworkOutput)
    return GuardrailFunctionOutput(
        output_info=final_output,
        tripwire_triggered=not final_output.is_homework,
    )

triage_agent = Agent(
    name="Triage Agent",
    instructions="You determine which agent to use based on the user's homework question",
    handoffs=[history_tutor_agent, math_tutor_agent],
    input_guardrails=[
        InputGuardrail(guardrail_function=homework_guardrail),
    ],
)

async def main():
    result = await Runner.run(triage_agent, "who was the first president of the united states?")
    print(result.final_output)

    result = await Runner.run(triage_agent, "what is life")
    print(result.final_output)

if __name__ == "__main__":
    asyncio.run(main())
```

## View your traces

To review what happened during your agent run, navigate to the [Trace viewer in the OpenAI Dashboard](https://platform.openai.com/traces) to view traces of your agent runs.

## Next steps

Learn how to build more complex agentic flows:

-   Learn about how to configure [Agents](agents.md).
-   Learn about [running agents](running_agents.md).
-   Learn about [tools](tools.md), [guardrails](guardrails.md) and [models](models/index.md).
</file>

<file path="docs/results.md">
# Results

When you call the `Runner.run` methods, you either get a:

-   [`RunResult`][agents.result.RunResult] if you call `run` or `run_sync`
-   [`RunResultStreaming`][agents.result.RunResultStreaming] if you call `run_streamed`

Both of these inherit from [`RunResultBase`][agents.result.RunResultBase], which is where most useful information is present.

## Final output

The [`final_output`][agents.result.RunResultBase.final_output] property contains the final output of the last agent that ran. This is either:

-   a `str`, if the last agent didn't have an `output_type` defined
-   an object of type `last_agent.output_type`, if the agent had an output type defined.

!!! note

    `final_output` is of type `Any`. We can't statically type this, because of handoffs. If handoffs occur, that means any Agent might be the last agent, so we don't statically know the set of possible output types.

## Inputs for the next turn

You can use [`result.to_input_list()`][agents.result.RunResultBase.to_input_list] to turn the result into an input list that concatenates the original input you provided, to the items generated during the agent run. This makes it convenient to take the outputs of one agent run and pass them into another run, or to run it in a loop and append new user inputs each time.

## Last agent

The [`last_agent`][agents.result.RunResultBase.last_agent] property contains the last agent that ran. Depending on your application, this is often useful for the next time the user inputs something. For example, if you have a frontline triage agent that hands off to a language-specific agent, you can store the last agent, and re-use it the next time the user messages the agent.

## New items

The [`new_items`][agents.result.RunResultBase.new_items] property contains the new items generated during the run. The items are [`RunItem`][agents.items.RunItem]s. A run item wraps the raw item generated by the LLM.

-   [`MessageOutputItem`][agents.items.MessageOutputItem] indicates a message from the LLM. The raw item is the message generated.
-   [`HandoffCallItem`][agents.items.HandoffCallItem] indicates that the LLM called the handoff tool. The raw item is the tool call item from the LLM.
-   [`HandoffOutputItem`][agents.items.HandoffOutputItem] indicates that a handoff occurred. The raw item is the tool response to the handoff tool call. You can also access the source/target agents from the item.
-   [`ToolCallItem`][agents.items.ToolCallItem] indicates that the LLM invoked a tool.
-   [`ToolCallOutputItem`][agents.items.ToolCallOutputItem] indicates that a tool was called. The raw item is the tool response. You can also access the tool output from the item.
-   [`ReasoningItem`][agents.items.ReasoningItem] indicates a reasoning item from the LLM. The raw item is the reasoning generated.

## Other information

### Guardrail results

The [`input_guardrail_results`][agents.result.RunResultBase.input_guardrail_results] and [`output_guardrail_results`][agents.result.RunResultBase.output_guardrail_results] properties contain the results of the guardrails, if any. Guardrail results can sometimes contain useful information you want to log or store, so we make these available to you.

### Raw responses

The [`raw_responses`][agents.result.RunResultBase.raw_responses] property contains the [`ModelResponse`][agents.items.ModelResponse]s generated by the LLM.

### Original input

The [`input`][agents.result.RunResultBase.input] property contains the original input you provided to the `run` method. In most cases you won't need this, but it's available in case you do.
</file>

<file path="docs/running_agents.md">
# Running agents

You can run agents via the [`Runner`][agents.run.Runner] class. You have 3 options:

1. [`Runner.run()`][agents.run.Runner.run], which runs async and returns a [`RunResult`][agents.result.RunResult].
2. [`Runner.run_sync()`][agents.run.Runner.run_sync], which is a sync method and just runs `.run()` under the hood.
3. [`Runner.run_streamed()`][agents.run.Runner.run_streamed], which runs async and returns a [`RunResultStreaming`][agents.result.RunResultStreaming]. It calls the LLM in streaming mode, and streams those events to you as they are received.

```python
from agents import Agent, Runner

async def main():
    agent = Agent(name="Assistant", instructions="You are a helpful assistant")

    result = await Runner.run(agent, "Write a haiku about recursion in programming.")
    print(result.final_output)
    # Code within the code,
    # Functions calling themselves,
    # Infinite loop's dance.
```

Read more in the [results guide](results.md).

## The agent loop

When you use the run method in `Runner`, you pass in a starting agent and input. The input can either be a string (which is considered a user message), or a list of input items, which are the items in the OpenAI Responses API.

The runner then runs a loop:

1. We call the LLM for the current agent, with the current input.
2. The LLM produces its output.
    1. If the LLM returns a `final_output`, the loop ends and we return the result.
    2. If the LLM does a handoff, we update the current agent and input, and re-run the loop.
    3. If the LLM produces tool calls, we run those tool calls, append the results, and re-run the loop.
3. If we exceed the `max_turns` passed, we raise a [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded] exception.

!!! note

    The rule for whether the LLM output is considered as a "final output" is that it produces text output with the desired type, and there are no tool calls.

## Streaming

Streaming allows you to additionally receive streaming events as the LLM runs. Once the stream is done, the [`RunResultStreaming`][agents.result.RunResultStreaming] will contain the complete information about the run, including all the new outputs produces. You can call `.stream_events()` for the streaming events. Read more in the [streaming guide](streaming.md).

## Run config

The `run_config` parameter lets you configure some global settings for the agent run:

-   [`model`][agents.run.RunConfig.model]: Allows setting a global LLM model to use, irrespective of what `model` each Agent has.
-   [`model_provider`][agents.run.RunConfig.model_provider]: A model provider for looking up model names, which defaults to OpenAI.
-   [`model_settings`][agents.run.RunConfig.model_settings]: Overrides agent-specific settings. For example, you can set a global `temperature` or `top_p`.
-   [`input_guardrails`][agents.run.RunConfig.input_guardrails], [`output_guardrails`][agents.run.RunConfig.output_guardrails]: A list of input or output guardrails to include on all runs.
-   [`handoff_input_filter`][agents.run.RunConfig.handoff_input_filter]: A global input filter to apply to all handoffs, if the handoff doesn't already have one. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] for more details.
-   [`tracing_disabled`][agents.run.RunConfig.tracing_disabled]: Allows you to disable [tracing](tracing.md) for the entire run.
-   [`trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data]: Configures whether traces will include potentially sensitive data, such as LLM and tool call inputs/outputs.
-   [`workflow_name`][agents.run.RunConfig.workflow_name], [`trace_id`][agents.run.RunConfig.trace_id], [`group_id`][agents.run.RunConfig.group_id]: Sets the tracing workflow name, trace ID and trace group ID for the run. We recommend at least setting `workflow_name`. The group ID is an optional field that lets you link traces across multiple runs.
-   [`trace_metadata`][agents.run.RunConfig.trace_metadata]: Metadata to include on all traces.

## Conversations/chat threads

Calling any of the run methods can result in one or more agents running (and hence one or more LLM calls), but it represents a single logical turn in a chat conversation. For example:

1. User turn: user enter text
2. Runner run: first agent calls LLM, runs tools, does a handoff to a second agent, second agent runs more tools, and then produces an output.

At the end of the agent run, you can choose what to show to the user. For example, you might show the user every new item generated by the agents, or just the final output. Either way, the user might then ask a followup question, in which case you can call the run method again.

You can use the base [`RunResultBase.to_input_list()`][agents.result.RunResultBase.to_input_list] method to get the inputs for the next turn.

```python
async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    with trace(workflow_name="Conversation", group_id=thread_id):
        # First turn
        result = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
        print(result.final_output)
        # San Francisco

        # Second turn
        new_input = result.to_input_list() + [{"role": "user", "content": "What state is it in?"}]
        result = await Runner.run(agent, new_input)
        print(result.final_output)
        # California
```

## Exceptions

The SDK raises exceptions in certain cases. The full list is in [`agents.exceptions`][]. As an overview:

-   [`AgentsException`][agents.exceptions.AgentsException] is the base class for all exceptions raised in the SDK.
-   [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded] is raised when the run exceeds the `max_turns` passed to the run methods.
-   [`ModelBehaviorError`][agents.exceptions.ModelBehaviorError] is raised when the model produces invalid outputs, e.g. malformed JSON or using non-existent tools.
-   [`UserError`][agents.exceptions.UserError] is raised when you (the person writing code using the SDK) make an error using the SDK.
-   [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered], [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered] is raised when a [guardrail](guardrails.md) is tripped.
</file>

<file path="docs/streaming.md">
# Streaming

Streaming lets you subscribe to updates of the agent run as it proceeds. This can be useful for showing the end-user progress updates and partial responses.

To stream, you can call [`Runner.run_streamed()`][agents.run.Runner.run_streamed], which will give you a [`RunResultStreaming`][agents.result.RunResultStreaming]. Calling `result.stream_events()` gives you an async stream of [`StreamEvent`][agents.stream_events.StreamEvent] objects, which are described below.

## Raw response events

[`RawResponsesStreamEvent`][agents.stream_events.RawResponsesStreamEvent] are raw events passed directly from the LLM. They are in OpenAI Responses API format, which means each event has a type (like `response.created`, `response.output_text.delta`, etc) and data. These events are useful if you want to stream response messages to the user as soon as they are generated.

For example, this will output the text generated by the LLM token-by-token.

```python
import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner

async def main():
    agent = Agent(
        name="Joker",
        instructions="You are a helpful assistant.",
    )

    result = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            print(event.data.delta, end="", flush=True)


if __name__ == "__main__":
    asyncio.run(main())
```

## Run item events and agent events

[`RunItemStreamEvent`][agents.stream_events.RunItemStreamEvent]s are higher level events. They inform you when an item has been fully generated. This allows you to push progress updates at the level of "message generated", "tool ran", etc, instead of each token. Similarly, [`AgentUpdatedStreamEvent`][agents.stream_events.AgentUpdatedStreamEvent] gives you updates when the current agent changes (e.g. as the result of a handoff).

For example, this will ignore raw events and stream updates to the user.

```python
import asyncio
import random
from agents import Agent, ItemHelpers, Runner, function_tool

@function_tool
def how_many_jokes() -> int:
    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="Joker",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],
    )

    result = Runner.run_streamed(
        agent,
        input="Hello",
    )
    print("=== Run starting ===")

    async for event in result.stream_events():
        # We'll ignore the raw responses event deltas
        if event.type == "raw_response_event":
            continue
        # When the agent updates, print that
        elif event.type == "agent_updated_stream_event":
            print(f"Agent updated: {event.new_agent.name}")
            continue
        # When items are generated, print them
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}")
            else:
                pass  # Ignore other event types

    print("=== Run complete ===")


if __name__ == "__main__":
    asyncio.run(main())
```
</file>

<file path="docs/tools.md">
# Tools

Tools let agents take actions: things like fetching data, running code, calling external APIs, and even using a computer. There are three classes of tools in the Agent SDK:

-   Hosted tools: these run on LLM servers alongside the AI models. OpenAI offers retrieval, web search and computer use as hosted tools.
-   Function calling: these allow you to use any Python function as a tool.
-   Agents as tools: this allows you to use an agent as a tool, allowing Agents to call other agents without handing off to them.

## Hosted tools

OpenAI offers a few built-in tools when using the [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel]:

-   The [`WebSearchTool`][agents.tool.WebSearchTool] lets an agent search the web.
-   The [`FileSearchTool`][agents.tool.FileSearchTool] allows retrieving information from your OpenAI Vector Stores.
-   The [`ComputerTool`][agents.tool.ComputerTool] allows automating computer use tasks.

```python
from agents import Agent, FileSearchTool, Runner, WebSearchTool

agent = Agent(
    name="Assistant",
    tools=[
        WebSearchTool(),
        FileSearchTool(
            max_num_results=3,
            vector_store_ids=["VECTOR_STORE_ID"],
        ),
    ],
)

async def main():
    result = await Runner.run(agent, "Which coffee shop should I go to, taking into account my preferences and the weather today in SF?")
    print(result.final_output)
```

## Function tools

You can use any Python function as a tool. The Agents SDK will setup the tool automatically:

-   The name of the tool will be the name of the Python function (or you can provide a name)
-   Tool description will be taken from the docstring of the function (or you can provide a description)
-   The schema for the function inputs is automatically created from the function's arguments
-   Descriptions for each input are taken from the docstring of the function, unless disabled

We use Python's `inspect` module to extract the function signature, along with [`griffe`](https://mkdocstrings.github.io/griffe/) to parse docstrings and `pydantic` for schema creation.

```python
import json

from typing_extensions import TypedDict, Any

from agents import Agent, FunctionTool, RunContextWrapper, function_tool


class Location(TypedDict):
    lat: float
    long: float

@function_tool  # (1)!
async def fetch_weather(location: Location) -> str:
    # (2)!
    """Fetch the weather for a given location.

    Args:
        location: The location to fetch the weather for.
    """
    # In real life, we'd fetch the weather from a weather API
    return "sunny"


@function_tool(name_override="fetch_data")  # (3)!
def read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -> str:
    """Read the contents of a file.

    Args:
        path: The path to the file to read.
        directory: The directory to read the file from.
    """
    # In real life, we'd read the file from the file system
    return "<file contents>"


agent = Agent(
    name="Assistant",
    tools=[fetch_weather, read_file],  # (4)!
)

for tool in agent.tools:
    if isinstance(tool, FunctionTool):
        print(tool.name)
        print(tool.description)
        print(json.dumps(tool.params_json_schema, indent=2))
        print()

```

1.  You can use any Python types as arguments to your functions, and the function can be sync or async.
2.  Docstrings, if present, are used to capture descriptions and argument descriptions
3.  Functions can optionally take the `context` (must be the first argument). You can also set overrides, like the name of the tool, description, which docstring style to use, etc.
4.  You can pass the decorated functions to the list of tools.

??? note "Expand to see output"

    ```
    fetch_weather
    Fetch the weather for a given location.
    {
    "$defs": {
      "Location": {
        "properties": {
          "lat": {
            "title": "Lat",
            "type": "number"
          },
          "long": {
            "title": "Long",
            "type": "number"
          }
        },
        "required": [
          "lat",
          "long"
        ],
        "title": "Location",
        "type": "object"
      }
    },
    "properties": {
      "location": {
        "$ref": "#/$defs/Location",
        "description": "The location to fetch the weather for."
      }
    },
    "required": [
      "location"
    ],
    "title": "fetch_weather_args",
    "type": "object"
    }

    fetch_data
    Read the contents of a file.
    {
    "properties": {
      "path": {
        "description": "The path to the file to read.",
        "title": "Path",
        "type": "string"
      },
      "directory": {
        "anyOf": [
          {
            "type": "string"
          },
          {
            "type": "null"
          }
        ],
        "default": null,
        "description": "The directory to read the file from.",
        "title": "Directory"
      }
    },
    "required": [
      "path"
    ],
    "title": "fetch_data_args",
    "type": "object"
    }
    ```

### Custom function tools

Sometimes, you don't want to use a Python function as a tool. You can directly create a [`FunctionTool`][agents.tool.FunctionTool] if you prefer. You'll need to provide:

-   `name`
-   `description`
-   `params_json_schema`, which is the JSON schema for the arguments
-   `on_invoke_tool`, which is an async function that receives the context and the arguments as a JSON string, and must return the tool output as a string.

```python
from typing import Any

from pydantic import BaseModel

from agents import RunContextWrapper, FunctionTool



def do_some_work(data: str) -> str:
    return "done"


class FunctionArgs(BaseModel):
    username: str
    age: int


async def run_function(ctx: RunContextWrapper[Any], args: str) -> str:
    parsed = FunctionArgs.model_validate_json(args)
    return do_some_work(data=f"{parsed.username} is {parsed.age} years old")


tool = FunctionTool(
    name="process_user",
    description="Processes extracted user data",
    params_json_schema=FunctionArgs.model_json_schema(),
    on_invoke_tool=run_function,
)
```

### Automatic argument and docstring parsing

As mentioned before, we automatically parse the function signature to extract the schema for the tool, and we parse the docstring to extract descriptions for the tool and for individual arguments. Some notes on that:

1. The signature parsing is done via the `inspect` module. We use type annotations to understand the types for the arguments, and dynamically build a Pydantic model to represent the overall schema. It supports most types, including Python primitives, Pydantic models, TypedDicts, and more.
2. We use `griffe` to parse docstrings. Supported docstring formats are `google`, `sphinx` and `numpy`. We attempt to automatically detect the docstring format, but this is best-effort and you can explicitly set it when calling `function_tool`. You can also disable docstring parsing by setting `use_docstring_info` to `False`.

The code for the schema extraction lives in [`agents.function_schema`][].

## Agents as tools

In some workflows, you may want a central agent to orchestrate a network of specialized agents, instead of handing off control. You can do this by modeling agents as tools.

```python
from agents import Agent, Runner
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You translate the user's message to Spanish",
)

french_agent = Agent(
    name="French agent",
    instructions="You translate the user's message to French",
)

orchestrator_agent = Agent(
    name="orchestrator_agent",
    instructions=(
        "You are a translation agent. You use the tools given to you to translate."
        "If asked for multiple translations, you call the relevant tools."
    ),
    tools=[
        spanish_agent.as_tool(
            tool_name="translate_to_spanish",
            tool_description="Translate the user's message to Spanish",
        ),
        french_agent.as_tool(
            tool_name="translate_to_french",
            tool_description="Translate the user's message to French",
        ),
    ],
)

async def main():
    result = await Runner.run(orchestrator_agent, input="Say 'Hello, how are you?' in Spanish.")
    print(result.final_output)
```

### Customizing tool-agents

The `agent.as_tool` function is a convenience method to make it easy to turn an agent into a tool. It doesn't support all configuration though; for example, you can't set `max_turns`. For advanced use cases, use `Runner.run` directly in your tool implementation:

```python
@function_tool
async def run_my_agent() -> str:
  """A tool that runs the agent with custom configs".

    agent = Agent(name="My agent", instructions="...")

    result = await Runner.run(
        agent,
        input="...",
        max_turns=5,
        run_config=...
    )

    return str(result.final_output)
```

## Handling errors in function tools

When you create a function tool via `@function_tool`, you can pass a `failure_error_function`. This is a function that provides an error response to the LLM in case the tool call crashes.

-   By default (i.e. if you don't pass anything), it runs a `default_tool_error_function` which tells the LLM an error occurred.
-   If you pass your own error function, it runs that instead, and sends the response to the LLM.
-   If you explicitly pass `None`, then any tool call errors will be re-raised for you to handle. This could be a `ModelBehaviorError` if the model produced invalid JSON, or a `UserError` if your code crashed, etc.

If you are manually creating a `FunctionTool` object, then you must handle errors inside the `on_invoke_tool` function.
</file>

<file path="docs/tracing.md">
# Tracing

The Agents SDK includes built-in tracing, collecting a comprehensive record of events during an agent run: LLM generations, tool calls, handoffs, guardrails, and even custom events that occur. Using the [Traces dashboard](https://platform.openai.com/traces), you can debug, visualize, and monitor your workflows during development and in production.

!!!note

    Tracing is enabled by default. There are two ways to disable tracing:

    1. You can globally disable tracing by setting the env var `OPENAI_AGENTS_DISABLE_TRACING=1`
    2. You can disable tracing for a single run by setting [`agents.run.RunConfig.tracing_disabled`][] to `True`

***For organizations operating under a Zero Data Retention (ZDR) policy using OpenAI's APIs, tracing is unavailable.***

## Traces and spans

-   **Traces** represent a single end-to-end operation of a "workflow". They're composed of Spans. Traces have the following properties:
    -   `workflow_name`: This is the logical workflow or app. For example "Code generation" or "Customer service".
    -   `trace_id`: A unique ID for the trace. Automatically generated if you don't pass one. Must have the format `trace_<32_alphanumeric>`.
    -   `group_id`: Optional group ID, to link multiple traces from the same conversation. For example, you might use a chat thread ID.
    -   `disabled`: If True, the trace will not be recorded.
    -   `metadata`: Optional metadata for the trace.
-   **Spans** represent operations that have a start and end time. Spans have:
    -   `started_at` and `ended_at` timestamps.
    -   `trace_id`, to represent the trace they belong to
    -   `parent_id`, which points to the parent Span of this Span (if any)
    -   `span_data`, which is information about the Span. For example, `AgentSpanData` contains information about the Agent, `GenerationSpanData` contains information about the LLM generation, etc.

## Default tracing

By default, the SDK traces the following:

-   The entire `Runner.{run, run_sync, run_streamed}()` is wrapped in a `trace()`.
-   Each time an agent runs, it is wrapped in `agent_span()`
-   LLM generations are wrapped in `generation_span()`
-   Function tool calls are each wrapped in `function_span()`
-   Guardrails are wrapped in `guardrail_span()`
-   Handoffs are wrapped in `handoff_span()`
-   Audio inputs (speech-to-text) are wrapped in a `transcription_span()`
-   Audio outputs (text-to-speech) are wrapped in a `speech_span()`
-   Related audio spans may be parented under a `speech_group_span()`

By default, the trace is named "Agent trace". You can set this name if you use `trace`, or you can can configure the name and other properties with the [`RunConfig`][agents.run.RunConfig].

In addition, you can set up [custom trace processors](#custom-tracing-processors) to push traces to other destinations (as a replacement, or secondary destination).

## Higher level traces

Sometimes, you might want multiple calls to `run()` to be part of a single trace. You can do this by wrapping the entire code in a `trace()`.

```python
from agents import Agent, Runner, trace

async def main():
    agent = Agent(name="Joke generator", instructions="Tell funny jokes.")

    with trace("Joke workflow"): # (1)!
        first_result = await Runner.run(agent, "Tell me a joke")
        second_result = await Runner.run(agent, f"Rate this joke: {first_result.final_output}")
        print(f"Joke: {first_result.final_output}")
        print(f"Rating: {second_result.final_output}")
```

1. Because the two calls to `Runner.run` are wrapped in a `with trace()`, the individual runs will be part of the overall trace rather than creating two traces.

## Creating traces

You can use the [`trace()`][agents.tracing.trace] function to create a trace. Traces need to be started and finished. You have two options to do so:

1. **Recommended**: use the trace as a context manager, i.e. `with trace(...) as my_trace`. This will automatically start and end the trace at the right time.
2. You can also manually call [`trace.start()`][agents.tracing.Trace.start] and [`trace.finish()`][agents.tracing.Trace.finish].

The current trace is tracked via a Python [`contextvar`](https://docs.python.org/3/library/contextvars.html). This means that it works with concurrency automatically. If you manually start/end a trace, you'll need to pass `mark_as_current` and `reset_current` to `start()`/`finish()` to update the current trace.

## Creating spans

You can use the various [`*_span()`][agents.tracing.create] methods to create a span. In general, you don't need to manually create spans. A [`custom_span()`][agents.tracing.custom_span] function is available for tracking custom span information.

Spans are automatically part of the current trace, and are nested under the nearest current span, which is tracked via a Python [`contextvar`](https://docs.python.org/3/library/contextvars.html).

## Sensitive data

Certain spans may capture potentially sensitive data.

The `generation_span()` stores the inputs/outputs of the LLM generation, and `function_span()` stores the inputs/outputs of function calls. These may contain sensitive data, so you can disable capturing that data via [`RunConfig.trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data].

Similarly, Audio spans include base64-encoded PCM data for input and output audio by default. You can disable capturing this audio data by configuring [`VoicePipelineConfig.trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data].

## Custom tracing processors

The high level architecture for tracing is:

-   At initialization, we create a global [`TraceProvider`][agents.tracing.setup.TraceProvider], which is responsible for creating traces.
-   We configure the `TraceProvider` with a [`BatchTraceProcessor`][agents.tracing.processors.BatchTraceProcessor] that sends traces/spans in batches to a [`BackendSpanExporter`][agents.tracing.processors.BackendSpanExporter], which exports the spans and traces to the OpenAI backend in batches.

To customize this default setup, to send traces to alternative or additional backends or modifying exporter behavior, you have two options:

1. [`add_trace_processor()`][agents.tracing.add_trace_processor] lets you add an **additional** trace processor that will receive traces and spans as they are ready. This lets you do your own processing in addition to sending traces to OpenAI's backend.
2. [`set_trace_processors()`][agents.tracing.set_trace_processors] lets you **replace** the default processors with your own trace processors. This means traces will not be sent to the OpenAI backend unless you include a `TracingProcessor` that does so.

## External tracing processors list

-   [Weights & Biases](https://weave-docs.wandb.ai/guides/integrations/openai_agents)
-   [Arize-Phoenix](https://docs.arize.com/phoenix/tracing/integrations-tracing/openai-agents-sdk)
-   [MLflow (self-hosted/OSS](https://mlflow.org/docs/latest/tracing/integrations/openai-agent)
-   [MLflow (Databricks hosted](https://docs.databricks.com/aws/en/mlflow/mlflow-tracing#-automatic-tracing)
-   [Braintrust](https://braintrust.dev/docs/guides/traces/integrations#openai-agents-sdk)
-   [Pydantic Logfire](https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents)
-   [AgentOps](https://docs.agentops.ai/v1/integrations/agentssdk)
-   [Scorecard](https://docs.scorecard.io/docs/documentation/features/tracing#openai-agents-sdk-integration)
-   [Keywords AI](https://docs.keywordsai.co/integration/development-frameworks/openai-agent)
-   [LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
-   [Maxim AI](https://www.getmaxim.ai/docs/observe/integrations/openai-agents-sdk)
-   [Comet Opik](https://www.comet.com/docs/opik/tracing/integrations/openai_agents)
-   [Langfuse](https://langfuse.com/docs/integrations/openaiagentssdk/openai-agents)
-   [Langtrace](https://docs.langtrace.ai/supported-integrations/llm-frameworks/openai-agents-sdk)
-   [Okahu-Monocle](https://github.com/monocle2ai/monocle)
</file>

<file path="docs/visualization.md">
# Agent Visualization

Agent visualization allows you to generate a structured graphical representation of agents and their relationships using **Graphviz**. This is useful for understanding how agents, tools, and handoffs interact within an application.

## Installation

Install the optional `viz` dependency group:

```bash
pip install "openai-agents[viz]"
```

## Generating a Graph

You can generate an agent visualization using the `draw_graph` function. This function creates a directed graph where:

- **Agents** are represented as yellow boxes.
- **Tools** are represented as green ellipses.
- **Handoffs** are directed edges from one agent to another.

### Example Usage

```python
from agents import Agent, function_tool
from agents.extensions.visualization import draw_graph

@function_tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    tools=[get_weather],
)

draw_graph(triage_agent)
```

![Agent Graph](./assets/images/graph.png)

This generates a graph that visually represents the structure of the **triage agent** and its connections to sub-agents and tools.


## Understanding the Visualization

The generated graph includes:

- A **start node** (`__start__`) indicating the entry point.
- Agents represented as **rectangles** with yellow fill.
- Tools represented as **ellipses** with green fill.
- Directed edges indicating interactions:
  - **Solid arrows** for agent-to-agent handoffs.
  - **Dotted arrows** for tool invocations.
- An **end node** (`__end__`) indicating where execution terminates.

## Customizing the Graph

### Showing the Graph
By default, `draw_graph` displays the graph inline. To show the graph in a separate window, write the following:

```python
draw_graph(triage_agent).view()
```

### Saving the Graph
By default, `draw_graph` displays the graph inline. To save it as a file, specify a filename:

```python
draw_graph(triage_agent, filename="agent_graph.png")
```

This will generate `agent_graph.png` in the working directory.
</file>

</files>
