================================================================
Files
================================================================

================
File: docs/ko/models/index.md
================
---
search:
  exclude: true
---
# ëª¨ë¸

Agents SDK ëŠ” OpenAI ëª¨ë¸ì„ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ê¸°ë³¸ ì§€ì›í•©ë‹ˆë‹¤:

- **ê¶Œì¥**: ìƒˆë¡œìš´ [Responses API](https://platform.openai.com/docs/api-reference/responses)ë¥¼ ì‚¬ìš©í•´ OpenAI API ë¥¼ í˜¸ì¶œí•˜ëŠ” [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel]
- [Chat Completions API](https://platform.openai.com/docs/api-reference/chat)ë¥¼ ì‚¬ìš©í•´ OpenAI API ë¥¼ í˜¸ì¶œí•˜ëŠ” [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel]

## OpenAI ëª¨ë¸

`Agent`ë¥¼ ì´ˆê¸°í™”í•  ë•Œ ëª¨ë¸ì„ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. í˜„ì¬ ê¸°ë³¸ê°’ì€ [`gpt-4.1`](https://platform.openai.com/docs/models/gpt-4.1)ì´ë©°, ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œì˜ ì˜ˆì¸¡ ê°€ëŠ¥ì„±ê³¼ ë‚®ì€ ì§€ì—° ì‹œê°„ ì‚¬ì´ì— ì¢‹ì€ ê· í˜•ì„ ì œê³µí•©ë‹ˆë‹¤.

[`gpt-5`](https://platform.openai.com/docs/models/gpt-5) ê°™ì€ ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì „í™˜í•˜ë ¤ë©´ ë‹¤ìŒ ì„¹ì…˜ì˜ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”.

### ê¸°ë³¸ OpenAI ëª¨ë¸

ì‚¬ìš©ì ì§€ì • ëª¨ë¸ì„ ì„¤ì •í•˜ì§€ ì•Šì€ ëª¨ë“  ì—ì´ì „íŠ¸ì— ëŒ€í•´ íŠ¹ì • ëª¨ë¸ì„ ì¼ê´€ë˜ê²Œ ì‚¬ìš©í•˜ë ¤ë©´, ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— `OPENAI_DEFAULT_MODEL` í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

```bash
export OPENAI_DEFAULT_MODEL=gpt-5
python3 my_awesome_agent.py
```

#### GPT-5 ëª¨ë¸

GPT-5 ì˜ reasoning ëª¨ë¸ë“¤([`gpt-5`](https://platform.openai.com/docs/models/gpt-5), [`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini), [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano))ì„ ì´ë ‡ê²Œ ì‚¬ìš©í•  ë•Œ, SDK ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í•©ë¦¬ì ì¸ `ModelSettings` ë¥¼ ì ìš©í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ `reasoning.effort`ì™€ `verbosity`ë¥¼ ëª¨ë‘ `"low"`ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì„¤ì •ì„ ì§ì ‘ êµ¬ì„±í•˜ë ¤ë©´ `agents.models.get_default_model_settings("gpt-5")`ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.

ë” ë‚®ì€ ì§€ì—° ì‹œê°„ì´ë‚˜ íŠ¹ì • ìš”êµ¬ ì‚¬í•­ì´ ìˆë‹¤ë©´, ë‹¤ë¥¸ ëª¨ë¸ê³¼ ì„¤ì •ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì˜ reasoning effort ë¥¼ ì¡°ì •í•˜ë ¤ë©´, ì§ì ‘ ì •ì˜í•œ `ModelSettings` ë¥¼ ì „ë‹¬í•˜ì„¸ìš”:

```python
from openai.types.shared import Reasoning
from agents import Agent, ModelSettings

my_agent = Agent(
    name="My Agent",
    instructions="You're a helpful agent.",
    model_settings=ModelSettings(reasoning=Reasoning(effort="minimal"), verbosity="low")
    # If OPENAI_DEFAULT_MODEL=gpt-5 is set, passing only model_settings works.
    # It's also fine to pass a GPT-5 model name explicitly:
    # model="gpt-5",
)
```

íŠ¹íˆ ì§€ì—° ì‹œê°„ì„ ë” ë‚®ì¶”ë ¤ë©´, [`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini) ë˜ëŠ” [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano) ëª¨ë¸ì— `reasoning.effort="minimal"`ì„ ì‚¬ìš©í•˜ë©´ ê¸°ë³¸ ì„¤ì •ë³´ë‹¤ ë” ë¹ ë¥´ê²Œ ì‘ë‹µí•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë‹¤ë§Œ Responses API ì˜ ì¼ë¶€ ë‚´ì¥ ë„êµ¬(ì˜ˆ: íŒŒì¼ ê²€ìƒ‰ê³¼ ì´ë¯¸ì§€ ìƒì„±)ëŠ” `"minimal"` reasoning effort ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì´ Agents SDK ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ `"low"`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### ë¹„ GPT-5 ëª¨ë¸

ì‚¬ìš©ì ì§€ì • `model_settings` ì—†ì´ GPT-5 ê°€ ì•„ë‹Œ ëª¨ë¸ ì´ë¦„ì„ ì „ë‹¬í•˜ë©´, SDK ëŠ” ëª¨ë“  ëª¨ë¸ê³¼ í˜¸í™˜ë˜ëŠ” ì¼ë°˜ì ì¸ `ModelSettings` ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤.

## ë¹„ OpenAI ëª¨ë¸

[LiteLLM í†µí•©](./litellm.md)ì„ í†µí•´ ëŒ€ë¶€ë¶„ì˜ ë¹„ OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¨¼ì €, litellm ì˜ì¡´ì„± ê·¸ë£¹ì„ ì„¤ì¹˜í•˜ì„¸ìš”:

```bash
pip install "openai-agents[litellm]"
```

ê·¸ëŸ° ë‹¤ìŒ `litellm/` ì ‘ë‘ì‚¬ë¥¼ ì‚¬ìš©í•´ [ì§€ì›ë˜ëŠ” ëª¨ë¸](https://docs.litellm.ai/docs/providers)ì„ ì‚¬ìš©í•˜ì„¸ìš”:

```python
claude_agent = Agent(model="litellm/anthropic/claude-3-5-sonnet-20240620", ...)
gemini_agent = Agent(model="litellm/gemini/gemini-2.5-flash-preview-04-17", ...)
```

### ë¹„ OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë‹¤ë¥¸ ë°©ë²•

ë‹¤ë¥¸ LLM ê³µê¸‰ìë¥¼ ë‹¤ìŒì˜ 3ê°€ì§€ ë°©ë²•ìœ¼ë¡œ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì½”ë“œ examples ì€ [ì—¬ê¸°](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)ì— ìˆìŒ):

1. [`set_default_openai_client`][agents.set_default_openai_client]ëŠ” ì „ì—­ì ìœ¼ë¡œ `AsyncOpenAI` ì¸ìŠ¤í„´ìŠ¤ë¥¼ LLM í´ë¼ì´ì–¸íŠ¸ë¡œ ì‚¬ìš©í•˜ê³ ì í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤. ì´ëŠ” LLM ê³µê¸‰ìê°€ OpenAI í˜¸í™˜ API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì œê³µí•˜ê³ , `base_url`ê³¼ `api_key`ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆëŠ” ê²½ìš°ì— í•´ë‹¹í•©ë‹ˆë‹¤. êµ¬ì„± ê°€ëŠ¥í•œ code examples ëŠ” [examples/model_providers/custom_example_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
2. [`ModelProvider`][agents.models.interface.ModelProvider]ëŠ” `Runner.run` ìˆ˜ì¤€ì—ì„œ ì ìš©ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ â€œì´ ì‹¤í–‰ì˜ ëª¨ë“  ì—ì´ì „íŠ¸ì— ëŒ€í•´ ì»¤ìŠ¤í…€ ëª¨ë¸ ê³µê¸‰ìë¥¼ ì‚¬ìš©â€í•˜ë„ë¡ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬ì„± ê°€ëŠ¥í•œ code examples ëŠ” [examples/model_providers/custom_example_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
3. [`Agent.model`][agents.agent.Agent.model]ì„ ì‚¬ìš©í•˜ë©´ íŠ¹ì • Agent ì¸ìŠ¤í„´ìŠ¤ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì„œë¡œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ê³µê¸‰ìë¥¼ í˜¼í•©í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬ì„± ê°€ëŠ¥í•œ code examples ëŠ” [examples/model_providers/custom_example_agent.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”. ëŒ€ë¶€ë¶„ì˜ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì‰½ê²Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ [LiteLLM í†µí•©](./litellm.md)ì„ í†µí•´ì„œì…ë‹ˆë‹¤.

`platform.openai.com`ì˜ API í‚¤ê°€ ì—†ëŠ” ê²½ìš°, `set_tracing_disabled()`ë¥¼ í†µí•´ íŠ¸ë ˆì´ì‹±ì„ ë¹„í™œì„±í™”í•˜ê±°ë‚˜, [ë‹¤ë¥¸ íŠ¸ë ˆì´ì‹± í”„ë¡œì„¸ì„œ](../tracing.md)ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

!!! note

    ì´ ì˜ˆì‹œë“¤ì—ì„œëŠ” ëŒ€ë¶€ë¶„ì˜ LLM ê³µê¸‰ìê°€ ì•„ì§ Responses API ë¥¼ ì§€ì›í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— Chat Completions API/ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‚¬ìš© ì¤‘ì¸ LLM ê³µê¸‰ìê°€ Responses API ë¥¼ ì§€ì›í•œë‹¤ë©´ Responses ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

## ëª¨ë¸ í˜¼í•© ì‚¬ìš©

ë‹¨ì¼ ì›Œí¬í”Œë¡œ ë‚´ì—ì„œ ì—ì´ì „íŠ¸ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  ì‹¶ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë¶„ë¥˜(íŠ¸ë¦¬ì•„ì§€)ì—ëŠ” ë” ì‘ê³  ë¹ ë¥¸ ëª¨ë¸ì„, ë³µì¡í•œ ì‘ì—…ì—ëŠ” ë” í¬ê³  ê°•ë ¥í•œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [`Agent`][agents.Agent]ë¥¼ êµ¬ì„±í•  ë•Œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ íŠ¹ì • ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. ëª¨ë¸ ì´ë¦„ì„ ì „ë‹¬
2. ì„ì˜ì˜ ëª¨ë¸ ì´ë¦„ + í•´ë‹¹ ì´ë¦„ì„ Model ì¸ìŠ¤í„´ìŠ¤ë¡œ ë§¤í•‘í•  ìˆ˜ ìˆëŠ” [`ModelProvider`][agents.models.interface.ModelProvider] ì „ë‹¬
3. [`Model`][agents.models.interface.Model] êµ¬í˜„ì²´ë¥¼ ì§ì ‘ ì œê³µ

!!!note

    SDK ëŠ” [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel]ê³¼ [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] ë‘ ê°€ì§€ ëª¨ë¸ í˜•íƒœë¥¼ ëª¨ë‘ ì§€ì›í•˜ì§€ë§Œ, ê° ì›Œí¬í”Œë¡œì—ì„œëŠ” ë‹¨ì¼ ëª¨ë¸ í˜•íƒœë¥¼ ì‚¬ìš©í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë‘ ëª¨ë¸ í˜•íƒœê°€ ì§€ì›í•˜ëŠ” ê¸°ëŠ¥ê³¼ ë„êµ¬ ì§‘í•©ì´ ì„œë¡œ ë‹¤ë¥´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì›Œí¬í”Œë¡œì— ë‘ ëª¨ë¸ í˜•íƒœë¥¼ í˜¼í•©í•´ì•¼ í•œë‹¤ë©´, ì‚¬ìš©í•˜ëŠ” ëª¨ë“  ê¸°ëŠ¥ì´ ì–‘ìª½ì—ì„œ ëª¨ë‘ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

```python
from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
    model="gpt-5-mini", # (1)!
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model=OpenAIChatCompletionsModel( # (2)!
        model="gpt-5-nano",
        openai_client=AsyncOpenAI()
    ),
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    model="gpt-5",
)

async def main():
    result = await Runner.run(triage_agent, input="Hola, Â¿cÃ³mo estÃ¡s?")
    print(result.final_output)
```

1. OpenAI ëª¨ë¸ì˜ ì´ë¦„ì„ ì§ì ‘ ì„¤ì •í•©ë‹ˆë‹¤.
2. [`Model`][agents.models.interface.Model] êµ¬í˜„ì²´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì—ì´ì „íŠ¸ì— ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ì„ ë” ì„¸ë°€í•˜ê²Œ êµ¬ì„±í•˜ë ¤ë©´, `temperature` ê°™ì€ ì„ íƒì  ëª¨ë¸ êµ¬ì„± ë§¤ê°œë³€ìˆ˜ë¥¼ ì œê³µí•˜ëŠ” [`ModelSettings`][agents.models.interface.ModelSettings] ë¥¼ ì „ë‹¬í•˜ë©´ ë©ë‹ˆë‹¤.

```python
from agents import Agent, ModelSettings

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model="gpt-4.1",
    model_settings=ModelSettings(temperature=0.1),
)
```

ë˜í•œ OpenAI ì˜ Responses API ë¥¼ ì‚¬ìš©í•  ë•Œ [ë‹¤ë¥¸ ëª‡ ê°€ì§€ ì„ íƒì  ë§¤ê°œë³€ìˆ˜](https://platform.openai.com/docs/api-reference/responses/create)(ì˜ˆ: `user`, `service_tier` ë“±)ê°€ ìˆìŠµë‹ˆë‹¤. ìµœìƒìœ„ ìˆ˜ì¤€ì—ì„œ ì œê³µë˜ì§€ ì•ŠëŠ” ê²½ìš° `extra_args` ë¥¼ ì‚¬ìš©í•´ í•¨ê»˜ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from agents import Agent, ModelSettings

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model="gpt-4.1",
    model_settings=ModelSettings(
        temperature=0.1,
        extra_args={"service_tier": "flex", "user": "user_12345"},
    ),
)
```

## ë‹¤ë¥¸ LLM ê³µê¸‰ì ì‚¬ìš© ì‹œ ì¼ë°˜ì ì¸ ë¬¸ì œ

### Tracing í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜ 401

íŠ¸ë ˆì´ì‹± ê´€ë ¨ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ê²½ìš°, ì´ëŠ” íŠ¸ë ˆì´ìŠ¤ê°€ OpenAI ì„œë²„ë¡œ ì—…ë¡œë“œë˜ëŠ”ë° OpenAI API í‚¤ê°€ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. í•´ê²° ë°©ë²•ì€ ë‹¤ìŒ ì„¸ ê°€ì§€ì…ë‹ˆë‹¤:

1. íŠ¸ë ˆì´ì‹± ì™„ì „ ë¹„í™œì„±í™”: [`set_tracing_disabled(True)`][agents.set_tracing_disabled]
2. íŠ¸ë ˆì´ì‹±ìš© OpenAI í‚¤ ì„¤ì •: [`set_tracing_export_api_key(...)`][agents.set_tracing_export_api_key]. ì´ API í‚¤ëŠ” íŠ¸ë ˆì´ìŠ¤ ì—…ë¡œë“œì—ë§Œ ì‚¬ìš©ë˜ë©°, [platform.openai.com](https://platform.openai.com/)ì—ì„œ ë°œê¸‰ëœ í‚¤ì—¬ì•¼ í•©ë‹ˆë‹¤.
3. OpenAI ê°€ ì•„ë‹Œ íŠ¸ë ˆì´ìŠ¤ í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©. [íŠ¸ë ˆì´ì‹± ë¬¸ì„œ](../tracing.md#custom-tracing-processors)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

### Responses API ì§€ì›

SDK ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Responses API ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ ë‹¤ë¥¸ LLM ê³µê¸‰ìëŠ” ì•„ì§ ì´ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ 404 ë“±ì˜ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•´ê²° ë°©ë²•ì€ ë‹¤ìŒ ë‘ ê°€ì§€ì…ë‹ˆë‹¤:

1. [`set_default_openai_api("chat_completions")`][agents.set_default_openai_api]ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. ì´ëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ `OPENAI_API_KEY`ì™€ `OPENAI_BASE_URL`ì„ ì„¤ì •í•˜ëŠ” ê²½ìš°ì— ë™ì‘í•©ë‹ˆë‹¤.
2. [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel]ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì½”ë“œ examples ì€ [ì—¬ê¸°](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)ì— ìˆìŠµë‹ˆë‹¤.

### Structured outputs ì§€ì›

ì¼ë¶€ ëª¨ë¸ ê³µê¸‰ìëŠ” [structured outputs](https://platform.openai.com/docs/guides/structured-outputs)ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ë‹¤ìŒê³¼ ìœ ì‚¬í•œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```

BadRequestError: Error code: 400 - {'error': {'message': "'response_format.type' : value is not one of the allowed values ['text','json_object']", 'type': 'invalid_request_error'}}

```

ì´ëŠ” ì¼ë¶€ ëª¨ë¸ ê³µê¸‰ìì˜ í•œê³„ë¡œ, JSON ì¶œë ¥ì„ ì§€ì›í•˜ì§€ë§Œ ì¶œë ¥ì— ì‚¬ìš©í•  `json_schema` ë¥¼ ì§€ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ì— ëŒ€í•œ í•´ê²°ì±…ì„ ë§ˆë ¨ ì¤‘ì´ì§€ë§Œ, JSON schema ì¶œë ¥ì„ ì§€ì›í•˜ëŠ” ê³µê¸‰ìë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì˜ëª»ëœ JSON ë•Œë¬¸ì— ì•±ì´ ìì£¼ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ê³µê¸‰ì ê°„ ëª¨ë¸ í˜¼í•©

ëª¨ë¸ ê³µê¸‰ì ê°„ ê¸°ëŠ¥ ì°¨ì´ë¥¼ ìœ ì˜í•˜ì§€ ì•Šìœ¼ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ OpenAI ëŠ” structured outputs, ë©€í‹°ëª¨ë‹¬ ì…ë ¥, í˜¸ìŠ¤í‹°ë“œ íŒŒì¼ ê²€ìƒ‰ê³¼ ì›¹ ê²€ìƒ‰ì„ ì§€ì›í•˜ì§€ë§Œ, ë§ì€ ë‹¤ë¥¸ ê³µê¸‰ìëŠ” ì´ëŸ¬í•œ ê¸°ëŠ¥ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ìŒ ì œí•œ ì‚¬í•­ì„ ìœ ì˜í•˜ì„¸ìš”:

- ì§€ì›í•˜ì§€ ì•ŠëŠ” ê³µê¸‰ìì—ê²Œ ì´í•´ë˜ì§€ ì•ŠëŠ” `tools` ë¥¼ ë³´ë‚´ì§€ ì•Šê¸°
- í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë¸ì„ í˜¸ì¶œí•˜ê¸° ì „ì— ë©€í‹°ëª¨ë‹¬ ì…ë ¥ì„ í•„í„°ë§í•˜ê¸°
- structured JSON ì¶œë ¥ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê³µê¸‰ìëŠ” ë•Œë•Œë¡œ ì˜ëª»ëœ JSON ì„ ìƒì„±í•  ìˆ˜ ìˆìŒì„ ìœ ì˜í•˜ê¸°

================
File: docs/ko/models/litellm.md
================
---
search:
  exclude: true
---
# LiteLLMë¥¼ í†µí•œ ì„ì˜ ëª¨ë¸ ì‚¬ìš©

!!! note

    LiteLLM í†µí•©ì€ ë² íƒ€ ë²„ì „ì…ë‹ˆë‹¤. íŠ¹íˆ ì†Œê·œëª¨ ëª¨ë¸ ì œê³µìì™€ í•¨ê»˜ ì‚¬ìš©í•  ë•Œ ë¬¸ì œë¥¼ ê²ªì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. [GitHub ì´ìŠˆ](https://github.com/openai/openai-agents-python/issues)ë¥¼ í†µí•´ ë¬¸ì œë¥¼ ì‹ ê³ í•´ ì£¼ì„¸ìš”. ì‹ ì†íˆ ìˆ˜ì •í•˜ê² ìŠµë‹ˆë‹¤.

[LiteLLM](https://docs.litellm.ai/docs/)ì€ ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤ë¡œ 100ê°œ ì´ìƒì˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. Agents SDKì— LiteLLM í†µí•©ì„ ì¶”ê°€í•˜ì—¬ ì–´ë–¤ AI ëª¨ë¸ì´ë“  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì„¤ì •

`litellm`ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ì„ íƒì  `litellm` ì˜ì¡´ì„± ê·¸ë£¹ì„ ì„¤ì¹˜í•˜ë©´ ë©ë‹ˆë‹¤:

```bash
pip install "openai-agents[litellm]"
```

ì„¤ì¹˜ê°€ ì™„ë£Œë˜ë©´, ì–´ë–¤ ì—ì´ì „íŠ¸ì—ì„œë“  [`LitellmModel`][agents.extensions.models.litellm_model.LitellmModel]ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì˜ˆì œ

ì™„ì „íˆ ë™ì‘í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤. ì‹¤í–‰í•˜ë©´ ëª¨ë¸ ì´ë¦„ê³¼ API í‚¤ë¥¼ ì…ë ¥í•˜ë¼ëŠ” í”„ë¡¬í”„íŠ¸ê°€ í‘œì‹œë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì´ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- `openai/gpt-4.1` ë¥¼ ëª¨ë¸ë¡œ, ê·¸ë¦¬ê³  OpenAI API í‚¤
- `anthropic/claude-3-5-sonnet-20240620` ë¥¼ ëª¨ë¸ë¡œ, ê·¸ë¦¬ê³  Anthropic API í‚¤
- ë“±

LiteLLMì—ì„œ ì§€ì›í•˜ëŠ” ëª¨ë¸ì˜ ì „ì²´ ëª©ë¡ì€ [litellm providers ë¬¸ì„œ](https://docs.litellm.ai/docs/providers)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

```python
from __future__ import annotations

import asyncio

from agents import Agent, Runner, function_tool, set_tracing_disabled
from agents.extensions.models.litellm_model import LitellmModel

@function_tool
def get_weather(city: str):
    print(f"[debug] getting weather for {city}")
    return f"The weather in {city} is sunny."


async def main(model: str, api_key: str):
    agent = Agent(
        name="Assistant",
        instructions="You only respond in haikus.",
        model=LitellmModel(model=model, api_key=api_key),
        tools=[get_weather],
    )

    result = await Runner.run(agent, "What's the weather in Tokyo?")
    print(result.final_output)


if __name__ == "__main__":
    # First try to get model/api key from args
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=False)
    parser.add_argument("--api-key", type=str, required=False)
    args = parser.parse_args()

    model = args.model
    if not model:
        model = input("Enter a model name for Litellm: ")

    api_key = args.api_key
    if not api_key:
        api_key = input("Enter an API key for Litellm: ")

    asyncio.run(main(model, api_key))
```

## ì‚¬ìš©ëŸ‰ ë°ì´í„° ì¶”ì 

LiteLLM ì‘ë‹µì´ Agents SDK ì‚¬ìš©ëŸ‰ ë©”íŠ¸ë¦­ì— ë°˜ì˜ë˜ë„ë¡ í•˜ë ¤ë©´, ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•  ë•Œ `ModelSettings(include_usage=True)` ë¥¼ ì „ë‹¬í•˜ì„¸ìš”.

```python
from agents import Agent, ModelSettings
from agents.extensions.models.litellm_model import LitellmModel

agent = Agent(
    name="Assistant",
    model=LitellmModel(model="your/model", api_key="..."),
    model_settings=ModelSettings(include_usage=True),
)
```

`include_usage=True` ì¸ ê²½ìš°, LiteLLM ìš”ì²­ì€ ê¸°ë³¸ OpenAI ëª¨ë¸ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ `result.context_wrapper.usage` ë¥¼ í†µí•´ í† í° ë° ìš”ì²­ ìˆ˜ë¥¼ ë³´ê³ í•©ë‹ˆë‹¤.

================
File: docs/ko/realtime/guide.md
================
---
search:
  exclude: true
---
# ê°€ì´ë“œ

ì´ ê°€ì´ë“œëŠ” OpenAI Agents SDKì˜ ì‹¤ì‹œê°„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ ìŒì„± ê¸°ë°˜ì˜ AI ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì‹¬ì¸µì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

!!! warning "Beta feature"
ì‹¤ì‹œê°„ ì—ì´ì „íŠ¸ëŠ” ë² íƒ€ì…ë‹ˆë‹¤. êµ¬í˜„ì„ ê°œì„ í•˜ëŠ” ê³¼ì •ì—ì„œ íŒŒê´´ì  ë³€ê²½ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ê°œìš”

ì‹¤ì‹œê°„ ì—ì´ì „íŠ¸ëŠ” ëŒ€í™”í˜• íë¦„ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ì˜¤ë””ì˜¤ì™€ í…ìŠ¤íŠ¸ ì…ë ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤. OpenAIì˜ Realtime APIì™€ ì§€ì†ì  ì—°ê²°ì„ ìœ ì§€í•˜ì—¬ ë‚®ì€ ì§€ì—°ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ëŒ€í™”ë¥¼ ì œê³µí•˜ê³ , ì‚¬ìš©ì ì¸í„°ëŸ½ì…˜(ì¤‘ë‹¨ ì²˜ë¦¬)ì„ ìš°ì•„í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

## ì•„í‚¤í…ì²˜

### í•µì‹¬ êµ¬ì„± ìš”ì†Œ

ì‹¤ì‹œê°„ ì‹œìŠ¤í…œì€ ë‹¤ìŒì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤:

-   **RealtimeAgent**: instructions, tools, í•¸ë“œì˜¤í”„ë¡œ êµ¬ì„±ëœ ì—ì´ì „íŠ¸
-   **RealtimeRunner**: ì„¤ì •ì„ ê´€ë¦¬í•©ë‹ˆë‹¤. `runner.run()`ì„ í˜¸ì¶œí•´ ì„¸ì…˜ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
-   **RealtimeSession**: ë‹¨ì¼ ìƒí˜¸ì‘ìš© ì„¸ì…˜ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ì‹œì‘í•  ë•Œë§ˆë‹¤ í•˜ë‚˜ë¥¼ ìƒì„±í•˜ê³ , ëŒ€í™”ê°€ ëë‚  ë•Œê¹Œì§€ ìœ ì§€í•©ë‹ˆë‹¤.
-   **RealtimeModel**: ê¸°ë³¸ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤(ì¼ë°˜ì ìœ¼ë¡œ OpenAIì˜ WebSocket êµ¬í˜„)

### ì„¸ì…˜ íë¦„

ì¼ë°˜ì ì¸ ì‹¤ì‹œê°„ ì„¸ì…˜ì€ ë‹¤ìŒ íë¦„ì„ ë”°ë¦…ë‹ˆë‹¤:

1. instructions, tools, í•¸ë“œì˜¤í”„ë¡œ **RealtimeAgent**ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
2. ì—ì´ì „íŠ¸ì™€ êµ¬ì„± ì˜µì…˜ìœ¼ë¡œ **RealtimeRunner**ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤
3. `await runner.run()`ì„ ì‚¬ìš©í•´ **ì„¸ì…˜ì„ ì‹œì‘**í•˜ê³  RealtimeSessionì„ ë°˜í™˜ë°›ìŠµë‹ˆë‹¤
4. `send_audio()` ë˜ëŠ” `send_message()`ë¡œ **ì˜¤ë””ì˜¤ ë˜ëŠ” í…ìŠ¤íŠ¸ ë©”ì‹œì§€**ë¥¼ ì„¸ì…˜ì— ë³´ëƒ…ë‹ˆë‹¤
5. ì„¸ì…˜ì„ ìˆœíšŒ(iterate)í•˜ì—¬ **ì´ë²¤íŠ¸ë¥¼ ìˆ˜ì‹ **í•©ë‹ˆë‹¤ - ì´ë²¤íŠ¸ì—ëŠ” ì˜¤ë””ì˜¤ ì¶œë ¥, ì „ì‚¬ ê²°ê³¼, ë„êµ¬ í˜¸ì¶œ, í•¸ë“œì˜¤í”„, ì˜¤ë¥˜ê°€ í¬í•¨ë©ë‹ˆë‹¤
6. ì‚¬ìš©ìê°€ ì—ì´ì „íŠ¸ ë§ì— ê²¹ì³ ë§í•  ë•Œ **ì¸í„°ëŸ½ì…˜(ì¤‘ë‹¨ ì²˜ë¦¬)** ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. í˜„ì¬ ì˜¤ë””ì˜¤ ìƒì„±ì´ ìë™ìœ¼ë¡œ ì¤‘ì§€ë©ë‹ˆë‹¤

ì„¸ì…˜ì€ ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•˜ë©° ì‹¤ì‹œê°„ ëª¨ë¸ê³¼ì˜ ì§€ì†ì  ì—°ê²°ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

## ì—ì´ì „íŠ¸ êµ¬ì„±

RealtimeAgentëŠ” ì¼ë°˜ Agent í´ë˜ìŠ¤ì™€ ìœ ì‚¬í•˜ì§€ë§Œ ëª‡ ê°€ì§€ ì¤‘ìš”í•œ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. ì „ì²´ API ì„¸ë¶€ ì •ë³´ëŠ” [`RealtimeAgent`][agents.realtime.agent.RealtimeAgent] API ë ˆí¼ëŸ°ìŠ¤ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

ì¼ë°˜ ì—ì´ì „íŠ¸ì™€ì˜ ì£¼ìš” ì°¨ì´ì :

-   ëª¨ë¸ ì„ íƒì€ ì—ì´ì „íŠ¸ ìˆ˜ì¤€ì´ ì•„ë‹Œ ì„¸ì…˜ ìˆ˜ì¤€ì—ì„œ êµ¬ì„±ë©ë‹ˆë‹¤
-   structured output ì§€ì›ì´ ì—†ìŠµë‹ˆë‹¤ (`outputType` ë¯¸ì§€ì›)
-   ìŒì„±ì€ ì—ì´ì „íŠ¸ë³„ë¡œ êµ¬ì„±í•  ìˆ˜ ìˆì§€ë§Œ ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ê°€ ë§í•œ í›„ì—ëŠ” ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤
-   tools, í•¸ë“œì˜¤í”„, instructions ë“±ì˜ ë‹¤ë¥¸ ê¸°ëŠ¥ì€ ë™ì¼í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤

## ì„¸ì…˜ êµ¬ì„±

### ëª¨ë¸ ì„¤ì •

ì„¸ì…˜ êµ¬ì„±ìœ¼ë¡œ ê¸°ë³¸ ì‹¤ì‹œê°„ ëª¨ë¸ ë™ì‘ì„ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì´ë¦„(ì˜ˆ: `gpt-realtime`), ìŒì„± ì„ íƒ(alloy, echo, fable, onyx, nova, shimmer), ì§€ì› ëª¨ë‹¬ë¦¬í‹°(í…ìŠ¤íŠ¸ ë°/ë˜ëŠ” ì˜¤ë””ì˜¤)ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜¤ë””ì˜¤ í˜•ì‹ì€ ì…ë ¥ê³¼ ì¶œë ¥ ëª¨ë‘ì— ëŒ€í•´ ì„¤ì • ê°€ëŠ¥í•˜ë©°, ê¸°ë³¸ê°’ì€ PCM16ì…ë‹ˆë‹¤.

### ì˜¤ë””ì˜¤ êµ¬ì„±

ì˜¤ë””ì˜¤ ì„¤ì •ì€ ì„¸ì…˜ì´ ìŒì„± ì…ë ¥ê³¼ ì¶œë ¥ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì„ ì œì–´í•©ë‹ˆë‹¤. Whisperì™€ ê°™ì€ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì…ë ¥ ì˜¤ë””ì˜¤ ì „ì‚¬ë¥¼ êµ¬ì„±í•˜ê³ , ì–¸ì–´ ê¸°ë³¸ê°’ì„ ì„¤ì •í•˜ë©°, ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ì˜ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ì „ì‚¬ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í„´ ê°ì§€ ì„¤ì •ì€ ì—ì´ì „íŠ¸ê°€ ì‘ë‹µì„ ì‹œì‘í•˜ê³  ì¢…ë£Œí•´ì•¼ í•˜ëŠ” ì‹œì ì„ ì œì–´í•˜ë©°, ìŒì„± í™œë™ ê°ì§€ ì„ê³„ê°’, ì¹¨ë¬µ ì§€ì† ì‹œê°„, ê°ì§€ëœ ìŒì„± ì£¼ë³€ íŒ¨ë”© ì˜µì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ë„êµ¬ì™€ í•¨ìˆ˜

### ë„êµ¬ ì¶”ê°€

ì¼ë°˜ ì—ì´ì „íŠ¸ì™€ ë§ˆì°¬ê°€ì§€ë¡œ, ì‹¤ì‹œê°„ ì—ì´ì „íŠ¸ëŠ” ëŒ€í™” ì¤‘ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜ ë„êµ¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤:

```python
from agents import function_tool

@function_tool
def get_weather(city: str) -> str:
    """Get current weather for a city."""
    # Your weather API logic here
    return f"The weather in {city} is sunny, 72Â°F"

@function_tool
def book_appointment(date: str, time: str, service: str) -> str:
    """Book an appointment."""
    # Your booking logic here
    return f"Appointment booked for {service} on {date} at {time}"

agent = RealtimeAgent(
    name="Assistant",
    instructions="You can help with weather and appointments.",
    tools=[get_weather, book_appointment],
)
```

## í•¸ë“œì˜¤í”„

### í•¸ë“œì˜¤í”„ ìƒì„±

í•¸ë“œì˜¤í”„ë¥¼ ì‚¬ìš©í•˜ë©´ ì „ë¬¸í™”ëœ ì—ì´ì „íŠ¸ ê°„ì— ëŒ€í™”ë¥¼ ì „í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from agents.realtime import realtime_handoff

# Specialized agents
billing_agent = RealtimeAgent(
    name="Billing Support",
    instructions="You specialize in billing and payment issues.",
)

technical_agent = RealtimeAgent(
    name="Technical Support",
    instructions="You handle technical troubleshooting.",
)

# Main agent with handoffs
main_agent = RealtimeAgent(
    name="Customer Service",
    instructions="You are the main customer service agent. Hand off to specialists when needed.",
    handoffs=[
        realtime_handoff(billing_agent, tool_description="Transfer to billing support"),
        realtime_handoff(technical_agent, tool_description="Transfer to technical support"),
    ]
)
```

## ì´ë²¤íŠ¸ ì²˜ë¦¬

ì„¸ì…˜ì€ ì„¸ì…˜ ê°ì²´ë¥¼ ìˆœíšŒí•˜ì—¬ ìˆ˜ì‹ í•  ìˆ˜ ìˆëŠ” ì´ë²¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤. ì´ë²¤íŠ¸ì—ëŠ” ì˜¤ë””ì˜¤ ì¶œë ¥ ì²­í¬, ì „ì‚¬ ê²°ê³¼, ë„êµ¬ ì‹¤í–‰ ì‹œì‘/ì¢…ë£Œ, ì—ì´ì „íŠ¸ í•¸ë“œì˜¤í”„, ì˜¤ë¥˜ê°€ í¬í•¨ë©ë‹ˆë‹¤. íŠ¹íˆ ë‹¤ìŒ ì´ë²¤íŠ¸ ì²˜ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤:

-   **audio**: ì—ì´ì „íŠ¸ ì‘ë‹µì˜ ì›ì‹œ ì˜¤ë””ì˜¤ ë°ì´í„°
-   **audio_end**: ì—ì´ì „íŠ¸ ë°œí™” ì¢…ë£Œ
-   **audio_interrupted**: ì‚¬ìš©ìê°€ ì—ì´ì „íŠ¸ë¥¼ ì¤‘ë‹¨í•¨
-   **tool_start/tool_end**: ë„êµ¬ ì‹¤í–‰ ë¼ì´í”„ì‚¬ì´í´
-   **handoff**: ì—ì´ì „íŠ¸ í•¸ë“œì˜¤í”„ ë°œìƒ
-   **error**: ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ

ì „ì²´ ì´ë²¤íŠ¸ ì„¸ë¶€ ì •ë³´ëŠ” [`RealtimeSessionEvent`][agents.realtime.events.RealtimeSessionEvent]ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

## ê°€ë“œë ˆì¼

ì‹¤ì‹œê°„ ì—ì´ì „íŠ¸ëŠ” ì¶œë ¥ ê°€ë“œë ˆì¼ë§Œ ì§€ì›í•©ë‹ˆë‹¤. ì„±ëŠ¥ ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ ë§¤ ë‹¨ì–´ë§ˆë‹¤ê°€ ì•„ë‹ˆë¼ ì£¼ê¸°ì ìœ¼ë¡œ ë””ë°”ìš´ìŠ¤ë˜ì–´ ì‹¤í–‰ë©ë‹ˆë‹¤. ê¸°ë³¸ ë””ë°”ìš´ìŠ¤ ê¸¸ì´ëŠ” 100ìì´ë©°, ì„¤ì • ê°€ëŠ¥í•©ë‹ˆë‹¤.

ê°€ë“œë ˆì¼ì€ `RealtimeAgent`ì— ì§ì ‘ ì—°ê²°í•˜ê±°ë‚˜ ì„¸ì…˜ì˜ `run_config`ë¥¼ í†µí•´ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘ ì†ŒìŠ¤ì˜ ê°€ë“œë ˆì¼ì€ í•¨ê»˜ ì‹¤í–‰ë©ë‹ˆë‹¤.

```python
from agents.guardrail import GuardrailFunctionOutput, OutputGuardrail

def sensitive_data_check(context, agent, output):
    return GuardrailFunctionOutput(
        tripwire_triggered="password" in output,
        output_info=None,
    )

agent = RealtimeAgent(
    name="Assistant",
    instructions="...",
    output_guardrails=[OutputGuardrail(guardrail_function=sensitive_data_check)],
)
```

ê°€ë“œë ˆì¼ì´ íŠ¸ë¦¬ê±°ë˜ë©´ `guardrail_tripped` ì´ë²¤íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì—ì´ì „íŠ¸ì˜ í˜„ì¬ ì‘ë‹µì„ ì¤‘ë‹¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë””ë°”ìš´ìŠ¤ ë™ì‘ì€ ì•ˆì „ì„±ê³¼ ì‹¤ì‹œê°„ ì„±ëŠ¥ ìš”êµ¬ ì‚¬ì´ì˜ ê· í˜•ì„ ë•ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì—ì´ì „íŠ¸ì™€ ë‹¬ë¦¬, ì‹¤ì‹œê°„ ì—ì´ì „íŠ¸ëŠ” ê°€ë“œë ˆì¼ì´ ì‘ë™í•´ë„ Exceptionì„ ë°œìƒì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤.

## ì˜¤ë””ì˜¤ ì²˜ë¦¬

[`session.send_audio(audio_bytes)`][agents.realtime.session.RealtimeSession.send_audio]ë¡œ ì˜¤ë””ì˜¤ë¥¼ ì„¸ì…˜ì— ì „ì†¡í•˜ê±°ë‚˜, [`session.send_message()`][agents.realtime.session.RealtimeSession.send_message]ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì „ì†¡í•˜ì„¸ìš”.

ì˜¤ë””ì˜¤ ì¶œë ¥ì„ ìœ„í•´ì„œëŠ” `audio` ì´ë²¤íŠ¸ë¥¼ ìˆ˜ì‹ í•˜ê³  ì„ í˜¸í•˜ëŠ” ì˜¤ë””ì˜¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ì¬ìƒí•˜ì„¸ìš”. ì‚¬ìš©ìê°€ ì—ì´ì „íŠ¸ë¥¼ ì¤‘ë‹¨í•  ë•Œ ì¦‰ì‹œ ì¬ìƒì„ ì¤‘ì§€í•˜ê³  ëŒ€ê¸° ì¤‘ì¸ ì˜¤ë””ì˜¤ë¥¼ ëª¨ë‘ ë¹„ìš°ê¸° ìœ„í•´ `audio_interrupted` ì´ë²¤íŠ¸ë¥¼ ë°˜ë“œì‹œ ìˆ˜ì‹ í•˜ì„¸ìš”.

## ëª¨ë¸ ì§ì ‘ ì•¡ì„¸ìŠ¤

ê¸°ë³¸ ëª¨ë¸ì— ì ‘ê·¼í•˜ì—¬ ì»¤ìŠ¤í…€ ë¦¬ìŠ¤ë„ˆë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ê³ ê¸‰ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# Add a custom listener to the model
session.model.add_listener(my_custom_listener)
```

ì´ë ‡ê²Œ í•˜ë©´ ì—°ê²°ì— ëŒ€í•œ ë” ë‚®ì€ ìˆ˜ì¤€ì˜ ì œì–´ê°€ í•„ìš”í•œ ê³ ê¸‰ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ìœ„í•´ [`RealtimeModel`][agents.realtime.model.RealtimeModel] ì¸í„°í˜ì´ìŠ¤ì— ì§ì ‘ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì˜ˆì‹œ

ì™„ì „í•œ ë™ì‘ code examplesëŠ” [examples/realtime ë””ë ‰í„°ë¦¬](https://github.com/openai/openai-agents-python/tree/main/examples/realtime)ë¥¼ í™•ì¸í•˜ì„¸ìš”. UI êµ¬ì„± ìš”ì†Œê°€ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš°ì˜ ë°ëª¨ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

================
File: docs/ko/realtime/quickstart.md
================
---
search:
  exclude: true
---
# ë¹ ë¥¸ ì‹œì‘

ì‹¤ì‹œê°„ ì—ì´ì „íŠ¸ëŠ” OpenAIì˜ Realtime APIë¥¼ ì‚¬ìš©í•´ AI ì—ì´ì „íŠ¸ì™€ì˜ ìŒì„± ëŒ€í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì´ ê°€ì´ë“œëŠ” ì²« ì‹¤ì‹œê°„ ìŒì„± ì—ì´ì „íŠ¸ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

!!! warning "Beta ê¸°ëŠ¥"
ì‹¤ì‹œê°„ ì—ì´ì „íŠ¸ëŠ” ë² íƒ€ ìƒíƒœì…ë‹ˆë‹¤. êµ¬í˜„ì„ ê°œì„ í•˜ëŠ” ë™ì•ˆ í˜¸í™˜ì„±ì— ì˜í–¥ì„ ì£¼ëŠ” ë³€ê²½ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì¤€ë¹„ ì‚¬í•­

- Python 3.9 ì´ìƒ
- OpenAI API í‚¤
- OpenAI Agents SDKì— ëŒ€í•œ ê¸°ë³¸ ì´í•´

## ì„¤ì¹˜

ì•„ì§ ì„¤ì¹˜í•˜ì§€ ì•Šì•˜ë‹¤ë©´ OpenAI Agents SDKë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:

```bash
pip install openai-agents
```

## ì²« ì‹¤ì‹œê°„ ì—ì´ì „íŠ¸ ë§Œë“¤ê¸°

### 1. í•„ìš”í•œ êµ¬ì„±ìš”ì†Œ ê°€ì ¸ì˜¤ê¸°

```python
import asyncio
from agents.realtime import RealtimeAgent, RealtimeRunner
```

### 2. ì‹¤ì‹œê°„ ì—ì´ì „íŠ¸ ìƒì„±

```python
agent = RealtimeAgent(
    name="Assistant",
    instructions="You are a helpful voice assistant. Keep your responses conversational and friendly.",
)
```

### 3. ëŸ¬ë„ˆ ì„¤ì •

```python
runner = RealtimeRunner(
    starting_agent=agent,
    config={
        "model_settings": {
            "model_name": "gpt-realtime",
            "voice": "ash",
            "modalities": ["audio"],
            "input_audio_format": "pcm16",
            "output_audio_format": "pcm16",
            "input_audio_transcription": {"model": "gpt-4o-mini-transcribe"},
            "turn_detection": {"type": "semantic_vad", "interrupt_response": True},
        }
    }
)
```

### 4. ì„¸ì…˜ ì‹œì‘

```python
# Start the session
session = await runner.run()

async with session:
    print("Session started! The agent will stream audio responses in real-time.")
    # Process events
    async for event in session:
        try:
            if event.type == "agent_start":
                print(f"Agent started: {event.agent.name}")
            elif event.type == "agent_end":
                print(f"Agent ended: {event.agent.name}")
            elif event.type == "handoff":
                print(f"Handoff from {event.from_agent.name} to {event.to_agent.name}")
            elif event.type == "tool_start":
                print(f"Tool started: {event.tool.name}")
            elif event.type == "tool_end":
                print(f"Tool ended: {event.tool.name}; output: {event.output}")
            elif event.type == "audio_end":
                print("Audio ended")
            elif event.type == "audio":
                # Enqueue audio for callback-based playback with metadata
                # Non-blocking put; queue is unbounded, so drops wonâ€™t occur.
                pass
            elif event.type == "audio_interrupted":
                print("Audio interrupted")
                # Begin graceful fade + flush in the audio callback and rebuild jitter buffer.
            elif event.type == "error":
                print(f"Error: {event.error}")
            elif event.type == "history_updated":
                pass  # Skip these frequent events
            elif event.type == "history_added":
                pass  # Skip these frequent events
            elif event.type == "raw_model_event":
                print(f"Raw model event: {_truncate_str(str(event.data), 200)}")
            else:
                print(f"Unknown event type: {event.type}")
        except Exception as e:
            print(f"Error processing event: {_truncate_str(str(e), 200)}")

def _truncate_str(s: str, max_length: int) -> str:
    if len(s) > max_length:
        return s[:max_length] + "..."
    return s
```

## ì „ì²´ ì˜ˆì œ

ë‹¤ìŒì€ ì™„ì „í•œ ë™ì‘ ì˜ˆì œì…ë‹ˆë‹¤:

```python
import asyncio
from agents.realtime import RealtimeAgent, RealtimeRunner

async def main():
    # Create the agent
    agent = RealtimeAgent(
        name="Assistant",
        instructions="You are a helpful voice assistant. Keep responses brief and conversational.",
    )
    # Set up the runner with configuration
    runner = RealtimeRunner(
        starting_agent=agent,
        config={
            "model_settings": {
                "model_name": "gpt-realtime",
                "voice": "ash",
                "modalities": ["audio"],
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "input_audio_transcription": {"model": "gpt-4o-mini-transcribe"},
                "turn_detection": {"type": "semantic_vad", "interrupt_response": True},
            }
        },
    )
    # Start the session
    session = await runner.run()

    async with session:
        print("Session started! The agent will stream audio responses in real-time.")
        # Process events
        async for event in session:
            try:
                if event.type == "agent_start":
                    print(f"Agent started: {event.agent.name}")
                elif event.type == "agent_end":
                    print(f"Agent ended: {event.agent.name}")
                elif event.type == "handoff":
                    print(f"Handoff from {event.from_agent.name} to {event.to_agent.name}")
                elif event.type == "tool_start":
                    print(f"Tool started: {event.tool.name}")
                elif event.type == "tool_end":
                    print(f"Tool ended: {event.tool.name}; output: {event.output}")
                elif event.type == "audio_end":
                    print("Audio ended")
                elif event.type == "audio":
                    # Enqueue audio for callback-based playback with metadata
                    # Non-blocking put; queue is unbounded, so drops wonâ€™t occur.
                    pass
                elif event.type == "audio_interrupted":
                    print("Audio interrupted")
                    # Begin graceful fade + flush in the audio callback and rebuild jitter buffer.
                elif event.type == "error":
                    print(f"Error: {event.error}")
                elif event.type == "history_updated":
                    pass  # Skip these frequent events
                elif event.type == "history_added":
                    pass  # Skip these frequent events
                elif event.type == "raw_model_event":
                    print(f"Raw model event: {_truncate_str(str(event.data), 200)}")
                else:
                    print(f"Unknown event type: {event.type}")
            except Exception as e:
                print(f"Error processing event: {_truncate_str(str(e), 200)}")

def _truncate_str(s: str, max_length: int) -> str:
    if len(s) > max_length:
        return s[:max_length] + "..."
    return s

if __name__ == "__main__":
    # Run the session
    asyncio.run(main())
```

## êµ¬ì„± ì˜µì…˜

### ëª¨ë¸ ì„¤ì •

- `model_name`: ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤ì‹œê°„ ëª¨ë¸ ì¤‘ ì„ íƒ (ì˜ˆ: `gpt-realtime`)
- `voice`: ìŒì„± ì„ íƒ (`alloy`, `echo`, `fable`, `onyx`, `nova`, `shimmer`)
- `modalities`: í…ìŠ¤íŠ¸ ë˜ëŠ” ì˜¤ë””ì˜¤ í™œì„±í™” (`["text"]` ë˜ëŠ” `["audio"]`)

### ì˜¤ë””ì˜¤ ì„¤ì •

- `input_audio_format`: ì…ë ¥ ì˜¤ë””ì˜¤ í˜•ì‹ (`pcm16`, `g711_ulaw`, `g711_alaw`)
- `output_audio_format`: ì¶œë ¥ ì˜¤ë””ì˜¤ í˜•ì‹
- `input_audio_transcription`: ì „ì‚¬ êµ¬ì„±

### í„´ ê°ì§€

- `type`: ê°ì§€ ë°©ì‹ (`server_vad`, `semantic_vad`)
- `threshold`: ìŒì„± í™œë™ ì„ê³„ê°’ (0.0-1.0)
- `silence_duration_ms`: í„´ ì¢…ë£Œ ê°ì§€ë¥¼ ìœ„í•œ ë¬´ìŒ ì§€ì† ì‹œê°„
- `prefix_padding_ms`: ë°œí™” ì „ ì˜¤ë””ì˜¤ íŒ¨ë”©

## ë‹¤ìŒ ë‹¨ê³„

- [ì‹¤ì‹œê°„ ì—ì´ì „íŠ¸ì— ëŒ€í•´ ë” ì•Œì•„ë³´ê¸°](guide.md)
- [examples/realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime) í´ë”ì—ì„œ ì‘ë™í•˜ëŠ” code examples í™•ì¸
- ì—ì´ì „íŠ¸ì— tools ì¶”ê°€
- ì—ì´ì „íŠ¸ ê°„ í•¸ë“œì˜¤í”„ êµ¬í˜„
- ì•ˆì „ì„ ìœ„í•œ ê°€ë“œë ˆì¼ ì„¤ì •

## ì¸ì¦

í™˜ê²½ì— OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

```bash
export OPENAI_API_KEY="your-api-key-here"
```

ë˜ëŠ” ì„¸ì…˜ì„ ìƒì„±í•  ë•Œ ì§ì ‘ ì „ë‹¬í•˜ì„¸ìš”:

```python
session = await runner.run(model_config={"api_key": "your-api-key"})
```

================
File: docs/ko/voice/pipeline.md
================
---
search:
  exclude: true
---
# íŒŒì´í”„ë¼ì¸ê³¼ ì›Œí¬í”Œë¡œ

[`VoicePipeline`][agents.voice.pipeline.VoicePipeline]ì€ ì—ì´ì „íŠ¸ ê¸°ë°˜ ì›Œí¬í”Œë¡œë¥¼ ìŒì„± ì•±ìœ¼ë¡œ ì†ì‰½ê²Œ ì „í™˜í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ì‹¤í–‰í•  ì›Œí¬í”Œë¡œë¥¼ ì „ë‹¬í•˜ë©´, íŒŒì´í”„ë¼ì¸ì´ ì…ë ¥ ì˜¤ë””ì˜¤ì˜ ìŒì„± ì¸ì‹, ì˜¤ë””ì˜¤ ì¢…ë£Œ ê°ì§€, ì ì ˆí•œ ì‹œì ì— ì›Œí¬í”Œë¡œ í˜¸ì¶œ, ê·¸ë¦¬ê³  ì›Œí¬í”Œë¡œ ì¶œë ¥ì˜ ì˜¤ë””ì˜¤ ë³€í™˜ê¹Œì§€ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```mermaid
graph LR
    %% Input
    A["ğŸ¤ Audio Input"]

    %% Voice Pipeline
    subgraph Voice_Pipeline [Voice Pipeline]
        direction TB
        B["Transcribe (speech-to-text)"]
        C["Your Code"]:::highlight
        D["Text-to-speech"]
        B --> C --> D
    end

    %% Output
    E["ğŸ§ Audio Output"]

    %% Flow
    A --> Voice_Pipeline
    Voice_Pipeline --> E

    %% Custom styling
    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;

```

## íŒŒì´í”„ë¼ì¸ êµ¬ì„±

íŒŒì´í”„ë¼ì¸ì„ ìƒì„±í•  ë•Œ ë‹¤ìŒ í•­ëª©ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. ìƒˆ ì˜¤ë””ì˜¤ê°€ ì „ì‚¬ë  ë•Œë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” ì½”ë“œì¸ [`workflow`][agents.voice.workflow.VoiceWorkflowBase]
2. ì‚¬ìš©ë˜ëŠ” [`speech-to-text`][agents.voice.model.STTModel] ë° [`text-to-speech`][agents.voice.model.TTSModel] ëª¨ë¸
3. ë‹¤ìŒê³¼ ê°™ì€ í•­ëª©ì„ ì„¤ì •í•  ìˆ˜ ìˆëŠ” [`config`][agents.voice.pipeline_config.VoicePipelineConfig]
    - ëª¨ë¸ ì´ë¦„ì„ ì‹¤ì œ ëª¨ë¸ë¡œ ë§¤í•‘í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ ì œê³µì
    - íŠ¸ë ˆì´ì‹± ì„¤ì •: íŠ¸ë ˆì´ì‹± ë¹„í™œì„±í™” ì—¬ë¶€, ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ì—¬ë¶€, ì›Œí¬í”Œë¡œ ì´ë¦„, íŠ¸ë ˆì´ìŠ¤ ID ë“±
    - TTS ë° STT ëª¨ë¸ì˜ í”„ë¡¬í”„íŠ¸, ì–¸ì–´, ì‚¬ìš© ë°ì´í„° íƒ€ì… ë“±ì˜ ì„¤ì •

## íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

íŒŒì´í”„ë¼ì¸ì€ [`run()`][agents.voice.pipeline.VoicePipeline.run] ë©”ì„œë“œë¡œ ì‹¤í–‰í•˜ë©°, ì˜¤ë””ì˜¤ ì…ë ¥ì„ ë‘ ê°€ì§€ í˜•íƒœë¡œ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. ì „ì²´ ì˜¤ë””ì˜¤ ì „ì‚¬ê°€ ìˆì„ ë•Œ ê²°ê³¼ë§Œ ìƒì„±í•˜ë©´ ë˜ëŠ” ê²½ìš° [`AudioInput`][agents.voice.input.AudioInput]ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” ë°œí™” ì¢…ë£Œ ê°ì§€ê°€ í•„ìš” ì—†ëŠ” ìƒí™©ì— ìœ ìš©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ì „ ë…¹ìŒëœ ì˜¤ë””ì˜¤ì´ê±°ë‚˜, ì‚¬ìš©ìì˜ ë°œí™” ì¢…ë£Œ ì‹œì ì´ ëª…í™•í•œ í‘¸ì‹œíˆ¬í† í¬ ì•±ì—ì„œ ìœ ìš©í•©ë‹ˆë‹¤
2. ì‚¬ìš©ìì˜ ë°œí™” ì¢…ë£Œë¥¼ ê°ì§€í•´ì•¼ í•  ìˆ˜ ìˆëŠ” ê²½ìš° [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê°ì§€ëœ ëŒ€ë¡œ ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ í‘¸ì‹œí•  ìˆ˜ ìˆìœ¼ë©°, ìŒì„± íŒŒì´í”„ë¼ì¸ì€ "activity detection(í™œë™ ê°ì§€)"ì´ë¼ê³  í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ ì ì ˆí•œ ì‹œì ì— ìë™ìœ¼ë¡œ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤

## ê²°ê³¼

ìŒì„± íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ëŠ” [`StreamedAudioResult`][agents.voice.result.StreamedAudioResult]ì…ë‹ˆë‹¤. ì´ëŠ” ë°œìƒí•˜ëŠ” ì´ë²¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ìˆ˜ì‹ í•  ìˆ˜ ìˆëŠ” ê°ì²´ì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ì—¬ëŸ¬ ì¢…ë¥˜ì˜ [`VoiceStreamEvent`][agents.voice.events.VoiceStreamEvent]ê°€ ìˆìŠµë‹ˆë‹¤:

1. ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ í¬í•¨í•˜ëŠ” [`VoiceStreamEventAudio`][agents.voice.events.VoiceStreamEventAudio]
2. í„´ ì‹œì‘/ì¢…ë£Œ ê°™ì€ ë¼ì´í”„ì‚¬ì´í´ ì´ë²¤íŠ¸ë¥¼ ì•Œë ¤ì£¼ëŠ” [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle]
3. ì˜¤ë¥˜ ì´ë²¤íŠ¸ì¸ [`VoiceStreamEventError`][agents.voice.events.VoiceStreamEventError]

```python

result = await pipeline.run(input)

async for event in result.stream():
    if event.type == "voice_stream_event_audio":
        # play audio
    elif event.type == "voice_stream_event_lifecycle":
        # lifecycle
    elif event.type == "voice_stream_event_error"
        # error
    ...
```

## ëª¨ë²” ì‚¬ë¡€

### ì¸í„°ëŸ½ì…˜(ì¤‘ë‹¨ ì²˜ë¦¬)

Agents SDKëŠ” í˜„ì¬ [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]ì— ëŒ€í•œ ë‚´ì¥ ì¸í„°ëŸ½ì…˜(ì¤‘ë‹¨ ì²˜ë¦¬) ê¸°ëŠ¥ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹  ê°ì§€ëœ ê° í„´ë§ˆë‹¤ ì›Œí¬í”Œë¡œì˜ ë³„ë„ ì‹¤í–‰ì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤. ì• í”Œë¦¬ì¼€ì´ì…˜ ë‚´ë¶€ì—ì„œ ì¸í„°ëŸ½ì…˜ì„ ì²˜ë¦¬í•˜ë ¤ë©´ [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle] ì´ë²¤íŠ¸ë¥¼ ìˆ˜ì‹ í•˜ë©´ ë©ë‹ˆë‹¤. `turn_started`ëŠ” ìƒˆ í„´ì´ ì „ì‚¬ë˜ì–´ ì²˜ë¦¬ê°€ ì‹œì‘ë˜ì—ˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. `turn_ended`ëŠ” í•´ë‹¹ í„´ì— ëŒ€í•œ ëª¨ë“  ì˜¤ë””ì˜¤ê°€ ë””ìŠ¤íŒ¨ì¹˜ëœ í›„ íŠ¸ë¦¬ê±°ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ë²¤íŠ¸ë¥¼ ì‚¬ìš©í•´, ëª¨ë¸ì´ í„´ì„ ì‹œì‘í•  ë•Œ í™”ìì˜ ë§ˆì´í¬ë¥¼ ìŒì†Œê±°í•˜ê³ , í•´ë‹¹ í„´ê³¼ ê´€ë ¨ëœ ì˜¤ë””ì˜¤ë¥¼ ëª¨ë‘ í”ŒëŸ¬ì‹œí•œ ë’¤ ìŒì†Œê±°ë¥¼ í•´ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

================
File: docs/ko/voice/quickstart.md
================
---
search:
  exclude: true
---
# ë¹ ë¥¸ ì‹œì‘

## ì¤€ë¹„ ì‚¬í•­

Agents SDKì˜ ê¸°ë³¸ [ë¹ ë¥¸ ì‹œì‘ ì•ˆë‚´](../quickstart.md)ë¥¼ ë”°ë¼ ê°€ìƒ í™˜ê²½ì„ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ê·¸ëŸ° ë‹¤ìŒ, SDKì—ì„œ ì œê³µí•˜ëŠ” ì„ íƒì  ìŒì„± ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤:

```bash
pip install 'openai-agents[voice]'
```

## ê°œë…

ì•Œì•„ë‘ì–´ì•¼ í•  í•µì‹¬ ê°œë…ì€ [`VoicePipeline`][agents.voice.pipeline.VoicePipeline]ì´ë©°, 3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤:

1. ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ìŒì„±-í…ìŠ¤íŠ¸ ëª¨ë¸ì„ ì‹¤í–‰
2. ê²°ê³¼ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ì½”ë“œ(ë³´í†µì€ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°)ë¥¼ ì‹¤í–‰
3. ê²°ê³¼ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ í…ìŠ¤íŠ¸-ìŒì„± ëª¨ë¸ì„ ì‹¤í–‰

```mermaid
graph LR
    %% Input
    A["ğŸ¤ Audio Input"]

    %% Voice Pipeline
    subgraph Voice_Pipeline [Voice Pipeline]
        direction TB
        B["Transcribe (speech-to-text)"]
        C["Your Code"]:::highlight
        D["Text-to-speech"]
        B --> C --> D
    end

    %% Output
    E["ğŸ§ Audio Output"]

    %% Flow
    A --> Voice_Pipeline
    Voice_Pipeline --> E

    %% Custom styling
    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;

```

## ì—ì´ì „íŠ¸

ë¨¼ì € ì—ì´ì „íŠ¸ë¥¼ ì„¤ì •í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì´ SDKë¡œ ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ì–´ ë³¸ ì ì´ ìˆë‹¤ë©´ ìµìˆ™í•˜ê²Œ ëŠê»´ì§ˆ ê²ƒì…ë‹ˆë‹¤. ì—ì´ì „íŠ¸ ëª‡ ê°œì™€ í•¸ë“œì˜¤í”„, ê·¸ë¦¬ê³  í•˜ë‚˜ì˜ ë„êµ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
import asyncio
import random

from agents import (
    Agent,
    function_tool,
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions



@function_tool
def get_weather(city: str) -> str:
    """Get the weather for a given city."""
    print(f"[debug] get_weather called with city: {city}")
    choices = ["sunny", "cloudy", "rainy", "snowy"]
    return f"The weather in {city} is {random.choice(choices)}."


spanish_agent = Agent(
    name="Spanish",
    handoff_description="A spanish speaking agent.",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. Speak in Spanish.",
    ),
    model="gpt-4.1",
)

agent = Agent(
    name="Assistant",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.",
    ),
    model="gpt-4.1",
    handoffs=[spanish_agent],
    tools=[get_weather],
)
```

## ìŒì„± íŒŒì´í”„ë¼ì¸

ì›Œí¬í”Œë¡œìš°ë¡œ [`SingleAgentVoiceWorkflow`][agents.voice.workflow.SingleAgentVoiceWorkflow]ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ìŒì„± íŒŒì´í”„ë¼ì¸ì„ ì„¤ì •í•˜ê² ìŠµë‹ˆë‹¤.

```python
from agents.voice import SingleAgentVoiceWorkflow, VoicePipeline
pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))
```

## íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

```python
import numpy as np
import sounddevice as sd
from agents.voice import AudioInput

# For simplicity, we'll just create 3 seconds of silence
# In reality, you'd get microphone data
buffer = np.zeros(24000 * 3, dtype=np.int16)
audio_input = AudioInput(buffer=buffer)

result = await pipeline.run(audio_input)

# Create an audio player using `sounddevice`
player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
player.start()

# Play the audio stream as it comes in
async for event in result.stream():
    if event.type == "voice_stream_event_audio":
        player.write(event.data)

```

## ì „ì²´ êµ¬ì„±

```python
import asyncio
import random

import numpy as np
import sounddevice as sd

from agents import (
    Agent,
    function_tool,
    set_tracing_disabled,
)
from agents.voice import (
    AudioInput,
    SingleAgentVoiceWorkflow,
    VoicePipeline,
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions


@function_tool
def get_weather(city: str) -> str:
    """Get the weather for a given city."""
    print(f"[debug] get_weather called with city: {city}")
    choices = ["sunny", "cloudy", "rainy", "snowy"]
    return f"The weather in {city} is {random.choice(choices)}."


spanish_agent = Agent(
    name="Spanish",
    handoff_description="A spanish speaking agent.",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. Speak in Spanish.",
    ),
    model="gpt-4.1",
)

agent = Agent(
    name="Assistant",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.",
    ),
    model="gpt-4.1",
    handoffs=[spanish_agent],
    tools=[get_weather],
)


async def main():
    pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))
    buffer = np.zeros(24000 * 3, dtype=np.int16)
    audio_input = AudioInput(buffer=buffer)

    result = await pipeline.run(audio_input)

    # Create an audio player using `sounddevice`
    player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
    player.start()

    # Play the audio stream as it comes in
    async for event in result.stream():
        if event.type == "voice_stream_event_audio":
            player.write(event.data)


if __name__ == "__main__":
    asyncio.run(main())
```

ì´ ì˜ˆì œë¥¼ ì‹¤í–‰í•˜ë©´ ì—ì´ì „íŠ¸ê°€ ë§í•©ë‹ˆë‹¤! ì§ì ‘ ì—ì´ì „íŠ¸ì™€ ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ë°ëª¨ëŠ” [examples/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static)ì—ì„œ í™•ì¸í•˜ì„¸ìš”.

================
File: docs/ko/voice/tracing.md
================
---
search:
  exclude: true
---
# íŠ¸ë ˆì´ì‹±

[ì—ì´ì „íŠ¸ íŠ¸ë ˆì´ì‹±](../tracing.md) ë°©ì‹ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ë³´ì´ìŠ¤ íŒŒì´í”„ë¼ì¸ë„ ìë™ìœ¼ë¡œ íŠ¸ë ˆì´ì‹±ë©ë‹ˆë‹¤.

ê¸°ë³¸ íŠ¸ë ˆì´ì‹± ì •ë³´ëŠ” ìœ„ ë§í¬ì˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì‹œê³ , ì¶”ê°€ë¡œ [`VoicePipelineConfig`][agents.voice.pipeline_config.VoicePipelineConfig]ë¥¼ í†µí•´ íŒŒì´í”„ë¼ì¸ íŠ¸ë ˆì´ì‹±ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

íŠ¸ë ˆì´ì‹± ê´€ë ¨ ì£¼ìš” í•„ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

-   [`tracing_disabled`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]: íŠ¸ë ˆì´ì‹± ë¹„í™œì„±í™” ì—¬ë¶€ë¥¼ ì œì–´í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ íŠ¸ë ˆì´ì‹± í™œì„±í™”ì…ë‹ˆë‹¤
-   [`trace_include_sensitive_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_data]: ì˜¤ë””ì˜¤ ì „ì‚¬ë³¸ê³¼ ê°™ì´ ë¯¼ê°í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë¥¼ íŠ¸ë ˆì´ìŠ¤ì— í¬í•¨í• ì§€ ì œì–´í•©ë‹ˆë‹¤. ì´ëŠ” ë³´ì´ìŠ¤ íŒŒì´í”„ë¼ì¸ì—ë§Œ í•´ë‹¹í•˜ë©°, Workflow ë‚´ë¶€ì—ì„œ ë°œìƒí•˜ëŠ” ì‘ì—…ì—ëŠ” ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
-   [`trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data]: íŠ¸ë ˆì´ìŠ¤ì— ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ í¬í•¨í• ì§€ ì œì–´í•©ë‹ˆë‹¤
-   [`workflow_name`][agents.voice.pipeline_config.VoicePipelineConfig.workflow_name]: íŠ¸ë ˆì´ìŠ¤ ì›Œí¬í”Œë¡œì˜ ì´ë¦„
-   [`group_id`][agents.voice.pipeline_config.VoicePipelineConfig.group_id]: íŠ¸ë ˆì´ìŠ¤ì˜ `group_id`ë¡œ, ì—¬ëŸ¬ íŠ¸ë ˆì´ìŠ¤ë¥¼ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
-   [`trace_metadata`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]: íŠ¸ë ˆì´ìŠ¤ì— í¬í•¨í•  ì¶”ê°€ ë©”íƒ€ë°ì´í„°

================
File: docs/ko/agents.md
================
---
search:
  exclude: true
---
# ì—ì´ì „íŠ¸

ì—ì´ì „íŠ¸ëŠ” ì•±ì˜ í•µì‹¬ ë¹Œë”© ë¸”ë¡ì…ë‹ˆë‹¤. ì—ì´ì „íŠ¸ëŠ” instructions ì™€ ë„êµ¬ë¡œ êµ¬ì„±ëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì…ë‹ˆë‹¤.

## ê¸°ë³¸ êµ¬ì„±

ì—ì´ì „íŠ¸ì—ì„œ ê°€ì¥ í”íˆ êµ¬ì„±í•˜ëŠ” ì†ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- `name`: ì—ì´ì „íŠ¸ë¥¼ ì‹ë³„í•˜ëŠ” í•„ìˆ˜ ë¬¸ìì—´
- `instructions`: ê°œë°œì ë©”ì‹œì§€ ë˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¼ê³ ë„ í•¨
- `model`: ì‚¬ìš©í•  LLM ë° temperature, top_p ë“± ëª¨ë¸ íŠœë‹ ë§¤ê°œë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ëŠ” ì„ íƒì  `model_settings`
- `tools`: ì—ì´ì „íŠ¸ê°€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„êµ¬

```python
from agents import Agent, ModelSettings, function_tool

@function_tool
def get_weather(city: str) -> str:
    """returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Haiku agent",
    instructions="Always respond in haiku form",
    model="gpt-5-nano",
    tools=[get_weather],
)
```

## ì»¨í…ìŠ¤íŠ¸

ì—ì´ì „íŠ¸ëŠ” `context` íƒ€ì…ì— ëŒ€í•´ ì œë„¤ë¦­ì…ë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ëŠ” ì˜ì¡´ì„± ì£¼ì… ë„êµ¬ë¡œ, ì—¬ëŸ¬ë¶„ì´ ìƒì„±í•˜ì—¬ `Runner.run()` ì— ì „ë‹¬í•˜ëŠ” ê°ì²´ì´ë©° ëª¨ë“  ì—ì´ì „íŠ¸, ë„êµ¬, í•¸ë“œì˜¤í”„ ë“±ì— ì „ë‹¬ë˜ì–´ ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ìœ„í•œ ì˜ì¡´ì„±ê³¼ ìƒíƒœë¥¼ ë‹´ëŠ” ì»¨í…Œì´ë„ˆ ì—­í• ì„ í•©ë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ë¡œëŠ” ì–´ë–¤ Python ê°ì²´ë“  ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
@dataclass
class UserContext:
    name: str
    uid: str
    is_pro_user: bool

    async def fetch_purchases() -> list[Purchase]:
        return ...

agent = Agent[UserContext](
    ...,
)
```

## ì¶œë ¥ ìœ í˜•

ê¸°ë³¸ì ìœ¼ë¡œ ì—ì´ì „íŠ¸ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸(ì¦‰, `str`) ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ê°€ íŠ¹ì • ìœ í˜•ì˜ ì¶œë ¥ì„ ìƒì„±í•˜ë„ë¡ í•˜ë ¤ë©´ `output_type` ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” [Pydantic](https://docs.pydantic.dev/) ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, Pydantic [TypeAdapter](https://docs.pydantic.dev/latest/api/type_adapter/) ë¡œ ë˜í•‘ë  ìˆ˜ ìˆëŠ” ëª¨ë“  íƒ€ì…ì„ ì§€ì›í•©ë‹ˆë‹¤ â€” dataclass, ë¦¬ìŠ¤íŠ¸, TypedDict ë“±

```python
from pydantic import BaseModel
from agents import Agent


class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

agent = Agent(
    name="Calendar extractor",
    instructions="Extract calendar events from text",
    output_type=CalendarEvent,
)
```

!!! note

    `output_type` ì„ ì „ë‹¬í•˜ë©´, ëª¨ë¸ì€ ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ ëŒ€ì‹  [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì§€ì‹œë°›ìŠµë‹ˆë‹¤.

## ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì„¤ê³„ íŒ¨í„´

ë©€í‹°â€‘ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ëŠ” ë°©ë²•ì€ ë‹¤ì–‘í•˜ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ë‘ ê°€ì§€ ê´‘ë²”ìœ„í•˜ê²Œ ì ìš© ê°€ëŠ¥í•œ íŒ¨í„´ì´ ìˆìŠµë‹ˆë‹¤:

1. ë§¤ë‹ˆì €(ì—ì´ì „íŠ¸ë¥¼ ë„êµ¬ë¡œ ì‚¬ìš©): ì¤‘ì•™ ë§¤ë‹ˆì €/ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ê°€ íŠ¹í™”ëœ í•˜ìœ„ ì—ì´ì „íŠ¸ë¥¼ ë„êµ¬ë¡œ í˜¸ì¶œí•˜ê³  ëŒ€í™”ì˜ ì œì–´ê¶Œì„ ìœ ì§€í•¨
2. í•¸ë“œì˜¤í”„: ë™ë“±í•œ ì—ì´ì „íŠ¸ê°€ ì œì–´ê¶Œì„ íŠ¹í™”ëœ ì—ì´ì „íŠ¸ì—ê²Œ ë„˜ê²¨ í•´ë‹¹ ì—ì´ì „íŠ¸ê°€ ëŒ€í™”ë¥¼ ì´ì–´ê°. ì´ëŠ” ë¶„ì‚°í˜•ì„

ìì„¸í•œ ë‚´ìš©ì€ [ì—ì´ì „íŠ¸ êµ¬ì¶• ì‹¤ë¬´ ê°€ì´ë“œ](https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf) ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

### ë§¤ë‹ˆì €(ì—ì´ì „íŠ¸ë¥¼ ë„êµ¬ë¡œ ì‚¬ìš©)

`customer_facing_agent` ê°€ ëª¨ë“  ì‚¬ìš©ì ìƒí˜¸ì‘ìš©ì„ ì²˜ë¦¬í•˜ê³ , ë„êµ¬ë¡œ ë…¸ì¶œëœ íŠ¹í™”ëœ í•˜ìœ„ ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ë„êµ¬](tools.md#agents-as-tools) ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

```python
from agents import Agent

booking_agent = Agent(...)
refund_agent = Agent(...)

customer_facing_agent = Agent(
    name="Customer-facing agent",
    instructions=(
        "Handle all direct user communication. "
        "Call the relevant tools when specialized expertise is needed."
    ),
    tools=[
        booking_agent.as_tool(
            tool_name="booking_expert",
            tool_description="Handles booking questions and requests.",
        ),
        refund_agent.as_tool(
            tool_name="refund_expert",
            tool_description="Handles refund questions and requests.",
        )
    ],
)
```

### í•¸ë“œì˜¤í”„

í•¸ë“œì˜¤í”„ëŠ” ì—ì´ì „íŠ¸ê°€ ìœ„ì„í•  ìˆ˜ ìˆëŠ” í•˜ìœ„ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. í•¸ë“œì˜¤í”„ê°€ ë°œìƒí•˜ë©´, ìœ„ì„ëœ ì—ì´ì „íŠ¸ê°€ ëŒ€í™” ê¸°ë¡ì„ ì „ë‹¬ë°›ì•„ ëŒ€í™”ë¥¼ ì´ì–´ê°‘ë‹ˆë‹¤. ì´ íŒ¨í„´ì€ ë‹¨ì¼ ì‘ì—…ì— íŠ¹í™”ë˜ì–´ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë“ˆì‹ íŠ¹í™” ì—ì´ì „íŠ¸ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [í•¸ë“œì˜¤í”„](handoffs.md) ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

```python
from agents import Agent

booking_agent = Agent(...)
refund_agent = Agent(...)

triage_agent = Agent(
    name="Triage agent",
    instructions=(
        "Help the user with their questions. "
        "If they ask about booking, hand off to the booking agent. "
        "If they ask about refunds, hand off to the refund agent."
    ),
    handoffs=[booking_agent, refund_agent],
)
```

## ë™ì  instructions

ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•  ë•Œ instructions ë¥¼ ì œê³µí•˜ë©´ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ í•¨ìˆ˜ë¡œ ë™ì  instructions ë¥¼ ì œê³µí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì—ì´ì „íŠ¸ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê³  í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ë™ê¸° ë° `async` í•¨ìˆ˜ ëª¨ë‘ í—ˆìš©ë©ë‹ˆë‹¤.

```python
def dynamic_instructions(
    context: RunContextWrapper[UserContext], agent: Agent[UserContext]
) -> str:
    return f"The user's name is {context.context.name}. Help them with their questions."


agent = Agent[UserContext](
    name="Triage agent",
    instructions=dynamic_instructions,
)
```

## ë¼ì´í”„ì‚¬ì´í´ ì´ë²¤íŠ¸(í›…)

ë•Œë•Œë¡œ ì—ì´ì „íŠ¸ì˜ ë¼ì´í”„ì‚¬ì´í´ì„ ê´€ì°°í•˜ê³  ì‹¶ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì´ë²¤íŠ¸ë¥¼ ë¡œê¹…í•˜ê±°ë‚˜ íŠ¹ì • ì´ë²¤íŠ¸ê°€ ë°œìƒí•  ë•Œ ë°ì´í„°ë¥¼ ì‚¬ì „ ë¡œë“œí•˜ê³  ì‹¶ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. `hooks` ì†ì„±ìœ¼ë¡œ ì—ì´ì „íŠ¸ ë¼ì´í”„ì‚¬ì´í´ì— í›…ì„ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [`AgentHooks`][agents.lifecycle.AgentHooks] í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ ê´€ì‹¬ ìˆëŠ” ë©”ì„œë“œë¥¼ ì˜¤ë²„ë¼ì´ë“œí•˜ì„¸ìš”.

## ê°€ë“œë ˆì¼

ê°€ë“œë ˆì¼ì€ ì—ì´ì „íŠ¸ê°€ ì‹¤í–‰ë˜ëŠ” ë™ì•ˆ ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ê²€ì‚¬/ê²€ì¦ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ê³ , ì—ì´ì „íŠ¸ ì¶œë ¥ì´ ìƒì„±ëœ í›„ì—ë„ ê²€ì‚¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ìš©ì ì…ë ¥ê³¼ ì—ì´ì „íŠ¸ ì¶œë ¥ì„ ê´€ë ¨ì„± ê¸°ì¤€ìœ¼ë¡œ ìŠ¤í¬ë¦¬ë‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ê°€ë“œë ˆì¼](guardrails.md) ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

## ì—ì´ì „íŠ¸ ë³µì œ/ë³µì‚¬

ì—ì´ì „íŠ¸ì˜ `clone()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ì—ì´ì „íŠ¸ë¥¼ ë³µì œí•˜ê³ , ì„ íƒì ìœ¼ë¡œ ì›í•˜ëŠ” ì†ì„±ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
pirate_agent = Agent(
    name="Pirate",
    instructions="Write like a pirate",
    model="gpt-4.1",
)

robot_agent = pirate_agent.clone(
    name="Robot",
    instructions="Write like a robot",
)
```

## ë„êµ¬ ì‚¬ìš© ê°•ì œ

ë„êµ¬ ëª©ë¡ì„ ì œê³µí•˜ë”ë¼ë„ LLM ì´ í•­ìƒ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. [`ModelSettings.tool_choice`][agents.model_settings.ModelSettings.tool_choice] ë¥¼ ì„¤ì •í•˜ì—¬ ë„êµ¬ ì‚¬ìš©ì„ ê°•ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ íš¨í•œ ê°’ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. `auto`: LLM ì´ ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ë¥¼ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•¨
2. `required`: LLM ì´ ë°˜ë“œì‹œ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨(í•˜ì§€ë§Œ ì–´ë–¤ ë„êµ¬ë¥¼ ì‚¬ìš©í• ì§€ëŠ” ì§€ëŠ¥ì ìœ¼ë¡œ ì„ íƒ)
3. `none`: LLM ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ìš”êµ¬í•¨
4. íŠ¹ì • ë¬¸ìì—´ ì„¤ì • ì˜ˆ: `my_tool`, í•´ë‹¹ íŠ¹ì • ë„êµ¬ë¥¼ ë°˜ë“œì‹œ ì‚¬ìš©í•˜ë„ë¡ ìš”êµ¬í•¨

```python
from agents import Agent, Runner, function_tool, ModelSettings

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Weather Agent",
    instructions="Retrieve weather details.",
    tools=[get_weather],
    model_settings=ModelSettings(tool_choice="get_weather")
)
```

## ë„êµ¬ ì‚¬ìš© ë™ì‘

`Agent` êµ¬ì„±ì˜ `tool_use_behavior` ë§¤ê°œë³€ìˆ˜ëŠ” ë„êµ¬ ì¶œë ¥ì´ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ëŠ”ì§€ë¥¼ ì œì–´í•©ë‹ˆë‹¤:

- "run_llm_again": ê¸°ë³¸ê°’. ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ê³ , LLM ì´ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•¨
- "stop_on_first_tool": ì²« ë²ˆì§¸ ë„êµ¬ í˜¸ì¶œì˜ ì¶œë ¥ì„ ì¶”ê°€ì ì¸ LLM ì²˜ë¦¬ ì—†ì´ ìµœì¢… ì‘ë‹µìœ¼ë¡œ ì‚¬ìš©í•¨

```python
from agents import Agent, Runner, function_tool, ModelSettings

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Weather Agent",
    instructions="Retrieve weather details.",
    tools=[get_weather],
    tool_use_behavior="stop_on_first_tool"
)
```

- `StopAtTools(stop_at_tool_names=[...])`: ì§€ì •ëœ ë„êµ¬ ì¤‘ í•˜ë‚˜ê°€ í˜¸ì¶œë˜ë©´ ì¤‘ì§€í•˜ê³ , í•´ë‹¹ ë„êµ¬ì˜ ì¶œë ¥ì„ ìµœì¢… ì‘ë‹µìœ¼ë¡œ ì‚¬ìš©í•¨

```python
from agents import Agent, Runner, function_tool
from agents.agent import StopAtTools

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

@function_tool
def sum_numbers(a: int, b: int) -> int:
    """Adds two numbers."""
    return a + b

agent = Agent(
    name="Stop At Stock Agent",
    instructions="Get weather or sum numbers.",
    tools=[get_weather, sum_numbers],
    tool_use_behavior=StopAtTools(stop_at_tool_names=["get_weather"])
)
```

- `ToolsToFinalOutputFunction`: ë„êµ¬ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ê³  ì¤‘ì§€í• ì§€ LLM ê³¼ ê³„ì†í• ì§€ë¥¼ ê²°ì •í•˜ëŠ” ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜

```python
from agents import Agent, Runner, function_tool, FunctionToolResult, RunContextWrapper
from agents.agent import ToolsToFinalOutputResult
from typing import List, Any

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

def custom_tool_handler(
    context: RunContextWrapper[Any],
    tool_results: List[FunctionToolResult]
) -> ToolsToFinalOutputResult:
    """Processes tool results to decide final output."""
    for result in tool_results:
        if result.output and "sunny" in result.output:
            return ToolsToFinalOutputResult(
                is_final_output=True,
                final_output=f"Final weather: {result.output}"
            )
    return ToolsToFinalOutputResult(
        is_final_output=False,
        final_output=None
    )

agent = Agent(
    name="Weather Agent",
    instructions="Retrieve weather details.",
    tools=[get_weather],
    tool_use_behavior=custom_tool_handler
)
```

!!! note

    ë¬´í•œ ë£¨í”„ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´, í”„ë ˆì„ì›Œí¬ëŠ” ë„êµ¬ í˜¸ì¶œ í›„ `tool_choice` ë¥¼ ìë™ìœ¼ë¡œ "auto" ë¡œ ì¬ì„¤ì •í•©ë‹ˆë‹¤. ì´ ë™ì‘ì€ [`agent.reset_tool_choice`][agents.agent.Agent.reset_tool_choice] ë¥¼ í†µí•´ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬´í•œ ë£¨í”„ì˜ ì›ì¸ì€ ë„êµ¬ ê²°ê³¼ê°€ LLM ìœ¼ë¡œ ì „ë‹¬ë˜ê³ , `tool_choice` ë¡œ ì¸í•´ LLM ì´ ë‹¤ì‹œ ë„êµ¬ í˜¸ì¶œì„ ìƒì„±í•˜ëŠ” ì¼ì´ ë°˜ë³µë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

================
File: docs/ko/config.md
================
---
search:
  exclude: true
---
# SDK êµ¬ì„±

## API í‚¤ì™€ í´ë¼ì´ì–¸íŠ¸

ê¸°ë³¸ì ìœ¼ë¡œ SDKëŠ” ê°€ì ¸ì˜¤ìë§ˆì LLM ìš”ì²­ê³¼ íŠ¸ë ˆì´ì‹±ì„ ìœ„í•´ `OPENAI_API_KEY` í™˜ê²½ ë³€ìˆ˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ì•± ì‹œì‘ ì „ì— í•´ë‹¹ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•  ìˆ˜ ì—†ë‹¤ë©´ [set_default_openai_key()][agents.set_default_openai_key] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í‚¤ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from agents import set_default_openai_key

set_default_openai_key("sk-...")
```

ë˜í•œ ì‚¬ìš©í•  OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ SDKëŠ” í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ìœ„ì—ì„œ ì„¤ì •í•œ ê¸°ë³¸ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ `AsyncOpenAI` ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŠ” [set_default_openai_client()][agents.set_default_openai_client] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from openai import AsyncOpenAI
from agents import set_default_openai_client

custom_client = AsyncOpenAI(base_url="...", api_key="...")
set_default_openai_client(custom_client)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, ì‚¬ìš©ë˜ëŠ” OpenAI APIë¥¼ ì‚¬ìš©ì ì§€ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œëŠ” OpenAI Responses APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. [set_default_openai_api()][agents.set_default_openai_api] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Chat Completions APIë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì¬ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from agents import set_default_openai_api

set_default_openai_api("chat_completions")
```

## íŠ¸ë ˆì´ì‹±

íŠ¸ë ˆì´ì‹±ì€ ê¸°ë³¸ì ìœ¼ë¡œ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ìœ„ ì„¹ì…˜ì˜ OpenAI API í‚¤(ì¦‰, í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ì„¤ì •í•œ ê¸°ë³¸ í‚¤)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. íŠ¸ë ˆì´ì‹±ì— ì‚¬ìš©í•  API í‚¤ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•˜ë ¤ë©´ [`set_tracing_export_api_key`][agents.set_tracing_export_api_key] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

```python
from agents import set_tracing_export_api_key

set_tracing_export_api_key("sk-...")
```

[`set_tracing_disabled()`][agents.set_tracing_disabled] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ë ˆì´ì‹±ì„ ì™„ì „íˆ ë¹„í™œì„±í™”í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```python
from agents import set_tracing_disabled

set_tracing_disabled(True)
```

## ë””ë²„ê·¸ ë¡œê¹…

SDKì—ëŠ” í•¸ë“¤ëŸ¬ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ë‘ ê°œì˜ Python ë¡œê±°ê°€ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ì´ëŠ” ê²½ê³ ì™€ ì˜¤ë¥˜ê°€ `stdout`ìœ¼ë¡œ ì „ì†¡ë˜ê³ , ë‹¤ë¥¸ ë¡œê·¸ëŠ” ì–µì œë¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

ìì„¸í•œ ë¡œê¹…ì„ í™œì„±í™”í•˜ë ¤ë©´ [`enable_verbose_stdout_logging()`][agents.enable_verbose_stdout_logging] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

```python
from agents import enable_verbose_stdout_logging

enable_verbose_stdout_logging()
```

ë˜ëŠ” í•¸ë“¤ëŸ¬, í•„í„°, í¬ë§¤í„° ë“±ì„ ì¶”ê°€í•˜ì—¬ ë¡œê·¸ë¥¼ ì‚¬ìš©ì ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [Python ë¡œê¹… ê°€ì´ë“œ](https://docs.python.org/3/howto/logging.html)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

```python
import logging

logger = logging.getLogger("openai.agents") # or openai.agents.tracing for the Tracing logger

# To make all logs show up
logger.setLevel(logging.DEBUG)
# To make info and above show up
logger.setLevel(logging.INFO)
# To make warning and above show up
logger.setLevel(logging.WARNING)
# etc

# You can customize this as needed, but this will output to `stderr` by default
logger.addHandler(logging.StreamHandler())
```

### ë¡œê·¸ì˜ ë¯¼ê°í•œ ë°ì´í„°

ì¼ë¶€ ë¡œê·¸ì—ëŠ” ë¯¼ê°í•œ ë°ì´í„°(ì˜ˆ: ì‚¬ìš©ì ë°ì´í„°)ê°€ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ì˜ ë¡œê¹…ì„ ë¹„í™œì„±í™”í•˜ë ¤ë©´ ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

LLM ì…ë ¥ê³¼ ì¶œë ¥ì„ ë¡œê¹…í•˜ì§€ ì•Šìœ¼ë ¤ë©´:

```bash
export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1
```

ë„êµ¬ ì…ë ¥ê³¼ ì¶œë ¥ì„ ë¡œê¹…í•˜ì§€ ì•Šìœ¼ë ¤ë©´:

```bash
export OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1
```

================
File: docs/ko/context.md
================
---
search:
  exclude: true
---
# ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬

ì»¨í…ìŠ¤íŠ¸ëŠ” ì—¬ëŸ¬ ì˜ë¯¸ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ì»¨í…ìŠ¤íŠ¸ëŠ” ë‘ ê°€ì§€ì…ë‹ˆë‹¤:

1. ì½”ë“œì—ì„œ ë¡œì»¬ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì»¨í…ìŠ¤íŠ¸: ë„êµ¬ í•¨ìˆ˜ê°€ ì‹¤í–‰ë  ë•Œ, `on_handoff` ê°™ì€ ì½œë°± ì¤‘, ë¼ì´í”„ì‚¬ì´í´ í›… ë“±ì—ì„œ í•„ìš”í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì™€ ì˜ì¡´ì„±
2. LLMì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì»¨í…ìŠ¤íŠ¸: LLMì´ ì‘ë‹µì„ ìƒì„±í•  ë•Œ ë³¼ ìˆ˜ ìˆëŠ” ë°ì´í„°

## ë¡œì»¬ ì»¨í…ìŠ¤íŠ¸

ì´ëŠ” [`RunContextWrapper`][agents.run_context.RunContextWrapper] í´ë˜ìŠ¤ì™€ ê·¸ ì•ˆì˜ [`context`][agents.run_context.RunContextWrapper.context] ì†ì„±ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ë™ì‘ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. ì›í•˜ëŠ” ì–´ë–¤ Python ê°ì²´ë“  ìƒì„±í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ dataclass ë˜ëŠ” Pydantic ê°ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
2. í•´ë‹¹ ê°ì²´ë¥¼ ë‹¤ì–‘í•œ run ë©”ì„œë“œì— ì „ë‹¬í•©ë‹ˆë‹¤(ì˜ˆ: `Runner.run(..., **context=whatever**)`)
3. ëª¨ë“  ë„êµ¬ í˜¸ì¶œ, ë¼ì´í”„ì‚¬ì´í´ í›… ë“±ì—ëŠ” ë˜í¼ ê°ì²´ `RunContextWrapper[T]`ê°€ ì „ë‹¬ë˜ë©°, ì—¬ê¸°ì„œ `T`ëŠ” ì»¨í…ìŠ¤íŠ¸ ê°ì²´ì˜ íƒ€ì…ì„ ì˜ë¯¸í•˜ê³  `wrapper.context`ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

ê°€ì¥ ì¤‘ìš”í•œ ì : íŠ¹ì • ì—ì´ì „íŠ¸ ì‹¤í–‰ì— í¬í•¨ëœ ëª¨ë“  ì—ì´ì „íŠ¸, ë„êµ¬ í•¨ìˆ˜, ë¼ì´í”„ì‚¬ì´í´ ë“±ì€ ë™ì¼í•œ ì»¨í…ìŠ¤íŠ¸ _type_ ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

ì»¨í…ìŠ¤íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìš©ë„ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

-   ì‹¤í–‰ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°(ì˜ˆ: ì‚¬ìš©ì ì´ë¦„/uid ë˜ëŠ” ì‚¬ìš©ìì— ëŒ€í•œ ê¸°íƒ€ ì •ë³´)
-   ì˜ì¡´ì„±(ì˜ˆ: ë¡œê±° ê°ì²´, ë°ì´í„° íŒ¨ì²˜ ë“±)
-   í—¬í¼ í•¨ìˆ˜

!!! danger "ì£¼ì˜"

    ì»¨í…ìŠ¤íŠ¸ ê°ì²´ëŠ” LLMìœ¼ë¡œ **ì „ì†¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤**. ì½ê³  ì“°ê³  ë©”ì„œë“œë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ìˆœìˆ˜ ë¡œì»¬ ê°ì²´ì…ë‹ˆë‹¤.

```python
import asyncio
from dataclasses import dataclass

from agents import Agent, RunContextWrapper, Runner, function_tool

@dataclass
class UserInfo:  # (1)!
    name: str
    uid: int

@function_tool
async def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str:  # (2)!
    """Fetch the age of the user. Call this function to get user's age information."""
    return f"The user {wrapper.context.name} is 47 years old"

async def main():
    user_info = UserInfo(name="John", uid=123)

    agent = Agent[UserInfo](  # (3)!
        name="Assistant",
        tools=[fetch_user_age],
    )

    result = await Runner.run(  # (4)!
        starting_agent=agent,
        input="What is the age of the user?",
        context=user_info,
    )

    print(result.final_output)  # (5)!
    # The user John is 47 years old.

if __name__ == "__main__":
    asyncio.run(main())
```

1. ì´ê²ƒì´ ì»¨í…ìŠ¤íŠ¸ ê°ì²´ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” dataclassë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, ì–´ë–¤ íƒ€ì…ì´ë“  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
2. ì´ê²ƒì€ ë„êµ¬ì…ë‹ˆë‹¤. `RunContextWrapper[UserInfo]` ë¥¼ ë°›ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë„êµ¬ êµ¬í˜„ì€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì½ìŠµë‹ˆë‹¤
3. ì—ì´ì „íŠ¸ë¥¼ ì œë„¤ë¦­ `UserInfo` ë¡œ í‘œì‹œí•˜ì—¬, íƒ€ì…ì²´ì»¤ê°€ ì˜¤ë¥˜ë¥¼ ì¡ì„ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤(ì˜ˆ: ë‹¤ë¥¸ ì»¨í…ìŠ¤íŠ¸ íƒ€ì…ì„ ë°›ëŠ” ë„êµ¬ë¥¼ ì „ë‹¬í•˜ë ¤ê³  í•  ë•Œ)
4. ì»¨í…ìŠ¤íŠ¸ê°€ `run` í•¨ìˆ˜ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤
5. ì—ì´ì „íŠ¸ê°€ ë„êµ¬ë¥¼ ì˜¬ë°”ë¥´ê²Œ í˜¸ì¶œí•˜ê³  ë‚˜ì´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤

---

### ê³ ê¸‰: `ToolContext`

ì¼ë¶€ ê²½ìš°, ì‹¤í–‰ ì¤‘ì¸ ë„êµ¬ì˜ ì¶”ê°€ ë©”íƒ€ë°ì´í„°(ì˜ˆ: ì´ë¦„, í˜¸ì¶œ ID, ì›ë¬¸ ì¸ì ë¬¸ìì—´)ì— ì ‘ê·¼í•˜ê³  ì‹¶ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤  
ì´ë¥¼ ìœ„í•´ `RunContextWrapper` ë¥¼ í™•ì¥í•œ [`ToolContext`][agents.tool_context.ToolContext] í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from typing import Annotated
from pydantic import BaseModel, Field
from agents import Agent, Runner, function_tool
from agents.tool_context import ToolContext

class WeatherContext(BaseModel):
    user_id: str

class Weather(BaseModel):
    city: str = Field(description="The city name")
    temperature_range: str = Field(description="The temperature range in Celsius")
    conditions: str = Field(description="The weather conditions")

@function_tool
def get_weather(ctx: ToolContext[WeatherContext], city: Annotated[str, "The city to get the weather for"]) -> Weather:
    print(f"[debug] Tool context: (name: {ctx.tool_name}, call_id: {ctx.tool_call_id}, args: {ctx.tool_arguments})")
    return Weather(city=city, temperature_range="14-20C", conditions="Sunny with wind.")

agent = Agent(
    name="Weather Agent",
    instructions="You are a helpful agent that can tell the weather of a given city.",
    tools=[get_weather],
)
```

`ToolContext` ëŠ” `RunContextWrapper` ì™€ ë™ì¼í•œ `.context` ì†ì„±ì„ ì œê³µí•˜ë©°,  
í˜„ì¬ ë„êµ¬ í˜¸ì¶œì— íŠ¹í™”ëœ ì¶”ê°€ í•„ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤:

- `tool_name` â€“ í˜¸ì¶œë˜ëŠ” ë„êµ¬ì˜ ì´ë¦„  
- `tool_call_id` â€“ ì´ ë„êµ¬ í˜¸ì¶œì˜ ê³ ìœ  ì‹ë³„ì  
- `tool_arguments` â€“ ë„êµ¬ì— ì „ë‹¬ëœ ì¸ìì˜ ì›ë¬¸ ë¬¸ìì—´  

ì‹¤í–‰ ì¤‘ ë„êµ¬ ë ˆë²¨ ë©”íƒ€ë°ì´í„°ê°€ í•„ìš”í•  ë•Œ `ToolContext` ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.  
ì—ì´ì „íŠ¸ì™€ ë„êµ¬ ê°„ì˜ ì¼ë°˜ì ì¸ ì»¨í…ìŠ¤íŠ¸ ê³µìœ ì—ëŠ” `RunContextWrapper` ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.

---

## ì—ì´ì „íŠ¸/LLM ì»¨í…ìŠ¤íŠ¸

LLMì´ í˜¸ì¶œë  ë•Œ, ë³¼ ìˆ˜ ìˆëŠ” ë°ì´í„°ëŠ” ëŒ€í™” ê¸°ë¡ì—ì„œë§Œ ë‚˜ì˜µë‹ˆë‹¤. ì¦‰, LLMì´ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•˜ë ¤ë©´, ê·¸ ë°ì´í„°ê°€ ëŒ€í™” ê¸°ë¡ì— í¬í•¨ë˜ë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤. ë°©ë²•ì€ ëª‡ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤:

1. ì—ì´ì „íŠ¸ì˜ `instructions` ì— ì¶”ê°€í•©ë‹ˆë‹¤. ì´ëŠ” "system prompt" ë˜ëŠ” "developer message" ë¼ê³ ë„ í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” ê³ ì • ë¬¸ìì—´ì¼ ìˆ˜ë„ ìˆê³ , ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë¬¸ìì—´ì„ ì¶œë ¥í•˜ëŠ” ë™ì  í•¨ìˆ˜ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì ì´ë¦„ì´ë‚˜ í˜„ì¬ ë‚ ì§œì²˜ëŸ¼ í•­ìƒ ìœ ìš©í•œ ì •ë³´ì— í”íˆ ì“°ëŠ” ë°©ë²•ì…ë‹ˆë‹¤
2. `Runner.run` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ë•Œ `input` ì— ì¶”ê°€í•©ë‹ˆë‹¤. ì´ëŠ” `instructions` ë°©ì‹ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, [ì§€íœ˜ ì²´ê³„](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command)ì—ì„œ ë” ë‚®ì€ ìš°ì„ ìˆœìœ„ì˜ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
3. í•¨ìˆ˜ ë„êµ¬ë¥¼ í†µí•´ ë…¸ì¶œí•©ë‹ˆë‹¤. ì´ëŠ” _on-demand_ ì»¨í…ìŠ¤íŠ¸ì— ìœ ìš©í•©ë‹ˆë‹¤. LLMì´ í•„ìš”í•œ ì‹œì ì„ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ê³ , í•´ë‹¹ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ë„êµ¬ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
4. íŒŒì¼ ê²€ìƒ‰ ë˜ëŠ” ì›¹ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” íŒŒì¼ì´ë‚˜ ë°ì´í„°ë² ì´ìŠ¤(íŒŒì¼ ê²€ìƒ‰) ë˜ëŠ” ì›¹(ì›¹ ê²€ìƒ‰)ì—ì„œ ê´€ë ¨ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” íŠ¹ìˆ˜ ë„êµ¬ì…ë‹ˆë‹¤. ì´ëŠ” ì‘ë‹µì„ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ì— "ê·¸ë¼ìš´ë”©"í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤

================
File: docs/ko/examples.md
================
---
search:
  exclude: true
---
# ì½”ë“œ ì˜ˆì œ

[repo](https://github.com/openai/openai-agents-python/tree/main/examples)ì˜ examples ì„¹ì…˜ì—ì„œ SDKì˜ ë‹¤ì–‘í•œ ìƒ˜í”Œ êµ¬í˜„ì„ í™•ì¸í•˜ì„¸ìš”. ì½”ë“œ ì˜ˆì œëŠ” ë‹¤ì–‘í•œ íŒ¨í„´ê³¼ ê¸°ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ì¹´í…Œê³ ë¦¬

-   **[agent_patterns](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns):**
    ì´ ì¹´í…Œê³ ë¦¬ì˜ ì½”ë“œ ì˜ˆì œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¼ë°˜ì ì¸ ì—ì´ì „íŠ¸ ì„¤ê³„ íŒ¨í„´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤

    -   ê²°ì •ì  ì›Œí¬í”Œë¡œ
    -   ë„êµ¬ë¡œì„œì˜ ì—ì´ì „íŠ¸
    -   ì—ì´ì „íŠ¸ ë³‘ë ¬ ì‹¤í–‰
    -   ì¡°ê±´ë¶€ ë„êµ¬ ì‚¬ìš©
    -   ì…ë ¥/ì¶œë ¥ ê°€ë“œë ˆì¼
    -   íŒì‚¬ë¡œì„œì˜ LLM
    -   ë¼ìš°íŒ…
    -   ìŠ¤íŠ¸ë¦¬ë° ê°€ë“œë ˆì¼

-   **[basic](https://github.com/openai/openai-agents-python/tree/main/examples/basic):**
    ì´ ì½”ë“œ ì˜ˆì œëŠ” ë‹¤ìŒê³¼ ê°™ì€ SDKì˜ ê¸°ë³¸ ê¸°ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤

    -   Hello World ì½”ë“œ ì˜ˆì œ (ê¸°ë³¸ ëª¨ë¸, GPT-5, ì˜¤í”ˆ ì›¨ì´íŠ¸ ëª¨ë¸)
    -   ì—ì´ì „íŠ¸ ìˆ˜ëª…ì£¼ê¸° ê´€ë¦¬
    -   ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    -   ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ (í…ìŠ¤íŠ¸, ì•„ì´í…œ, í•¨ìˆ˜ í˜¸ì¶œ args)
    -   í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    -   íŒŒì¼ ì²˜ë¦¬ (ë¡œì»¬/ì›ê²©, ì´ë¯¸ì§€/PDF)
    -   ì‚¬ìš©ëŸ‰ ì¶”ì 
    -   ë¹„ì—„ê²© ì¶œë ¥ íƒ€ì…
    -   ì´ì „ ì‘ë‹µ ID ì‚¬ìš©

-   **[customer_service](https://github.com/openai/openai-agents-python/tree/main/examples/customer_service):**
    í•­ê³µì‚¬ë¥¼ ìœ„í•œ ê³ ê° ì„œë¹„ìŠ¤ ì‹œìŠ¤í…œ ì½”ë“œ ì˜ˆì œ

-   **[financial_research_agent](https://github.com/openai/openai-agents-python/tree/main/examples/financial_research_agent):**
    ê¸ˆìœµ ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ ì—ì´ì „íŠ¸ì™€ ë„êµ¬ë¡œ êµ¬ì¡°í™”ëœ ë¦¬ì„œì¹˜ ì›Œí¬í”Œë¡œë¥¼ ì‹œì—°í•˜ëŠ” ê¸ˆìœµ ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸

-   **[handoffs](https://github.com/openai/openai-agents-python/tree/main/examples/handoffs):**
    ë©”ì‹œì§€ í•„í„°ë§ì„ í™œìš©í•œ ì—ì´ì „íŠ¸ í•¸ë“œì˜¤í”„ì˜ ì‹¤ìš©ì ì¸ ì½”ë“œ ì˜ˆì œ

-   **[hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp):**
    í˜¸ìŠ¤í‹°ë“œ MCP (Model Context Protocol) ì»¤ë„¥í„°ì™€ ìŠ¹ì¸ ì‚¬ìš©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ ì˜ˆì œ

-   **[mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp):**
    MCP (Model Context Protocol)ë¡œ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ì„¸ìš”. ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤

    -   íŒŒì¼ì‹œìŠ¤í…œ ì½”ë“œ ì˜ˆì œ
    -   Git ì½”ë“œ ì˜ˆì œ
    -   MCP í”„ë¡¬í”„íŠ¸ ì„œë²„ ì½”ë“œ ì˜ˆì œ
    -   SSE (Server-Sent Events) ì½”ë“œ ì˜ˆì œ
    -   ìŠ¤íŠ¸ë¦¬ë¨¸ë¸” HTTP ì½”ë“œ ì˜ˆì œ

-   **[memory](https://github.com/openai/openai-agents-python/tree/main/examples/memory):**
    ì—ì´ì „íŠ¸ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ë©”ëª¨ë¦¬ êµ¬í˜„ ì½”ë“œ ì˜ˆì œ, ë‹¤ìŒ í¬í•¨

    -   SQLite ì„¸ì…˜ ì €ì¥ì†Œ
    -   ê³ ê¸‰ SQLite ì„¸ì…˜ ì €ì¥ì†Œ
    -   Redis ì„¸ì…˜ ì €ì¥ì†Œ
    -   SQLAlchemy ì„¸ì…˜ ì €ì¥ì†Œ
    -   ì•”í˜¸í™”ëœ ì„¸ì…˜ ì €ì¥ì†Œ
    -   OpenAI ì„¸ì…˜ ì €ì¥ì†Œ

-   **[model_providers](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers):**
    ì»¤ìŠ¤í…€ ì œê³µìì™€ LiteLLM í†µí•©ì„ í¬í•¨í•˜ì—¬ SDKë¡œ OpenAIê°€ ì•„ë‹Œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²• íƒìƒ‰

-   **[realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime):**
    SDKë¥¼ ì‚¬ìš©í•´ ì‹¤ì‹œê°„ ê²½í—˜ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ ì˜ˆì œ, ë‹¤ìŒ í¬í•¨

    -   ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜
    -   ì»¤ë§¨ë“œë¼ì¸ ì¸í„°í˜ì´ìŠ¤
    -   Twilio í†µí•©

-   **[reasoning_content](https://github.com/openai/openai-agents-python/tree/main/examples/reasoning_content):**
    ì¶”ë¡  ì½˜í…ì¸ ì™€ structured outputsì„ ë‹¤ë£¨ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ ì˜ˆì œ

-   **[research_bot](https://github.com/openai/openai-agents-python/tree/main/examples/research_bot):**
    ë³µì¡í•œ ë©€í‹° ì—ì´ì „íŠ¸ ë¦¬ì„œì¹˜ ì›Œí¬í”Œë¡œë¥¼ ì‹œì—°í•˜ëŠ” ê°„ë‹¨í•œ ë”¥ ë¦¬ì„œì¹˜ í´ë¡ 

-   **[tools](https://github.com/openai/openai-agents-python/tree/main/examples/tools):**
    ë‹¤ìŒê³¼ ê°™ì€ OpenAI í˜¸ìŠ¤íŠ¸í•˜ëŠ” ë„êµ¬ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ì„¸ìš”

    -   ì›¹ ê²€ìƒ‰ ë° í•„í„°ê°€ ìˆëŠ” ì›¹ ê²€ìƒ‰
    -   íŒŒì¼ ê²€ìƒ‰
    -   Code interpreter
    -   ì»´í“¨í„° ì‚¬ìš©
    -   ì´ë¯¸ì§€ ìƒì„±

-   **[voice](https://github.com/openai/openai-agents-python/tree/main/examples/voice):**
    TTSì™€ STT ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë³´ì´ìŠ¤ ì—ì´ì „íŠ¸ ì½”ë“œ ì˜ˆì œ, ìŠ¤íŠ¸ë¦¬ë° ìŒì„± ì½”ë“œ ì˜ˆì œ í¬í•¨

================
File: docs/ko/guardrails.md
================
---
search:
  exclude: true
---
# ê°€ë“œë ˆì¼

ê°€ë“œë ˆì¼ì€ ì—ì´ì „íŠ¸ì™€ ë³‘ë ¬ë¡œ ì‹¤í–‰ë˜ì–´ ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ê²€ì‚¬ì™€ ê²€ì¦ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ê° ìš”ì²­ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë§¤ìš° ë˜‘ë˜‘í•œ(ë”°ë¼ì„œ ëŠë¦¬ê³  ë¹„ìš©ì´ í°) ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì—ì´ì „íŠ¸ê°€ ìˆë‹¤ê³  ê°€ì •í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì•…ì˜ì ì¸ ì‚¬ìš©ìê°€ ìˆ˜í•™ ìˆ™ì œë¥¼ ë„ì™€ ë‹¬ë¼ê³  ëª¨ë¸ì— ìš”ì²­í•˜ëŠ” ê²ƒì€ ì›ì¹˜ ì•Šì„ ê²ƒì…ë‹ˆë‹¤. ì´ë•Œ ë¹ ë¥´ê³  ì €ë ´í•œ ëª¨ë¸ë¡œ ê°€ë“œë ˆì¼ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ë“œë ˆì¼ì´ ì•…ì˜ì ì¸ ì‚¬ìš©ì„ ê°ì§€í•˜ë©´ ì¦‰ì‹œ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œì¼œ ë¹„ìš©ì´ í° ëª¨ë¸ì˜ ì‹¤í–‰ì„ ì¤‘ë‹¨í•˜ê³  ì‹œê°„/ë¹„ìš©ì„ ì ˆì•½í•©ë‹ˆë‹¤.

ê°€ë“œë ˆì¼ì—ëŠ” ë‘ ê°€ì§€ ì¢…ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤:

1. ì…ë ¥ ê°€ë“œë ˆì¼ì€ ì´ˆê¸° ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì‹¤í–‰ë¨
2. ì¶œë ¥ ê°€ë“œë ˆì¼ì€ ìµœì¢… ì—ì´ì „íŠ¸ ì¶œë ¥ì—ì„œ ì‹¤í–‰ë¨

## ì…ë ¥ ê°€ë“œë ˆì¼

ì…ë ¥ ê°€ë“œë ˆì¼ì€ 3ë‹¨ê³„ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤:

1. ë¨¼ì €, ê°€ë“œë ˆì¼ì€ ì—ì´ì „íŠ¸ì— ì „ë‹¬ëœ ê²ƒê³¼ ë™ì¼í•œ ì…ë ¥ì„ ë°›ìŠµë‹ˆë‹¤.
2. ë‹¤ìŒìœ¼ë¡œ, ê°€ë“œë ˆì¼ í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ì–´ [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]ì„ ìƒì„±í•˜ê³ , ì´ë¥¼ [`InputGuardrailResult`][agents.guardrail.InputGuardrailResult]ë¡œ ë˜í•‘í•©ë‹ˆë‹¤
3. ë§ˆì§€ë§‰ìœ¼ë¡œ [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered]ê°€ trueì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤. trueì´ë©´ [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered] ì˜ˆì™¸ê°€ ë°œìƒí•˜ë©°, ì´ì— ì ì ˆíˆ ì‚¬ìš©ìì—ê²Œ ì‘ë‹µí•˜ê±°ë‚˜ ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

!!! Note

    ì…ë ¥ ê°€ë“œë ˆì¼ì€ ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë¯€ë¡œ, ì—ì´ì „íŠ¸ì˜ ê°€ë“œë ˆì¼ì€ í•´ë‹¹ ì—ì´ì „íŠ¸ê°€ ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ì¼ ë•Œë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤. ì™œ `guardrails` ì†ì„±ì´ ì—ì´ì „íŠ¸ì— ìˆê³  `Runner.run`ì— ì „ë‹¬ë˜ì§€ ì•ŠëŠ”ì§€ ê¶ê¸ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ë“œë ˆì¼ì€ ì‹¤ì œ ì—ì´ì „íŠ¸ì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ëŠ” ê²½í–¥ì´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì—ì´ì „íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ ê°€ë“œë ˆì¼ì„ ì‹¤í–‰í•˜ë¯€ë¡œ ì½”ë“œë¥¼ ê°™ì€ ìœ„ì¹˜ì— ë‘ë©´ ê°€ë…ì„±ì— ë„ì›€ì´ ë©ë‹ˆë‹¤.

## ì¶œë ¥ ê°€ë“œë ˆì¼

ì¶œë ¥ ê°€ë“œë ˆì¼ì€ 3ë‹¨ê³„ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤:

1. ë¨¼ì €, ê°€ë“œë ˆì¼ì€ ì—ì´ì „íŠ¸ê°€ ìƒì„±í•œ ì¶œë ¥ì„ ë°›ìŠµë‹ˆë‹¤.
2. ë‹¤ìŒìœ¼ë¡œ, ê°€ë“œë ˆì¼ í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ì–´ [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]ì„ ìƒì„±í•˜ê³ , ì´ë¥¼ [`OutputGuardrailResult`][agents.guardrail.OutputGuardrailResult]ë¡œ ë˜í•‘í•©ë‹ˆë‹¤
3. ë§ˆì§€ë§‰ìœ¼ë¡œ [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered]ê°€ trueì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤. trueì´ë©´ [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered] ì˜ˆì™¸ê°€ ë°œìƒí•˜ë©°, ì´ì— ì ì ˆíˆ ì‚¬ìš©ìì—ê²Œ ì‘ë‹µí•˜ê±°ë‚˜ ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

!!! Note

    ì¶œë ¥ ê°€ë“œë ˆì¼ì€ ìµœì¢… ì—ì´ì „íŠ¸ ì¶œë ¥ì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë¯€ë¡œ, ì—ì´ì „íŠ¸ì˜ ê°€ë“œë ˆì¼ì€ í•´ë‹¹ ì—ì´ì „íŠ¸ê°€ ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ì¼ ë•Œë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤. ì…ë ¥ ê°€ë“œë ˆì¼ê³¼ ìœ ì‚¬í•˜ê²Œ, ê°€ë“œë ˆì¼ì€ ì‹¤ì œ ì—ì´ì „íŠ¸ì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ëŠ” ê²½í–¥ì´ ìˆì–´ ì—ì´ì „íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ ê°€ë“œë ˆì¼ì„ ì‹¤í–‰í•˜ê²Œ ë˜ë¯€ë¡œ, ì½”ë“œë¥¼ ê°™ì€ ìœ„ì¹˜ì— ë‘ë©´ ê°€ë…ì„±ì— ë„ì›€ì´ ë©ë‹ˆë‹¤.

## íŠ¸ë¦½ì™€ì´ì–´(Tripwires)

ì…ë ¥ ë˜ëŠ” ì¶œë ¥ì´ ê°€ë“œë ˆì¼ì„ í†µê³¼í•˜ì§€ ëª»í•˜ë©´, ê°€ë“œë ˆì¼ì€ íŠ¸ë¦½ì™€ì´ì–´ë¡œ ì´ë¥¼ ì‹ í˜¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¸ë¦½ì™€ì´ì–´ê°€ íŠ¸ë¦¬ê±°ëœ ê°€ë“œë ˆì¼ì„ í™•ì¸í•˜ëŠ” ì¦‰ì‹œ `{Input,Output}GuardrailTripwireTriggered` ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ê³  ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.

## ê°€ë“œë ˆì¼ êµ¬í˜„

ì…ë ¥ì„ ë°›ì•„ [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. ì´ ì˜ˆì‹œì—ì„œëŠ”, ë‚´ë¶€ì ìœ¼ë¡œ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ì´ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    InputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    TResponseInputItem,
    input_guardrail,
)

class MathHomeworkOutput(BaseModel):
    is_math_homework: bool
    reasoning: str

guardrail_agent = Agent( # (1)!
    name="Guardrail check",
    instructions="Check if the user is asking you to do their math homework.",
    output_type=MathHomeworkOutput,
)


@input_guardrail
async def math_guardrail( # (2)!
    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, input, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output, # (3)!
        tripwire_triggered=result.final_output.is_math_homework,
    )


agent = Agent(  # (4)!
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    input_guardrails=[math_guardrail],
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except InputGuardrailTripwireTriggered:
        print("Math homework guardrail tripped")
```

1. ì´ ì—ì´ì „íŠ¸ë¥¼ ê°€ë“œë ˆì¼ í•¨ìˆ˜ì—ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤
2. ì´ëŠ” ì—ì´ì „íŠ¸ì˜ ì…ë ¥/ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ê°€ë“œë ˆì¼ í•¨ìˆ˜ì…ë‹ˆë‹¤
3. ê°€ë“œë ˆì¼ ê²°ê³¼ì— ì¶”ê°€ ì •ë³´ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
4. ì›Œí¬í”Œë¡œë¥¼ ì •ì˜í•˜ëŠ” ì‹¤ì œ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤

ì¶œë ¥ ê°€ë“œë ˆì¼ë„ ìœ ì‚¬í•©ë‹ˆë‹¤.

```python
from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    OutputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    output_guardrail,
)
class MessageOutput(BaseModel): # (1)!
    response: str

class MathOutput(BaseModel): # (2)!
    reasoning: str
    is_math: bool

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the output includes any math.",
    output_type=MathOutput,
)

@output_guardrail
async def math_guardrail(  # (3)!
    ctx: RunContextWrapper, agent: Agent, output: MessageOutput
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, output.response, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output,
        tripwire_triggered=result.final_output.is_math,
    )

agent = Agent( # (4)!
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    output_guardrails=[math_guardrail],
    output_type=MessageOutput,
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except OutputGuardrailTripwireTriggered:
        print("Math output guardrail tripped")
```

1. ì´ê²ƒì€ ì‹¤ì œ ì—ì´ì „íŠ¸ì˜ ì¶œë ¥ íƒ€ì…ì…ë‹ˆë‹¤
2. ì´ê²ƒì€ ê°€ë“œë ˆì¼ì˜ ì¶œë ¥ íƒ€ì…ì…ë‹ˆë‹¤
3. ì´ê²ƒì€ ì—ì´ì „íŠ¸ì˜ ì¶œë ¥ì„ ë°›ì•„ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ê°€ë“œë ˆì¼ í•¨ìˆ˜ì…ë‹ˆë‹¤
4. ì›Œí¬í”Œë¡œë¥¼ ì •ì˜í•˜ëŠ” ì‹¤ì œ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤

================
File: docs/ko/handoffs.md
================
---
search:
  exclude: true
---
# í•¸ë“œì˜¤í”„

í•¸ë“œì˜¤í”„ëŠ” í•œ ì—ì´ì „íŠ¸ê°€ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë¶„ì•¼ì— íŠ¹í™”ëœ ì—ì´ì „íŠ¸ë“¤ì´ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ê° ì§€ì› ì•±ì—ëŠ” ì£¼ë¬¸ ìƒíƒœ, í™˜ë¶ˆ, FAQ ë“±ê³¼ ê°™ì€ ì‘ì—…ì„ ê°ê° ì „ë‹´í•˜ëŠ” ì—ì´ì „íŠ¸ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•¸ë“œì˜¤í”„ëŠ” LLM ì—ê²Œ ë„êµ¬ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ `Refund Agent`ë¡œì˜ í•¸ë“œì˜¤í”„ê°€ ìˆë‹¤ë©´, í•´ë‹¹ ë„êµ¬ ì´ë¦„ì€ `transfer_to_refund_agent`ê°€ ë©ë‹ˆë‹¤.

## í•¸ë“œì˜¤í”„ ìƒì„±

ëª¨ë“  ì—ì´ì „íŠ¸ì—ëŠ” [`handoffs`][agents.agent.Agent.handoffs] ë§¤ê°œë³€ìˆ˜ê°€ ìˆìœ¼ë©°, `Agent`ë¥¼ ì§ì ‘ ì „ë‹¬í•˜ê±°ë‚˜, í•¸ë“œì˜¤í”„ë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•˜ëŠ” `Handoff` ê°ì²´ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Agents SDKì—ì„œ ì œê³µí•˜ëŠ” [`handoff()`][agents.handoffs.handoff] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í•¸ë“œì˜¤í”„ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” í•¸ë“œì˜¤í”„ ëŒ€ìƒ ì—ì´ì „íŠ¸ì™€ í•¨ê»˜ ì„ íƒì ì¸ override ë° ì…ë ¥ í•„í„°ë¥¼ ì§€ì •í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

### ê¸°ë³¸ ì‚¬ìš©ë²•

ê°„ë‹¨í•œ í•¸ë“œì˜¤í”„ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```python
from agents import Agent, handoff

billing_agent = Agent(name="Billing agent")
refund_agent = Agent(name="Refund agent")

# (1)!
triage_agent = Agent(name="Triage agent", handoffs=[billing_agent, handoff(refund_agent)])
```

1. ì—ì´ì „íŠ¸ë¥¼ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ë„ ìˆê³ (ì˜ˆ: `billing_agent`), `handoff()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

### `handoff()` í•¨ìˆ˜ë¥¼ í†µí•œ í•¸ë“œì˜¤í”„ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ

[`handoff()`][agents.handoffs.handoff] í•¨ìˆ˜ë¡œ ë‹¤ì–‘í•œ ì„¤ì •ì„ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- `agent`: ì‘ì—…ì„ ë„˜ê²¨ë°›ì„ ì—ì´ì „íŠ¸
- `tool_name_override`: ê¸°ë³¸ì ìœ¼ë¡œ `Handoff.default_tool_name()` í•¨ìˆ˜ê°€ ì‚¬ìš©ë˜ë©°, ì´ëŠ” `transfer_to_<agent_name>`ìœ¼ë¡œ ê²°ì •ë©ë‹ˆë‹¤. ì´ë¥¼ ì˜¤ë²„ë¼ì´ë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- `tool_description_override`: `Handoff.default_tool_description()`ì—ì„œ ì œê³µë˜ëŠ” ê¸°ë³¸ ë„êµ¬ ì„¤ëª…ì„ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤.
- `on_handoff`: í•¸ë“œì˜¤í”„ê°€ í˜¸ì¶œë  ë•Œ ì‹¤í–‰ë˜ëŠ” ì½œë°± í•¨ìˆ˜ì…ë‹ˆë‹¤. í•¸ë“œì˜¤í”„ê°€ í˜¸ì¶œë˜ëŠ” ì¦‰ì‹œ ë°ì´í„° í˜ì¹­ì„ ì‹œì‘í•˜ëŠ” ë“±ì˜ ìš©ë„ë¡œ ìœ ìš©í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì—ì´ì „íŠ¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬ë°›ìœ¼ë©°, ì„ íƒì ìœ¼ë¡œ LLM ì´ ìƒì„±í•œ ì…ë ¥ë„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì…ë ¥ ë°ì´í„°ëŠ” `input_type` ë§¤ê°œë³€ìˆ˜ë¡œ ì œì–´ë©ë‹ˆë‹¤.
- `input_type`: í•¸ë“œì˜¤í”„ê°€ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ì˜ íƒ€ì…(ì„ íƒ ì‚¬í•­)
- `input_filter`: ë‹¤ìŒ ì—ì´ì „íŠ¸ê°€ ë°›ê²Œ ë  ì…ë ¥ì„ í•„í„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ì„¤ëª…ì„ ì°¸ê³ í•˜ì„¸ìš”.
- `is_enabled`: í•¸ë“œì˜¤í”„ í™œì„±í™” ì—¬ë¶€ì…ë‹ˆë‹¤. ë¶ˆë¦¬ì–¸ì´ê±°ë‚˜ ë¶ˆë¦¬ì–¸ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ê°€ ë  ìˆ˜ ìˆìœ¼ë©°, ëŸ°íƒ€ì„ì—ì„œ ë™ì ìœ¼ë¡œ í•¸ë“œì˜¤í”„ë¥¼ í™œì„±í™”/ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from agents import Agent, handoff, RunContextWrapper

def on_handoff(ctx: RunContextWrapper[None]):
    print("Handoff called")

agent = Agent(name="My agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    tool_name_override="custom_handoff_tool",
    tool_description_override="Custom description",
)
```

## í•¸ë“œì˜¤í”„ ì…ë ¥

íŠ¹ì • ìƒí™©ì—ì„œëŠ”, LLM ì´ í•¸ë“œì˜¤í”„ë¥¼ í˜¸ì¶œí•  ë•Œ ì¼ë¶€ ë°ì´í„°ë¥¼ ì œê³µí•˜ê¸¸ ì›í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "Escalation agent"ë¡œì˜ í•¸ë“œì˜¤í”„ë¥¼ ìƒìƒí•´ ë³´ì„¸ìš”. ë¡œê·¸ë¥¼ ìœ„í•´ ì‚¬ìœ ë¥¼ í•¨ê»˜ ì œê³µë°›ê³  ì‹¶ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from pydantic import BaseModel

from agents import Agent, handoff, RunContextWrapper

class EscalationData(BaseModel):
    reason: str

async def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):
    print(f"Escalation agent called with reason: {input_data.reason}")

agent = Agent(name="Escalation agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    input_type=EscalationData,
)
```

## ì…ë ¥ í•„í„°

í•¸ë“œì˜¤í”„ê°€ ë°œìƒí•˜ë©´, ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ê°€ ëŒ€í™”ë¥¼ ì¸ê³„ë°›ì•„ ì´ì „ ëŒ€í™” ê¸°ë¡ ì „ì²´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ì´ë¥¼ ë³€ê²½í•˜ê³  ì‹¶ë‹¤ë©´ [`input_filter`][agents.handoffs.Handoff.input_filter]ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì…ë ¥ í•„í„°ëŠ” ê¸°ì¡´ ì…ë ¥ì„ [`HandoffInputData`][agents.handoffs.HandoffInputData]ë¡œ ë°›ì•„, ìƒˆë¡œìš´ `HandoffInputData`ë¥¼ ë°˜í™˜í•´ì•¼ í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

ëª‡ ê°€ì§€ ì¼ë°˜ì ì¸ íŒ¨í„´(ì˜ˆ: ê¸°ë¡ì—ì„œ ëª¨ë“  ë„êµ¬ í˜¸ì¶œ ì œê±°)ì€ [`agents.extensions.handoff_filters`][]ì— ì´ë¯¸ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

```python
from agents import Agent, handoff
from agents.extensions import handoff_filters

agent = Agent(name="FAQ agent")

handoff_obj = handoff(
    agent=agent,
    input_filter=handoff_filters.remove_all_tools, # (1)!
)
```

1. ì´ëŠ” `FAQ agent`ê°€ í˜¸ì¶œë  ë•Œ ê¸°ë¡ì—ì„œ ëª¨ë“  ë„êµ¬ë¥¼ ìë™ìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤.

## ê¶Œì¥ í”„ë¡¬í”„íŠ¸

LLM ì´ í•¸ë“œì˜¤í”„ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì´í•´í•˜ë„ë¡, ì—ì´ì „íŠ¸ì— í•¸ë“œì˜¤í”„ ê´€ë ¨ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. [`agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX`][]ì— ê¶Œì¥ ì ‘ë‘ì‚¬ê°€ ì¤€ë¹„ë˜ì–´ ìˆìœ¼ë©°, ë˜ëŠ” [`agents.extensions.handoff_prompt.prompt_with_handoff_instructions`][]ë¥¼ í˜¸ì¶œí•´ í”„ë¡¬í”„íŠ¸ì— ê¶Œì¥ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from agents import Agent
from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX

billing_agent = Agent(
    name="Billing agent",
    instructions=f"""{RECOMMENDED_PROMPT_PREFIX}
    <Fill in the rest of your prompt here>.""",
)
```

================
File: docs/ko/index.md
================
---
search:
  exclude: true
---
# OpenAI Agents SDK

[OpenAI Agents SDK](https://github.com/openai/openai-agents-python)ëŠ” ìµœì†Œí•œì˜ ì¶”ìƒí™”ë¡œ ê°€ë³ê³  ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ íŒ¨í‚¤ì§€ì—ì„œ ì—ì´ì „íŠ¸ ê¸°ë°˜ AI ì•±ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ëŠ” ì´ì „ ì—ì´ì „íŠ¸ ì‹¤í—˜ í”„ë¡œì íŠ¸ì¸ [Swarm](https://github.com/openai/swarm/tree/main)ì˜ í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ ì—…ê·¸ë ˆì´ë“œì…ë‹ˆë‹¤. Agents SDKì—ëŠ” ë§¤ìš° ì†Œìˆ˜ì˜ ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ê°€ ìˆìŠµë‹ˆë‹¤:

-   **ì—ì´ì „íŠ¸**: instructionsì™€ ë„êµ¬ë¥¼ ê°–ì¶˜ LLM
-   **í•¸ë“œì˜¤í”„**: íŠ¹ì • ì‘ì—…ì— ëŒ€í•´ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì— ìœ„ì„í•  ìˆ˜ ìˆê²Œ í•¨
-   **ê°€ë“œë ˆì¼**: ì—ì´ì „íŠ¸ ì…ë ¥ê³¼ ì¶œë ¥ì˜ ìœ íš¨ì„± ê²€ì¦ì„ ê°€ëŠ¥í•˜ê²Œ í•¨
-   **ì„¸ì…˜**: ì—ì´ì „íŠ¸ ì‹¤í–‰ ì „ë°˜ì— ê±¸ì³ ëŒ€í™” ì´ë ¥ì„ ìë™ìœ¼ë¡œ ìœ ì§€ ê´€ë¦¬í•¨

íŒŒì´ì¬ê³¼ ê²°í•©í•˜ë©´, ì´ëŸ¬í•œ ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ë§Œìœ¼ë¡œë„ ë„êµ¬ì™€ ì—ì´ì „íŠ¸ ê°„ì˜ ë³µì¡í•œ ê´€ê³„ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìœ¼ë©°, ê°€íŒŒë¥¸ í•™ìŠµ ê³¡ì„  ì—†ì´ ì‹¤ì œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ SDKì—ëŠ” ì—ì´ì „íŠ¸ í”Œë¡œìš°ë¥¼ ì‹œê°í™”í•˜ê³  ë””ë²„ê·¸í•˜ë©°, í‰ê°€í•˜ê³  ì‹¬ì§€ì–´ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ë§ê²Œ ëª¨ë¸ì„ ë¯¸ì„¸ íŠœë‹í•  ìˆ˜ ìˆëŠ” ë‚´ì¥ **íŠ¸ë ˆì´ì‹±**ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## Why use the Agents SDK

SDKì˜ ë‘ ê°€ì§€ í•µì‹¬ ì„¤ê³„ ì›ì¹™ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. ì‚¬ìš©í•  ê°€ì¹˜ê°€ ìˆì„ ë§Œí¼ ì¶©ë¶„í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ë˜, ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ëŠ” ìµœì†Œí™”í•©ë‹ˆë‹¤
2. ê¸°ë³¸ ì„¤ì •ë§Œìœ¼ë¡œë„ ì˜ ë™ì‘í•˜ì§€ë§Œ, ë™ì‘ì„ ì •í™•íˆ ì›í•˜ëŠ” ëŒ€ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

SDKì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

-   ì—ì´ì „íŠ¸ ë£¨í”„: ë„êµ¬ í˜¸ì¶œ, ê²°ê³¼ë¥¼ LLMì— ì „ë‹¬, LLM ì¢…ë£Œ ì‹œì ê¹Œì§€ì˜ ë£¨í”„ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë‚´ì¥ ì—ì´ì „íŠ¸ ë£¨í”„
-   íŒŒì´ì¬ ìš°ì„ : ìƒˆë¡œìš´ ì¶”ìƒí™”ë¥¼ í•™ìŠµí•  í•„ìš” ì—†ì´, ì–¸ì–´ì˜ ê¸°ë³¸ ê¸°ëŠ¥ìœ¼ë¡œ ì—ì´ì „íŠ¸ë¥¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•˜ê³  ì²´ì´ë‹
-   í•¸ë“œì˜¤í”„: ì—¬ëŸ¬ ì—ì´ì „íŠ¸ ê°„ì˜ ì¡°ì •ê³¼ ìœ„ì„ì„ ìœ„í•œ ê°•ë ¥í•œ ê¸°ëŠ¥
-   ê°€ë“œë ˆì¼: ì—ì´ì „íŠ¸ì™€ ë³‘ë ¬ë¡œ ì…ë ¥ ê²€ì¦ê³¼ ì ê²€ì„ ìˆ˜í–‰í•˜ê³ , ì‹¤íŒ¨ ì‹œ ì¡°ê¸° ì¤‘ë‹¨
-   ì„¸ì…˜: ì—ì´ì „íŠ¸ ì‹¤í–‰ ì „ë°˜ì˜ ëŒ€í™” ì´ë ¥ì„ ìë™ìœ¼ë¡œ ê´€ë¦¬í•˜ì—¬ ìˆ˜ë™ ìƒíƒœ ê´€ë¦¬ë¥¼ ì œê±°
-   í•¨ìˆ˜ ë„êµ¬: ì–´ë–¤ íŒŒì´ì¬ í•¨ìˆ˜ë“  ìë™ ìŠ¤í‚¤ë§ˆ ìƒì„±ê³¼ Pydantic ê¸°ë°˜ ê²€ì¦ìœ¼ë¡œ ë„êµ¬ë¡œ ì „í™˜
-   íŠ¸ë ˆì´ì‹±: ì›Œí¬í”Œë¡œë¥¼ ì‹œê°í™”, ë””ë²„ê·¸, ëª¨ë‹ˆí„°ë§í•˜ê³  OpenAIì˜ í‰ê°€, íŒŒì¸íŠœë‹, ì¦ë¥˜ ë„êµ¬ë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ë‚´ì¥ íŠ¸ë ˆì´ì‹±

## Installation

```bash
pip install openai-agents
```

## Hello World ì˜ˆì œ

```python
from agents import Agent, Runner

agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)

# Code within the code,
# Functions calling themselves,
# Infinite loop's dance.
```

(_ì´ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ `OPENAI_API_KEY` í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤_)

```bash
export OPENAI_API_KEY=sk-...
```

================
File: docs/ko/mcp.md
================
---
search:
  exclude: true
---
# Model context protocol (MCP)

[Model context protocol](https://modelcontextprotocol.io/introduction) (MCP)ì€ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ë„êµ¬ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì–¸ì–´ ëª¨ë¸ì— ë…¸ì¶œí•˜ëŠ” ë°©ì‹ì„ í‘œì¤€í™”í•©ë‹ˆë‹¤. ê³µì‹ ë¬¸ì„œì—ì„œ ì¸ìš©:

> MCPëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì´ LLMì— ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ë°©ì‹ì„ í‘œì¤€í™”í•œ ì˜¤í”ˆ í”„ë¡œí† ì½œì…ë‹ˆë‹¤. MCPë¥¼ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ USB-C í¬íŠ¸ë¼ê³  ìƒê°í•´ ë³´ì„¸ìš”. USB-Cê°€ ë‹¤ì–‘í•œ ì£¼ë³€ê¸°ê¸°ì™€ ì•¡ì„¸ì„œë¦¬ì— ê¸°ê¸°ë¥¼ í‘œì¤€ ë°©ì‹ìœ¼ë¡œ ì—°ê²°í•´ ì£¼ë“¯, MCPëŠ” AI ëª¨ë¸ì„ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ ë„êµ¬ì— í‘œì¤€ ë°©ì‹ìœ¼ë¡œ ì—°ê²°í•´ ì¤ë‹ˆë‹¤.

Agents Python SDKëŠ” ì—¬ëŸ¬ MCP íŠ¸ëœìŠ¤í¬íŠ¸ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê¸°ì¡´ MCP ì„œë²„ë¥¼ ì¬ì‚¬ìš©í•˜ê±°ë‚˜ ì§ì ‘ êµ¬ì¶•í•˜ì—¬ íŒŒì¼ì‹œìŠ¤í…œ, HTTP, ì»¤ë„¥í„° ê¸°ë°˜ ë„êµ¬ë¥¼ ì—ì´ì „íŠ¸ì— ë…¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## MCP í†µí•© ì„ íƒ

MCP ì„œë²„ë¥¼ ì—ì´ì „íŠ¸ì— ì—°ê²°í•˜ê¸° ì „ì— ë„êµ¬ í˜¸ì¶œì´ ì–´ë””ì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼ í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ì–´ë–¤ íŠ¸ëœìŠ¤í¬íŠ¸ì— ì ‘ê·¼í•  ìˆ˜ ìˆëŠ”ì§€ ê²°ì •í•˜ì„¸ìš”. ì•„ë˜ ë§¤íŠ¸ë¦­ìŠ¤ëŠ” Python SDKê°€ ì§€ì›í•˜ëŠ” ì˜µì…˜ì„ ìš”ì•½í•©ë‹ˆë‹¤.

| í•„ìš”í•œ ë‚´ìš©                                                                            | ì¶”ì²œ ì˜µì…˜                                              |
| ------------------------------------------------------------------------------------ | ----------------------------------------------------- |
| ëª¨ë¸ì„ ëŒ€ì‹ í•´ OpenAIì˜ Responses APIê°€ ê³µê°œ ì ‘ê·¼ ê°€ëŠ¥í•œ MCP ì„œë²„ë¥¼ í˜¸ì¶œí•˜ë„ë¡ í•˜ê¸°          | **í˜¸ìŠ¤í‹°ë“œ MCP ì„œë²„ ë„êµ¬** via [`HostedMCPTool`][agents.tool.HostedMCPTool] |
| ë¡œì»¬ ë˜ëŠ” ì›ê²©ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ Streamable HTTP ì„œë²„ì— ì—°ê²°í•˜ê¸°                           | **Streamable HTTP MCP ì„œë²„** via [`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp] |
| Server-Sent Eventsë¥¼ ì‚¬ìš©í•˜ëŠ” HTTPë¥¼ êµ¬í˜„í•œ ì„œë²„ì™€ í†µì‹ í•˜ê¸°                             | **HTTP with SSE MCP ì„œë²„** via [`MCPServerSse`][agents.mcp.server.MCPServerSse] |
| ë¡œì»¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ê³  stdin/stdoutìœ¼ë¡œ í†µì‹ í•˜ê¸°                                      | **stdio MCP ì„œë²„** via [`MCPServerStdio`][agents.mcp.server.MCPServerStdio] |

ì•„ë˜ ì„¹ì…˜ì—ì„œëŠ” ê° ì˜µì…˜ì„ ì„¤ì •í•˜ëŠ” ë°©ë²•ê³¼ ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–¤ íŠ¸ëœìŠ¤í¬íŠ¸ë¥¼ ì„ íƒí•˜ë©´ ì¢‹ì€ì§€ ì‚´í´ë´…ë‹ˆë‹¤.

## 1. Hosted MCP server tools

í˜¸ìŠ¤í‹°ë“œ íˆ´ì€ ì „ì²´ ë„êµ¬ ì™•ë³µ ìˆ˜í–‰ì„ OpenAI ì¸í”„ë¼ì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì½”ë“œê°€ ë„êµ¬ë¥¼ ë‚˜ì—´í•˜ê³  í˜¸ì¶œí•˜ëŠ” ëŒ€ì‹ ,
[`HostedMCPTool`][agents.tool.HostedMCPTool]ì´ ì„œë²„ ë¼ë²¨(ë° ì„ íƒì  ì»¤ë„¥í„° ë©”íƒ€ë°ì´í„°)ì„ Responses APIë¡œ ì „ë‹¬í•©ë‹ˆë‹¤. ëª¨ë¸ì€ ì›ê²© ì„œë²„ì˜ ë„êµ¬ë¥¼ ë‚˜ì—´í•˜ê³ , Python í”„ë¡œì„¸ìŠ¤ë¡œì˜ ì¶”ê°€ ì½œë°± ì—†ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. í˜¸ìŠ¤í‹°ë“œ íˆ´ì€ í˜„ì¬ Responses APIì˜ í˜¸ìŠ¤í‹°ë“œ MCP í†µí•©ì„ ì§€ì›í•˜ëŠ” OpenAI ëª¨ë¸ì—ì„œ ë™ì‘í•©ë‹ˆë‹¤.

### ê¸°ë³¸ í˜¸ìŠ¤í‹°ë“œ MCP ë„êµ¬

ì—ì´ì „íŠ¸ì˜ `tools` ëª©ë¡ì— [`HostedMCPTool`][agents.tool.HostedMCPTool]ì„ ì¶”ê°€í•˜ì—¬ í˜¸ìŠ¤í‹°ë“œ íˆ´ì„ ìƒì„±í•©ë‹ˆë‹¤. `tool_config`
dictëŠ” REST APIì— ì „ì†¡í•˜ëŠ” JSONê³¼ ë™ì¼í•œ í˜•íƒœë¥¼ ë”°ë¦…ë‹ˆë‹¤:

```python
import asyncio

from agents import Agent, HostedMCPTool, Runner

async def main() -> None:
    agent = Agent(
        name="Assistant",
        tools=[
            HostedMCPTool(
                tool_config={
                    "type": "mcp",
                    "server_label": "gitmcp",
                    "server_url": "https://gitmcp.io/openai/codex",
                    "require_approval": "never",
                }
            )
        ],
    )

    result = await Runner.run(agent, "Which language is this repository written in?")
    print(result.final_output)

asyncio.run(main())
```

í˜¸ìŠ¤í‹°ë“œ ì„œë²„ëŠ” ë„êµ¬ë¥¼ ìë™ìœ¼ë¡œ ë…¸ì¶œí•©ë‹ˆë‹¤. `mcp_servers`ì— ì¶”ê°€í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.

### ìŠ¤íŠ¸ë¦¬ë° í˜¸ìŠ¤í‹°ë“œ MCP ê²°ê³¼

í˜¸ìŠ¤í‹°ë“œ íˆ´ì€ í•¨ìˆ˜ ë„êµ¬ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. `Runner.run_streamed`ì— `stream=True`ë¥¼ ì „ë‹¬í•˜ì—¬
ëª¨ë¸ì´ ì—¬ì „íˆ ì‹¤í–‰ ì¤‘ì¼ ë•Œ ì ì§„ì ì¸ MCP ì¶œë ¥ì„ ì†Œë¹„í•˜ì„¸ìš”:

```python
result = Runner.run_streamed(agent, "Summarise this repository's top languages")
async for event in result.stream_events():
    if event.type == "run_item_stream_event":
        print(f"Received: {event.item}")
print(result.final_output)
```

### ì„ íƒì  ìŠ¹ì¸ íë¦„

ì„œë²„ê°€ ë¯¼ê°í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê²½ìš° ê° ë„êµ¬ ì‹¤í–‰ ì „ì— ì‚¬ëŒ ë˜ëŠ” í”„ë¡œê·¸ë¨ì˜ ìŠ¹ì¸ì„ ìš”êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `tool_config`ì˜
`require_approval`ì„ ë‹¨ì¼ ì •ì±…(`"always"`, `"never"`) ë˜ëŠ” ë„êµ¬ ì´ë¦„ê³¼ ì •ì±…ì˜ dict ë§¤í•‘ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”. Python ë‚´ë¶€ì—ì„œ ê²°ì •ì„ ë‚´ë¦¬ë ¤ë©´ `on_approval_request` ì½œë°±ì„ ì œê³µí•˜ì„¸ìš”.

```python
from agents import MCPToolApprovalFunctionResult, MCPToolApprovalRequest

SAFE_TOOLS = {"read_project_metadata"}

def approve_tool(request: MCPToolApprovalRequest) -> MCPToolApprovalFunctionResult:
    if request.data.name in SAFE_TOOLS:
        return {"approve": True}
    return {"approve": False, "reason": "Escalate to a human reviewer"}

agent = Agent(
    name="Assistant",
    tools=[
        HostedMCPTool(
            tool_config={
                "type": "mcp",
                "server_label": "gitmcp",
                "server_url": "https://gitmcp.io/openai/codex",
                "require_approval": "always",
            },
            on_approval_request=approve_tool,
        )
    ],
)
```

ì½œë°±ì€ ë™ê¸° ë˜ëŠ” ë¹„ë™ê¸°ì¼ ìˆ˜ ìˆìœ¼ë©°, ëª¨ë¸ì´ ì‹¤í–‰ì„ ê³„ì†í•˜ê¸° ìœ„í•´ ìŠ¹ì¸ ë°ì´í„°ê°€ í•„ìš”í•  ë•Œë§ˆë‹¤ í˜¸ì¶œë©ë‹ˆë‹¤.

### ì»¤ë„¥í„° ê¸°ë°˜ í˜¸ìŠ¤í‹°ë“œ ì„œë²„

í˜¸ìŠ¤í‹°ë“œ MCPëŠ” OpenAI ì»¤ë„¥í„°ë„ ì§€ì›í•©ë‹ˆë‹¤. `server_url`ì„ ì§€ì •í•˜ëŠ” ëŒ€ì‹  `connector_id`ì™€ ì•¡ì„¸ìŠ¤ í† í°ì„ ì œê³µí•˜ì„¸ìš”.
Responses APIê°€ ì¸ì¦ì„ ì²˜ë¦¬í•˜ë©°, í˜¸ìŠ¤í‹°ë“œ ì„œë²„ëŠ” ì»¤ë„¥í„°ì˜ ë„êµ¬ë¥¼ ë…¸ì¶œí•©ë‹ˆë‹¤.

```python
import os

HostedMCPTool(
    tool_config={
        "type": "mcp",
        "server_label": "google_calendar",
        "connector_id": "connector_googlecalendar",
        "authorization": os.environ["GOOGLE_CALENDAR_AUTHORIZATION"],
        "require_approval": "never",
    }
)
```

ìŠ¤íŠ¸ë¦¬ë°, ìŠ¹ì¸, ì»¤ë„¥í„°ë¥¼ í¬í•¨í•œ ì™„ì „í•œ í˜¸ìŠ¤í‹°ë“œ íˆ´ ìƒ˜í”Œì€
[`examples/hosted_mcp`](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp)ì— ìˆìŠµë‹ˆë‹¤.

## 2. Streamable HTTP MCP ì„œë²„

ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ ì§ì ‘ ê´€ë¦¬í•˜ë ¤ë©´
[`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp]ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. Streamable HTTP ì„œë²„ëŠ”
íŠ¸ëœìŠ¤í¬íŠ¸ë¥¼ ì§ì ‘ ì œì–´í•˜ê±°ë‚˜ ì„œë²„ë¥¼ ìì²´ ì¸í”„ë¼ ë‚´ì—ì„œ ì‹¤í–‰í•˜ë©´ì„œë„ ë‚®ì€ ì§€ì—° ì‹œê°„ì„ ìœ ì§€í•˜ê³  ì‹¶ì„ ë•Œ ì´ìƒì ì…ë‹ˆë‹¤.

```python
import asyncio
import os

from agents import Agent, Runner
from agents.mcp import MCPServerStreamableHttp
from agents.model_settings import ModelSettings

async def main() -> None:
    token = os.environ["MCP_SERVER_TOKEN"]
    async with MCPServerStreamableHttp(
        name="Streamable HTTP Python Server",
        params={
            "url": "http://localhost:8000/mcp",
            "headers": {"Authorization": f"Bearer {token}"},
            "timeout": 10,
        },
        cache_tools_list=True,
        max_retry_attempts=3,
    ) as server:
        agent = Agent(
            name="Assistant",
            instructions="Use the MCP tools to answer the questions.",
            mcp_servers=[server],
            model_settings=ModelSettings(tool_choice="required"),
        )

        result = await Runner.run(agent, "Add 7 and 22.")
        print(result.final_output)

asyncio.run(main())
```

ìƒì„±ìëŠ” ë‹¤ìŒ ì¶”ê°€ ì˜µì…˜ì„ í—ˆìš©í•©ë‹ˆë‹¤:

- `client_session_timeout_seconds`ëŠ” HTTP ì½ê¸° íƒ€ì„ì•„ì›ƒì„ ì œì–´í•©ë‹ˆë‹¤
- `use_structured_content`ëŠ” `tool_result.structured_content`ë¥¼ í…ìŠ¤íŠ¸ ì¶œë ¥ë³´ë‹¤ ìš°ì„ í• ì§€ ì—¬ë¶€ë¥¼ ì „í™˜í•©ë‹ˆë‹¤
- `max_retry_attempts`ì™€ `retry_backoff_seconds_base`ëŠ” `list_tools()`ì™€ `call_tool()`ì— ìë™ ì¬ì‹œë„ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤
- `tool_filter`ë¥¼ ì‚¬ìš©í•˜ë©´ ë…¸ì¶œí•  ë„êµ¬ì˜ ë¶€ë¶„ì§‘í•©ë§Œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤([ë„êµ¬ í•„í„°ë§](#tool-filtering) ì°¸ì¡°)

## 3. HTTP with SSE MCP ì„œë²„

MCP ì„œë²„ê°€ HTTP with SSE íŠ¸ëœìŠ¤í¬íŠ¸ë¥¼ êµ¬í˜„í•˜ëŠ” ê²½ìš°
[`MCPServerSse`][agents.mcp.server.MCPServerSse]ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ì„¸ìš”. íŠ¸ëœìŠ¤í¬íŠ¸ë¥¼ ì œì™¸í•˜ë©´ APIëŠ” Streamable HTTP ì„œë²„ì™€ ë™ì¼í•©ë‹ˆë‹¤.

```python

from agents import Agent, Runner
from agents.model_settings import ModelSettings
from agents.mcp import MCPServerSse

workspace_id = "demo-workspace"

async with MCPServerSse(
    name="SSE Python Server",
    params={
        "url": "http://localhost:8000/sse",
        "headers": {"X-Workspace": workspace_id},
    },
    cache_tools_list=True,
) as server:
    agent = Agent(
        name="Assistant",
        mcp_servers=[server],
        model_settings=ModelSettings(tool_choice="required"),
    )
    result = await Runner.run(agent, "What's the weather in Tokyo?")
    print(result.final_output)
```

## 4. stdio MCP ì„œë²„

ë¡œì»¬ í•˜ìœ„ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ë˜ëŠ” MCP ì„œë²„ì—ëŠ” [`MCPServerStdio`][agents.mcp.server.MCPServerStdio]ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. SDKê°€
í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•˜ê³  íŒŒì´í”„ë¥¼ ì—´ì–´ë‘ë©°, ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ê°€ ì¢…ë£Œë  ë•Œ ìë™ìœ¼ë¡œ ë‹«ìŠµë‹ˆë‹¤. ì´ ì˜µì…˜ì€ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…ì„ ë§Œë“¤ê±°ë‚˜ ì„œë²„ê°€ ì»¤ë§¨ë“œë¼ì¸ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ë§Œ ë…¸ì¶œí•˜ëŠ” ê²½ìš°ì— ìœ ìš©í•©ë‹ˆë‹¤.

```python
from pathlib import Path
from agents import Agent, Runner
from agents.mcp import MCPServerStdio

current_dir = Path(__file__).parent
samples_dir = current_dir / "sample_files"

async with MCPServerStdio(
    name="Filesystem Server via npx",
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", str(samples_dir)],
    },
) as server:
    agent = Agent(
        name="Assistant",
        instructions="Use the files in the sample directory to answer questions.",
        mcp_servers=[server],
    )
    result = await Runner.run(agent, "List the files available to you.")
    print(result.final_output)
```

## ë„êµ¬ í•„í„°ë§

ê° MCP ì„œë²„ëŠ” ì—ì´ì „íŠ¸ì— í•„ìš”í•œ ê¸°ëŠ¥ë§Œ ë…¸ì¶œí•  ìˆ˜ ìˆë„ë¡ ë„êµ¬ í•„í„°ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. í•„í„°ë§ì€ ìƒì„± ì‹œì  ë˜ëŠ” ì‹¤í–‰ë³„ë¡œ ë™ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì •ì  ë„êµ¬ í•„í„°ë§

ê°„ë‹¨í•œ í—ˆìš©/ì°¨ë‹¨ ëª©ë¡ì„ êµ¬ì„±í•˜ë ¤ë©´ [`create_static_tool_filter`][agents.mcp.create_static_tool_filter]ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

```python
from pathlib import Path

from agents.mcp import MCPServerStdio, create_static_tool_filter

samples_dir = Path("/path/to/files")

filesystem_server = MCPServerStdio(
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", str(samples_dir)],
    },
    tool_filter=create_static_tool_filter(allowed_tool_names=["read_file", "write_file"]),
)
```

`allowed_tool_names`ì™€ `blocked_tool_names`ê°€ ëª¨ë‘ ì œê³µë˜ë©´ SDKëŠ” ë¨¼ì € í—ˆìš© ëª©ë¡ì„ ì ìš©í•œ ë’¤, ë‚¨ì€ ì§‘í•©ì—ì„œ ì°¨ë‹¨ëœ ë„êµ¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.

### ë™ì  ë„êµ¬ í•„í„°ë§

ë” ì •êµí•œ ë¡œì§ì´ í•„ìš”í•˜ë©´ [`ToolFilterContext`][agents.mcp.ToolFilterContext]ë¥¼ ë°›ëŠ” í˜¸ì¶œ ê°€ëŠ¥ ê°ì²´ë¥¼ ì „ë‹¬í•˜ì„¸ìš”. í˜¸ì¶œ ê°€ëŠ¥ ê°ì²´ëŠ” ë™ê¸° ë˜ëŠ” ë¹„ë™ê¸°ì¼ ìˆ˜ ìˆìœ¼ë©°, ë„êµ¬ë¥¼ ë…¸ì¶œí•´ì•¼ í•˜ë©´ `True`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
from pathlib import Path

from agents.mcp import MCPServerStdio, ToolFilterContext

samples_dir = Path("/path/to/files")

async def context_aware_filter(context: ToolFilterContext, tool) -> bool:
    if context.agent.name == "Code Reviewer" and tool.name.startswith("danger_"):
        return False
    return True

async with MCPServerStdio(
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", str(samples_dir)],
    },
    tool_filter=context_aware_filter,
) as server:
    ...
```

í•„í„° ì»¨í…ìŠ¤íŠ¸ëŠ” í™œì„± `run_context`, ë„êµ¬ë¥¼ ìš”ì²­í•˜ëŠ” `agent`, ê·¸ë¦¬ê³  `server_name`ì„ ì œê³µí•©ë‹ˆë‹¤.

## í”„ë¡¬í”„íŠ¸

MCP ì„œë²„ëŠ” ì—ì´ì „íŠ¸ instructionsë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ì§€ì›í•˜ëŠ” ì„œë²„ëŠ” ë‘ ê°€ì§€
ë©”ì„œë“œë¥¼ ë…¸ì¶œí•©ë‹ˆë‹¤:

- `list_prompts()`ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì—´ê±°í•©ë‹ˆë‹¤
- `get_prompt(name, arguments)`ëŠ” ì„ íƒì  ë§¤ê°œë³€ìˆ˜ì™€ í•¨ê»˜ êµ¬ì²´ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤

```python
from agents import Agent

prompt_result = await server.get_prompt(
    "generate_code_review_instructions",
    {"focus": "security vulnerabilities", "language": "python"},
)
instructions = prompt_result.messages[0].content.text

agent = Agent(
    name="Code Reviewer",
    instructions=instructions,
    mcp_servers=[server],
)
```

## ìºì‹±

ëª¨ë“  ì—ì´ì „íŠ¸ ì‹¤í–‰ì€ ê° MCP ì„œë²„ì—ì„œ `list_tools()`ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. ì›ê²© ì„œë²„ëŠ” ëˆˆì— ë„ëŠ” ì§€ì—°ì„ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ëª¨ë“  MCP
ì„œë²„ í´ë˜ìŠ¤ëŠ” `cache_tools_list` ì˜µì…˜ì„ ì œê³µí•©ë‹ˆë‹¤. ë„êµ¬ ì •ì˜ê°€ ìì£¼ ë³€ê²½ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  í™•ì‹ í•˜ëŠ” ê²½ìš°ì—ë§Œ `True`ë¡œ ì„¤ì •í•˜ì„¸ìš”. ë‚˜ì¤‘ì— ìƒˆ ëª©ë¡ì„ ê°•ì œë¡œ ê°€ì ¸ì˜¤ë ¤ë©´ ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ `invalidate_tools_cache()`ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.

## íŠ¸ë ˆì´ì‹±

[íŠ¸ë ˆì´ì‹±](./tracing.md)ì€ MCP í™œë™ì„ ìë™ìœ¼ë¡œ ìº¡ì²˜í•©ë‹ˆë‹¤. í¬í•¨ ë‚´ìš©:

1. MCP ì„œë²„ì— ëŒ€í•œ ë„êµ¬ ëª©ë¡ ì¡°íšŒ í˜¸ì¶œ
2. ë„êµ¬ í˜¸ì¶œì˜ MCP ê´€ë ¨ ì •ë³´

![MCP íŠ¸ë ˆì´ì‹± ìŠ¤í¬ë¦°ìƒ·](../assets/images/mcp-tracing.jpg)

## ì¶”ê°€ ì½ì„ê±°ë¦¬

- [Model Context Protocol](https://modelcontextprotocol.io/) â€“ ì‚¬ì–‘ê³¼ ì„¤ê³„ ê°€ì´ë“œ
- [examples/mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp) â€“ ì‹¤í–‰ ê°€ëŠ¥í•œ stdio, SSE, Streamable HTTP ì˜ˆì‹œ
- [examples/hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp) â€“ ìŠ¹ì¸ ë° ì»¤ë„¥í„°ë¥¼ í¬í•¨í•œ ì™„ì „í•œ í˜¸ìŠ¤í‹°ë“œ MCP ë°ëª¨

================
File: docs/ko/multi_agent.md
================
---
search:
  exclude: true
---
# ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì€ ì•±ì—ì„œ ì—ì´ì „íŠ¸ê°€ ë™ì‘í•˜ëŠ” íë¦„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì–´ë–¤ ì—ì´ì „íŠ¸ë¥¼ ì–´ë–¤ ìˆœì„œë¡œ ì‹¤í–‰í•˜ê³ , ë‹¤ìŒì— ë¬´ì—‡ì„ í• ì§€ ì–´ë–»ê²Œ ê²°ì •í• ê¹Œìš”? ì—ì´ì „íŠ¸ë¥¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•˜ëŠ” ì£¼ìš” ë°©ë²•ì€ ë‘ ê°€ì§€ì…ë‹ˆë‹¤:

1. LLM ì´ ê²°ì •ì„ ë‚´ë¦¬ë„ë¡ í—ˆìš©í•˜ê¸°: LLM ì˜ ì§€ëŠ¥ì„ í™œìš©í•´ ê³„íší•˜ê³  ì¶”ë¡ í•˜ë©° ê·¸ì— ë”°ë¼ ìˆ˜í–‰í•  ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ë°©ì‹
2. ì½”ë“œë¡œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•˜ê¸°: ì½”ë“œë¡œ ì—ì´ì „íŠ¸ì˜ íë¦„ì„ ê²°ì •í•˜ëŠ” ë°©ì‹

ì´ íŒ¨í„´ë“¤ì„ í˜¼í•©í•´ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê°ê°ì˜ ì ˆì¶©ì ì€ ì•„ë˜ì— ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## LLM ì„ í†µí•œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

ì—ì´ì „íŠ¸ëŠ” instructions, tools, í•¸ë“œì˜¤í”„ ë¥¼ ê°–ì¶˜ LLM ì…ë‹ˆë‹¤. ì´ëŠ” ê°œë°©í˜• ê³¼ì œê°€ ì£¼ì–´ì¡Œì„ ë•Œ LLM ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ í–‰ë™í•˜ê³  ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ë©°, í•¸ë“œì˜¤í”„ ë¥¼ í†µí•´ í•˜ìœ„ ì—ì´ì „íŠ¸ì— ì‘ì—…ì„ ìœ„ì„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê³¼ì œë¥¼ í•´ê²°í•˜ëŠ” ê³„íšì„ ììœ¨ì ìœ¼ë¡œ ìˆ˜ë¦½í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë„êµ¬ë¥¼ ê°–ì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- ì›¹ ê²€ìƒ‰ ì„ í†µí•´ ì˜¨ë¼ì¸ì—ì„œ ì •ë³´ ì°¾ê¸°
- íŒŒì¼ ê²€ìƒ‰ ë° ê²€ìƒ‰ìœ¼ë¡œ ì‚¬ë‚´ ë°ì´í„°ì™€ ì—°ê²°ì„ íƒìƒ‰í•˜ê¸°
- ì»´í“¨í„° ì‚¬ìš© ìœ¼ë¡œ ì»´í“¨í„°ì—ì„œ ì‘ì—… ìˆ˜í–‰í•˜ê¸°
- ì½”ë“œ ì‹¤í–‰ ìœ¼ë¡œ ë°ì´í„° ë¶„ì„ ìˆ˜í–‰í•˜ê¸°
- ê¸°íš, ë³´ê³ ì„œ ì‘ì„± ë“±ì— íŠ¹í™”ëœ ì—ì´ì „íŠ¸ë¡œì˜ í•¸ë“œì˜¤í”„

ì´ íŒ¨í„´ì€ ê³¼ì œê°€ ê°œë°©í˜•ì´ê³  LLM ì˜ ì§€ëŠ¥ì— ì˜ì¡´í•˜ê³ ì í•  ë•Œ ì í•©í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì „ìˆ ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. ì¢‹ì€ í”„ë¡¬í”„íŠ¸ì— íˆ¬ìí•˜ì„¸ìš”. ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬, ì‚¬ìš© ë°©ë²•, ê·¸ë¦¬ê³  ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ ë¥¼ ëª…í™•íˆ í•˜ì„¸ìš”.
2. ì•±ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ë°˜ë³µ ê°œì„ í•˜ì„¸ìš”. ì–´ë””ì—ì„œ ë¬¸ì œê°€ ìƒê¸°ëŠ”ì§€ íŒŒì•…í•˜ê³  í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ì„¸ìš”.
3. ì—ì´ì „íŠ¸ê°€ ìê¸° ì„±ì°°í•˜ê³  ê°œì„ í•˜ë„ë¡ í—ˆìš©í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ ë£¨í”„ì—ì„œ ì‹¤í–‰í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ë¹„íŒí•˜ê²Œ í•˜ê±°ë‚˜, ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ì œê³µí•˜ê³  ì´ë¥¼ í†µí•´ ê°œì„ í•˜ë„ë¡ í•˜ì„¸ìš”.
4. ë‹¤ëª©ì  ì—ì´ì „íŠ¸ í•˜ë‚˜ì— ëª¨ë“  ê²ƒì„ ê¸°ëŒ€í•˜ê¸°ë³´ë‹¤, í•˜ë‚˜ì˜ ì‘ì—…ì— íŠ¹í™”ë˜ì–´ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë‚´ëŠ” ì „ë¬¸ ì—ì´ì „íŠ¸ë¥¼ ë‘ì„¸ìš”.
5. [evals](https://platform.openai.com/docs/guides/evals)ì— íˆ¬ìí•˜ì„¸ìš”. ì´ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨í•˜ì—¬ ê³¼ì œ ìˆ˜í–‰ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì½”ë“œ ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

LLM ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì´ ê°•ë ¥í•˜ê¸´ í•˜ì§€ë§Œ, ì½”ë“œ ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì€ ì†ë„, ë¹„ìš©, ì„±ëŠ¥ ì¸¡ë©´ì—ì„œ ë” ê²°ì •ì ì´ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. ì¼ë°˜ì ì¸ íŒ¨í„´ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- [structured outputs](https://platform.openai.com/docs/guides/structured-outputs)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”ë“œë¡œ ê²€ì‚¬í•  ìˆ˜ ìˆëŠ” ì ì ˆí•œ í˜•ì‹ì˜ ë°ì´í„° ë¥¼ ìƒì„±í•˜ê¸°. ì˜ˆë¥¼ ë“¤ì–´, ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—…ì„ ëª‡ ê°œì˜ ì¹´í…Œê³ ë¦¬ ë¡œ ë¶„ë¥˜í•˜ë„ë¡ ìš”ì²­í•œ ë‹¤ìŒ, í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ ë‹¤ìŒ ì—ì´ì „íŠ¸ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í•˜ë‚˜ì˜ ì—ì´ì „íŠ¸ ì¶œë ¥ì„ ë‹¤ìŒ ì—ì´ì „íŠ¸ì˜ ì…ë ¥ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¥¼ ì²´ì´ë‹í•˜ê¸°. ì˜ˆì»¨ëŒ€ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì‘ì„± ì‘ì—…ì„ ë¦¬ì„œì¹˜, ê°œìš” ì‘ì„±, ë³¸ë¬¸ ì‘ì„±, ë¹„íŒ, ê°œì„ ì˜ ì¼ë ¨ì˜ ë‹¨ê³„ë¡œ ë¶„í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- í‰ê°€ ë° í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” ì—ì´ì „íŠ¸ì™€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ `while` ë£¨í”„ë¡œ í•¨ê»˜ ì‹¤í–‰í•˜ì—¬, í‰ê°€ìê°€ ê²°ê³¼ê°€ íŠ¹ì • ê¸°ì¤€ì„ í†µê³¼í–ˆë‹¤ê³  í•  ë•Œê¹Œì§€ ë°˜ë³µí•˜ê¸°
- `asyncio.gather` ê°™ì€ íŒŒì´ì¬ ê¸°ë³¸ êµ¬ì„±ìš”ì†Œë¥¼ í†µí•´ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê¸°. ì„œë¡œ ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ì—¬ëŸ¬ ì‘ì—…ì´ ìˆì„ ë•Œ ì†ë„ í–¥ìƒì— ìœ ìš©í•©ë‹ˆë‹¤

[`examples/agent_patterns`](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns)ì— ì—¬ëŸ¬ code examples ê°€ ìˆìŠµë‹ˆë‹¤.

================
File: docs/ko/quickstart.md
================
---
search:
  exclude: true
---
# ë¹ ë¥¸ ì‹œì‘

## í”„ë¡œì íŠ¸ ë° ê°€ìƒ í™˜ê²½ ìƒì„±

í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ë©´ ë©ë‹ˆë‹¤.

```bash
mkdir my_project
cd my_project
python -m venv .venv
```

### ê°€ìƒ í™˜ê²½ í™œì„±í™”

ìƒˆ í„°ë¯¸ë„ ì„¸ì…˜ì„ ì‹œì‘í•  ë•Œë§ˆë‹¤ ìˆ˜í–‰í•˜ì„¸ìš”.

```bash
source .venv/bin/activate
```

### Agents SDK ì„¤ì¹˜

```bash
pip install openai-agents # or `uv add openai-agents`, etc
```

### OpenAI API í‚¤ ì„¤ì •

í‚¤ê°€ ì—†ë‹¤ë©´ OpenAI API í‚¤ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ [ì´ ì•ˆë‚´](https://platform.openai.com/docs/quickstart#create-and-export-an-api-key)ë¥¼ ë”°ë¥´ì„¸ìš”.

```bash
export OPENAI_API_KEY=sk-...
```

## ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ ë§Œë“¤ê¸°

ì—ì´ì „íŠ¸ëŠ” instructions, ì´ë¦„, ì„ íƒì  êµ¬ì„±(ì˜ˆ: `model_config`)ìœ¼ë¡œ ì •ì˜ë©ë‹ˆë‹¤

```python
from agents import Agent

agent = Agent(
    name="Math Tutor",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)
```

## ì—ì´ì „íŠ¸ ì¶”ê°€

ì¶”ê°€ ì—ì´ì „íŠ¸ë„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `handoff_descriptions`ëŠ” í•¸ë“œì˜¤í”„ ë¼ìš°íŒ…ì„ ê²°ì •í•˜ëŠ” ë° ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤

```python
from agents import Agent

history_tutor_agent = Agent(
    name="History Tutor",
    handoff_description="Specialist agent for historical questions",
    instructions="You provide assistance with historical queries. Explain important events and context clearly.",
)

math_tutor_agent = Agent(
    name="Math Tutor",
    handoff_description="Specialist agent for math questions",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)
```

## í•¸ë“œì˜¤í”„ ì •ì˜

ê° ì—ì´ì „íŠ¸ì—ì„œ, ì‘ì—…ì„ ì§„í–‰í•˜ëŠ” ë°©ë²•ì„ ê²°ì •í•˜ê¸° ìœ„í•´ ì„ íƒí•  ìˆ˜ ìˆëŠ” ë‚˜ê°€ëŠ” í•¸ë“œì˜¤í”„ ì˜µì…˜ì˜ ì¸ë²¤í† ë¦¬ë¥¼ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
triage_agent = Agent(
    name="Triage Agent",
    instructions="You determine which agent to use based on the user's homework question",
    handoffs=[history_tutor_agent, math_tutor_agent]
)
```

## ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹¤í–‰

ì›Œí¬í”Œë¡œê°€ ì‹¤í–‰ë˜ê³  ë¶„ë¥˜ ì—ì´ì „íŠ¸ê°€ ë‘ ì „ë¬¸ ì—ì´ì „íŠ¸ ì‚¬ì´ë¥¼ ì˜¬ë°”ë¥´ê²Œ ë¼ìš°íŒ…í•˜ëŠ”ì§€ í™•ì¸í•´ ë´…ì‹œë‹¤.

```python
from agents import Runner

async def main():
    result = await Runner.run(triage_agent, "What is the capital of France?")
    print(result.final_output)
```

## ê°€ë“œë ˆì¼ ì¶”ê°€

ì…ë ¥ ë˜ëŠ” ì¶œë ¥ì— ëŒ€í•´ ì‹¤í–‰í•  ì‚¬ìš©ì ì •ì˜ ê°€ë“œë ˆì¼ì„ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from agents import GuardrailFunctionOutput, Agent, Runner
from pydantic import BaseModel


class HomeworkOutput(BaseModel):
    is_homework: bool
    reasoning: str

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the user is asking about homework.",
    output_type=HomeworkOutput,
)

async def homework_guardrail(ctx, agent, input_data):
    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
    final_output = result.final_output_as(HomeworkOutput)
    return GuardrailFunctionOutput(
        output_info=final_output,
        tripwire_triggered=not final_output.is_homework,
    )
```

## ì „ì²´ í†µí•©

ëª¨ë“  ê²ƒì„ í•©ì³ í•¸ë“œì˜¤í”„ì™€ ì…ë ¥ ê°€ë“œë ˆì¼ì„ ì‚¬ìš©í•´ ì „ì²´ ì›Œí¬í”Œë¡œë¥¼ ì‹¤í–‰í•´ ë´…ì‹œë‹¤.

```python
from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner
from agents.exceptions import InputGuardrailTripwireTriggered
from pydantic import BaseModel
import asyncio

class HomeworkOutput(BaseModel):
    is_homework: bool
    reasoning: str

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the user is asking about homework.",
    output_type=HomeworkOutput,
)

math_tutor_agent = Agent(
    name="Math Tutor",
    handoff_description="Specialist agent for math questions",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)

history_tutor_agent = Agent(
    name="History Tutor",
    handoff_description="Specialist agent for historical questions",
    instructions="You provide assistance with historical queries. Explain important events and context clearly.",
)


async def homework_guardrail(ctx, agent, input_data):
    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
    final_output = result.final_output_as(HomeworkOutput)
    return GuardrailFunctionOutput(
        output_info=final_output,
        tripwire_triggered=not final_output.is_homework,
    )

triage_agent = Agent(
    name="Triage Agent",
    instructions="You determine which agent to use based on the user's homework question",
    handoffs=[history_tutor_agent, math_tutor_agent],
    input_guardrails=[
        InputGuardrail(guardrail_function=homework_guardrail),
    ],
)

async def main():
    # Example 1: History question
    try:
        result = await Runner.run(triage_agent, "who was the first president of the united states?")
        print(result.final_output)
    except InputGuardrailTripwireTriggered as e:
        print("Guardrail blocked this input:", e)

    # Example 2: General/philosophical question
    try:
        result = await Runner.run(triage_agent, "What is the meaning of life?")
        print(result.final_output)
    except InputGuardrailTripwireTriggered as e:
        print("Guardrail blocked this input:", e)

if __name__ == "__main__":
    asyncio.run(main())
```

## íŠ¸ë ˆì´ìŠ¤ ë³´ê¸°

ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ë°œìƒí•œ ë‚´ìš©ì„ ê²€í† í•˜ë ¤ë©´ [OpenAI ëŒ€ì‹œë³´ë“œì˜ Trace viewer](https://platform.openai.com/traces)ë¡œ ì´ë™í•´ ì‹¤í–‰ íŠ¸ë ˆì´ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.

## ë‹¤ìŒ ë‹¨ê³„

ë” ë³µì¡í•œ ì—ì´ì „íŠ¸ í”Œë¡œìš°ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì„¸ìš”:

- Learn about how to configure [ì—ì´ì „íŠ¸](agents.md).
- Learn about [ì—ì´ì „íŠ¸ ì‹¤í–‰](running_agents.md).
- Learn about [tools](tools.md), [ê°€ë“œë ˆì¼](guardrails.md) and [ëª¨ë¸](models/index.md).

================
File: docs/ko/release.md
================
---
search:
  exclude: true
---
# ë¦´ë¦¬ìŠ¤ í”„ë¡œì„¸ìŠ¤/ë³€ê²½ ë¡œê·¸

ì´ í”„ë¡œì íŠ¸ëŠ” `0.Y.Z` í˜•ì‹ì˜ ì•½ê°„ ìˆ˜ì •ëœ ì‹œë§¨í‹± ë²„ì „ ê´€ë¦¬ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ì•ì˜ `0`ì€ SDKê°€ ì•„ì§ ë¹ ë¥´ê²Œ ë°œì „ ì¤‘ì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ê° êµ¬ì„± ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì¦ë¶„í•©ë‹ˆë‹¤:

## ë§ˆì´ë„ˆ(`Y`) ë²„ì „

ë² íƒ€ë¡œ í‘œì‹œë˜ì§€ ì•Šì€ í¼ë¸”ë¦­ ì¸í„°í˜ì´ìŠ¤ì— **í˜¸í™˜ì„± ê¹¨ì§€ëŠ” ë³€ê²½ ì‚¬í•­**ì´ ìˆì„ ë•Œ ë§ˆì´ë„ˆ ë²„ì „ `Y`ë¥¼ ì˜¬ë¦½ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, `0.0.x`ì—ì„œ `0.1.x`ë¡œ ì˜¬ë¼ê°ˆ ë•Œ í˜¸í™˜ì„± ê¹¨ì§€ëŠ” ë³€ê²½ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í˜¸í™˜ì„± ê¹¨ì§€ëŠ” ë³€ê²½ì„ ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, í”„ë¡œì íŠ¸ì—ì„œ `0.0.x` ë²„ì „ì— ê³ ì •í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

## íŒ¨ì¹˜(`Z`) ë²„ì „

í˜¸í™˜ì„±ì„ ê¹¨ì§€ ì•ŠëŠ” ë³€ê²½ì—ëŠ” `Z`ë¥¼ ì˜¬ë¦½ë‹ˆë‹¤:

- ë²„ê·¸ ìˆ˜ì •
- ìƒˆë¡œìš´ ê¸°ëŠ¥
- í”„ë¼ì´ë¹— ì¸í„°í˜ì´ìŠ¤ ë³€ê²½
- ë² íƒ€ ê¸°ëŠ¥ ì—…ë°ì´íŠ¸

## í˜¸í™˜ì„± ê¹¨ì§€ëŠ” ë³€ê²½ ë¡œê·¸

### 0.2.0

ì´ ë²„ì „ì—ì„œëŠ” ì´ì „ì— ì¸ìë¡œ `Agent`ë¥¼ ë°›ë˜ ëª‡ëª‡ ìœ„ì¹˜ê°€ ì´ì œ `AgentBase`ë¥¼ ì¸ìë¡œ ë°›ë„ë¡ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜ˆ: MCP ì„œë²„ì˜ `list_tools()` í˜¸ì¶œ. ì´ëŠ” ìˆœì „íˆ íƒ€ì…ê³¼ ê´€ë ¨ëœ ë³€ê²½ìœ¼ë¡œ, ì—¬ì „íˆ `Agent` ê°ì²´ë¥¼ ë°›ê²Œ ë©ë‹ˆë‹¤. ì—…ë°ì´íŠ¸í•˜ë ¤ë©´ `Agent`ë¥¼ `AgentBase`ë¡œ ë°”ê¿” íƒ€ì… ì—ëŸ¬ë§Œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤.

### 0.1.0

ì´ ë²„ì „ì—ì„œ [`MCPServer.list_tools()`][agents.mcp.server.MCPServer]ì—ëŠ” `run_context`ì™€ `agent`ë¼ëŠ” ë‘ ê°œì˜ ìƒˆ ë§¤ê°œë³€ìˆ˜ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. `MCPServer`ë¥¼ ìƒì†í•˜ëŠ” ëª¨ë“  í´ë˜ìŠ¤ì— ì´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

================
File: docs/ko/repl.md
================
---
search:
  exclude: true
---
# REPL ìœ í‹¸ë¦¬í‹°

SDKëŠ” í„°ë¯¸ë„ì—ì„œ ì—ì´ì „íŠ¸ì˜ ë™ì‘ì„ ë¹ ë¥´ê³  ëŒ€í™”í˜•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë„ë¡ `run_demo_loop`ë¥¼ ì œê³µí•©ë‹ˆë‹¤.


```python
import asyncio
from agents import Agent, run_demo_loop

async def main() -> None:
    agent = Agent(name="Assistant", instructions="You are a helpful assistant.")
    await run_demo_loop(agent)

if __name__ == "__main__":
    asyncio.run(main())
```

`run_demo_loop`ëŠ” ë£¨í”„ì—ì„œ ì‚¬ìš©ì ì…ë ¥ì„ ìš”ì²­í•˜ë©°, í„´ ê°„ ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•©ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ëŒ€ë¡œ ëª¨ë¸ ì¶œë ¥ì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤. ìœ„ ì˜ˆì œë¥¼ ì‹¤í–‰í•˜ë©´, run_demo_loopëŠ” ëŒ€í™”í˜• ì±„íŒ… ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì§€ì†í•´ì„œ ì…ë ¥ì„ ìš”ì²­í•˜ê³ , í„´ ê°„ ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ ê¸°ì–µí•˜ì—¬(ì—ì´ì „íŠ¸ê°€ ì–´ë–¤ ë‚´ìš©ì´ ë…¼ì˜ë˜ì—ˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆë„ë¡) ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¦‰ì‹œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ ìë™ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

ì´ ì±„íŒ… ì„¸ì…˜ì„ ì¢…ë£Œí•˜ë ¤ë©´ `quit` ë˜ëŠ” `exit`ë¥¼ ì…ë ¥í•˜ê³  Enterë¥¼ ëˆ„ë¥´ê±°ë‚˜, `Ctrl-D` í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

================
File: docs/ko/results.md
================
---
search:
  exclude: true
---
# ê²°ê³¼

`Runner.run` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë©´ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ë°›ìŠµë‹ˆë‹¤:

- [`RunResult`][agents.result.RunResult] (`run` ë˜ëŠ” `run_sync` í˜¸ì¶œ ì‹œ)
- [`RunResultStreaming`][agents.result.RunResultStreaming] (`run_streamed` í˜¸ì¶œ ì‹œ)

ë‘˜ ë‹¤ [`RunResultBase`][agents.result.RunResultBase]ë¥¼ ìƒì†í•˜ë©°, ëŒ€ë¶€ë¶„ì˜ ìœ ìš©í•œ ì •ë³´ê°€ ì—¬ê¸°ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ìµœì¢… ì¶œë ¥

[`final_output`][agents.result.RunResultBase.final_output] ì†ì„±ì—ëŠ” ë§ˆì§€ë§‰ìœ¼ë¡œ ì‹¤í–‰ëœ ì—ì´ì „íŠ¸ì˜ ìµœì¢… ì¶œë ¥ì´ ë“¤ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤:

- ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ì— `output_type`ì´ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš° `str`
- ì—ì´ì „íŠ¸ì— ì¶œë ¥ íƒ€ì…ì´ ì •ì˜ëœ ê²½ìš° `last_agent.output_type` íƒ€ì…ì˜ ê°ì²´

!!! note

    `final_output`ì˜ íƒ€ì…ì€ `Any`ì…ë‹ˆë‹¤. í•¸ë“œì˜¤í”„ ë•Œë¬¸ì— ì •ì ìœ¼ë¡œ íƒ€ì…ì„ ì§€ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•¸ë“œì˜¤í”„ê°€ ë°œìƒí•˜ë©´ ì–´ë–¤ ì—ì´ì „íŠ¸ë“  ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ê°€ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê°€ëŠ¥í•œ ì¶œë ¥ íƒ€ì…ì˜ ì§‘í•©ì„ ì •ì ìœ¼ë¡œ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

## ë‹¤ìŒ í„´ì„ ìœ„í•œ ì…ë ¥

[`result.to_input_list()`][agents.result.RunResultBase.to_input_list]ë¥¼ ì‚¬ìš©í•˜ë©´, ì œê³µí•œ ì›ë³¸ ì…ë ¥ê³¼ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ìƒì„±ëœ í•­ëª©ì„ ì´ì–´ë¶™ì¸ ì…ë ¥ ë¦¬ìŠ¤íŠ¸ë¡œ ê²°ê³¼ë¥¼ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í•œ ë²ˆì˜ ì—ì´ì „íŠ¸ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë‹¤ìŒ ì‹¤í–‰ì— ì „ë‹¬í•˜ê±°ë‚˜, ë£¨í”„ì—ì„œ ì‹¤í–‰í•˜ë©´ì„œ ë§¤ë²ˆ ìƒˆë¡œìš´ ì‚¬ìš©ì ì…ë ¥ì„ ë§ë¶™ì´ëŠ” ì‘ì—…ì´ í¸ë¦¬í•´ì§‘ë‹ˆë‹¤.

## ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸

[`last_agent`][agents.result.RunResultBase.last_agent] ì†ì„±ì—ëŠ” ë§ˆì§€ë§‰ìœ¼ë¡œ ì‹¤í–‰ëœ ì—ì´ì „íŠ¸ê°€ ë“¤ì–´ ìˆìŠµë‹ˆë‹¤. ì• í”Œë¦¬ì¼€ì´ì…˜ì— ë”°ë¼, ì´ëŠ” ì‚¬ìš©ìê°€ ë‹¤ìŒì— ë¬´ì–¸ê°€ë¥¼ ì…ë ¥í•  ë•Œ ìœ ìš©í•œ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í”„ëŸ°íŠ¸ë¼ì¸ íŠ¸ë¦¬ì•„ì§€ ì—ì´ì „íŠ¸ê°€ ì–¸ì–´ë³„ ì—ì´ì „íŠ¸ë¡œ í•¸ë“œì˜¤í”„í•˜ëŠ” ê²½ìš°, ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ë¥¼ ì €ì¥í•´ ë‘ì—ˆë‹¤ê°€ ì‚¬ìš©ìê°€ ë‹¤ìŒì— ì—ì´ì „íŠ¸ì—ê²Œ ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ë•Œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ìƒˆ í•­ëª©

[`new_items`][agents.result.RunResultBase.new_items] ì†ì„±ì—ëŠ” ì‹¤í–‰ ì¤‘ì— ìƒì„±ëœ ìƒˆ í•­ëª©ì´ ë“¤ì–´ ìˆìŠµë‹ˆë‹¤. í•­ëª©ì€ [`RunItem`][agents.items.RunItem]ì…ë‹ˆë‹¤. ì‹¤í–‰ í•­ëª©ì€ LLMì´ ìƒì„±í•œ ì›ë¬¸ í•­ëª©ì„ ë˜í•‘í•©ë‹ˆë‹¤.

- [`MessageOutputItem`][agents.items.MessageOutputItem]ì€ LLMì˜ ë©”ì‹œì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì›ë¬¸ í•­ëª©ì€ ìƒì„±ëœ ë©”ì‹œì§€ì…ë‹ˆë‹¤.
- [`HandoffCallItem`][agents.items.HandoffCallItem]ì€ LLMì´ í•¸ë“œì˜¤í”„ ë„êµ¬ë¥¼ í˜¸ì¶œí–ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì›ë¬¸ í•­ëª©ì€ LLMì˜ ë„êµ¬ í˜¸ì¶œ í•­ëª©ì…ë‹ˆë‹¤.
- [`HandoffOutputItem`][agents.items.HandoffOutputItem]ì€ í•¸ë“œì˜¤í”„ê°€ ë°œìƒí–ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì›ë¬¸ í•­ëª©ì€ í•¸ë“œì˜¤í”„ ë„êµ¬ í˜¸ì¶œì— ëŒ€í•œ ë„êµ¬ ì‘ë‹µì…ë‹ˆë‹¤. í•­ëª©ì—ì„œ ì†ŒìŠ¤/íƒ€ê¹ƒ ì—ì´ì „íŠ¸ì—ë„ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- [`ToolCallItem`][agents.items.ToolCallItem]ì€ LLMì´ ë„êµ¬ë¥¼ í˜¸ì¶œí–ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
- [`ToolCallOutputItem`][agents.items.ToolCallOutputItem]ì€ ë„êµ¬ê°€ í˜¸ì¶œë˜ì—ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì›ë¬¸ í•­ëª©ì€ ë„êµ¬ ì‘ë‹µì…ë‹ˆë‹¤. í•­ëª©ì—ì„œ ë„êµ¬ ì¶œë ¥ì—ë„ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- [`ReasoningItem`][agents.items.ReasoningItem]ì€ LLMì˜ ì¶”ë¡  í•­ëª©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì›ë¬¸ í•­ëª©ì€ ìƒì„±ëœ ì¶”ë¡ ì…ë‹ˆë‹¤.

## ê¸°íƒ€ ì •ë³´

### ê°€ë“œë ˆì¼ ê²°ê³¼

[`input_guardrail_results`][agents.result.RunResultBase.input_guardrail_results] ë° [`output_guardrail_results`][agents.result.RunResultBase.output_guardrail_results] ì†ì„±ì—ëŠ” (ìˆë‹¤ë©´) ê°€ë“œë ˆì¼ì˜ ê²°ê³¼ê°€ ë“¤ì–´ ìˆìŠµë‹ˆë‹¤. ê°€ë“œë ˆì¼ ê²°ê³¼ì—ëŠ” ë¡œê¹…í•˜ê±°ë‚˜ ì €ì¥í•˜ê³  ì‹¶ì€ ìœ ìš©í•œ ì •ë³´ê°€ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ì œê³µí•´ ë“œë¦½ë‹ˆë‹¤.

### ì›ë¬¸ ì‘ë‹µ

[`raw_responses`][agents.result.RunResultBase.raw_responses] ì†ì„±ì—ëŠ” LLMì´ ìƒì„±í•œ [`ModelResponse`][agents.items.ModelResponse]ê°€ ë“¤ì–´ ìˆìŠµë‹ˆë‹¤.

### ì›ë˜ ì…ë ¥

[`input`][agents.result.RunResultBase.input] ì†ì„±ì—ëŠ” `run` ë©”ì„œë“œì— ì œê³µí•œ ì›ë˜ ì…ë ¥ì´ ë“¤ì–´ ìˆìŠµë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ê²½ìš° í•„ìš”í•˜ì§€ ì•Šì§€ë§Œ, í•„ìš”í•œ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì œê³µë©ë‹ˆë‹¤.

================
File: docs/ko/running_agents.md
================
---
search:
  exclude: true
---
# ì—ì´ì „íŠ¸ ì‹¤í–‰

[`Runner`][agents.run.Runner] í´ë˜ìŠ¤ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜µì…˜ì€ 3ê°€ì§€ì…ë‹ˆë‹¤:

1. [`Runner.run()`][agents.run.Runner.run]: ë¹„ë™ê¸° ì‹¤í–‰í•˜ë©° [`RunResult`][agents.result.RunResult] ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤
2. [`Runner.run_sync()`][agents.run.Runner.run_sync]: ë™ê¸° ë©”ì„œë“œë¡œ ë‚´ë¶€ì ìœ¼ë¡œ `.run()` ì„ ì‹¤í–‰í•©ë‹ˆë‹¤
3. [`Runner.run_streamed()`][agents.run.Runner.run_streamed]: ë¹„ë™ê¸° ì‹¤í–‰í•˜ë©° [`RunResultStreaming`][agents.result.RunResultStreaming] ì„ ë°˜í™˜í•©ë‹ˆë‹¤. LLM ì„ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ í˜¸ì¶œí•˜ê³ , ìˆ˜ì‹ ë˜ëŠ” ëŒ€ë¡œ ì´ë²¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤

```python
from agents import Agent, Runner

async def main():
    agent = Agent(name="Assistant", instructions="You are a helpful assistant")

    result = await Runner.run(agent, "Write a haiku about recursion in programming.")
    print(result.final_output)
    # Code within the code,
    # Functions calling themselves,
    # Infinite loop's dance
```

ìì„¸í•œ ë‚´ìš©ì€ [ê²°ê³¼ ê°€ì´ë“œ](results.md)ì—ì„œ í™•ì¸í•˜ì„¸ìš”.

## ì—ì´ì „íŠ¸ ë£¨í”„

`Runner` ì˜ run ë©”ì„œë“œë¥¼ ì‚¬ìš©í•  ë•Œ ì‹œì‘ ì—ì´ì „íŠ¸ì™€ ì…ë ¥ì„ ì „ë‹¬í•©ë‹ˆë‹¤. ì…ë ¥ì€ ë¬¸ìì—´(ì‚¬ìš©ì ë©”ì‹œì§€ë¡œ ê°„ì£¼) ë˜ëŠ” OpenAI Responses API ì˜ ì…ë ¥ í•­ëª© ëª©ë¡ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Runner ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë£¨í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:

1. í˜„ì¬ ì—ì´ì „íŠ¸ì™€ í˜„ì¬ ì…ë ¥ìœ¼ë¡œ LLM ì„ í˜¸ì¶œí•©ë‹ˆë‹¤
2. LLM ì´ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤
    1. LLM ì´ `final_output` ì„ ë°˜í™˜í•˜ë©´ ë£¨í”„ë¥¼ ì¢…ë£Œí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤
    2. LLM ì´ í•¸ë“œì˜¤í”„ë¥¼ ìˆ˜í–‰í•˜ë©´ í˜„ì¬ ì—ì´ì „íŠ¸ì™€ ì…ë ¥ì„ ì—…ë°ì´íŠ¸í•˜ê³  ë£¨í”„ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤
    3. LLM ì´ ë„êµ¬ í˜¸ì¶œì„ ìƒì„±í•˜ë©´ í•´ë‹¹ ë„êµ¬ í˜¸ì¶œì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶”ê°€í•œ ë’¤, ë£¨í”„ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤
3. ì „ë‹¬ëœ `max_turns` ë¥¼ ì´ˆê³¼í•˜ë©´ [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded] ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤

!!! note

    LLM ì¶œë ¥ì´ "final output" ìœ¼ë¡œ ê°„ì£¼ë˜ëŠ” ê·œì¹™ì€ ì›í•˜ëŠ” íƒ€ì…ì˜ í…ìŠ¤íŠ¸ ì¶œë ¥ì„ ìƒì„±í•˜ê³ , ë„êµ¬ í˜¸ì¶œì´ ì—†ëŠ” ê²½ìš°ì…ë‹ˆë‹¤.

## ìŠ¤íŠ¸ë¦¬ë°

ìŠ¤íŠ¸ë¦¬ë°ì„ ì‚¬ìš©í•˜ë©´ LLM ì´ ì‹¤í–‰ë˜ëŠ” ë™ì•ˆ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¥¼ ì¶”ê°€ë¡œ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë¦¼ì´ ì™„ë£Œë˜ë©´ [`RunResultStreaming`][agents.result.RunResultStreaming] ì— ì‹¤í–‰ì— ëŒ€í•œ ì „ì²´ ì •ë³´ì™€ ìƒì„±ëœ ëª¨ë“  ìƒˆ ì¶œë ¥ì´ í¬í•¨ë©ë‹ˆë‹¤. ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ëŠ” `.stream_events()` ë¥¼ í˜¸ì¶œí•˜ì—¬ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ìŠ¤íŠ¸ë¦¬ë° ê°€ì´ë“œ](streaming.md)ì—ì„œ í™•ì¸í•˜ì„¸ìš”.

## ì‹¤í–‰ êµ¬ì„±

`run_config` ë§¤ê°œë³€ìˆ˜ë¡œ ì—ì´ì „íŠ¸ ì‹¤í–‰ì— ëŒ€í•œ ì „ì—­ ì„¤ì •ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

-   [`model`][agents.run.RunConfig.model]: ê° Agent ì˜ `model` ê³¼ ê´€ê³„ì—†ì´ ì‚¬ìš©í•  ì „ì—­ LLM ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤
-   [`model_provider`][agents.run.RunConfig.model_provider]: ëª¨ë¸ ì´ë¦„ì„ ì¡°íšŒí•˜ê¸° ìœ„í•œ ëª¨ë¸ í”„ë¡œë°”ì´ë”ë¡œ, ê¸°ë³¸ê°’ì€ OpenAI ì…ë‹ˆë‹¤
-   [`model_settings`][agents.run.RunConfig.model_settings]: ì—ì´ì „íŠ¸ë³„ ì„¤ì •ì„ ì¬ì •ì˜í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì „ì—­ `temperature` ë˜ëŠ” `top_p` ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
-   [`input_guardrails`][agents.run.RunConfig.input_guardrails], [`output_guardrails`][agents.run.RunConfig.output_guardrails]: ëª¨ë“  ì‹¤í–‰ì— í¬í•¨í•  ì…ë ¥ ë˜ëŠ” ì¶œë ¥ ê°€ë“œë ˆì¼ ëª©ë¡ì…ë‹ˆë‹¤
-   [`handoff_input_filter`][agents.run.RunConfig.handoff_input_filter]: í•¸ë“œì˜¤í”„ì— ì´ë¯¸ ì§€ì •ëœ í•„í„°ê°€ ì—†ëŠ” ê²½ìš° ëª¨ë“  í•¸ë“œì˜¤í”„ì— ì ìš©í•  ì „ì—­ ì…ë ¥ í•„í„°ì…ë‹ˆë‹¤. ì…ë ¥ í•„í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ìƒˆ ì—ì´ì „íŠ¸ì— ì „ë‹¬ë˜ëŠ” ì…ë ¥ì„ í¸ì§‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”
-   [`tracing_disabled`][agents.run.RunConfig.tracing_disabled]: ì „ì²´ ì‹¤í–‰ì— ëŒ€í•´ [íŠ¸ë ˆì´ì‹±](tracing.md) ì„ ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
-   [`trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data]: LLM ë° ë„êµ¬ í˜¸ì¶œì˜ ì…ë ¥/ì¶œë ¥ ë“± ì ì¬ì ìœ¼ë¡œ ë¯¼ê°í•œ ë°ì´í„°ë¥¼ íŠ¸ë ˆì´ìŠ¤ì— í¬í•¨í• ì§€ êµ¬ì„±í•©ë‹ˆë‹¤
-   [`workflow_name`][agents.run.RunConfig.workflow_name], [`trace_id`][agents.run.RunConfig.trace_id], [`group_id`][agents.run.RunConfig.group_id]: ì‹¤í–‰ì— ëŒ€í•œ íŠ¸ë ˆì´ì‹± ì›Œí¬í”Œë¡œ ì´ë¦„, íŠ¸ë ˆì´ìŠ¤ ID, íŠ¸ë ˆì´ìŠ¤ ê·¸ë£¹ ID ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ìµœì†Œí•œ `workflow_name` ì„¤ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ê·¸ë£¹ ID ëŠ” ì„ íƒ í•­ëª©ìœ¼ë¡œ ì—¬ëŸ¬ ì‹¤í–‰ì— ê±¸ì³ íŠ¸ë ˆì´ìŠ¤ë¥¼ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
-   [`trace_metadata`][agents.run.RunConfig.trace_metadata]: ëª¨ë“  íŠ¸ë ˆì´ìŠ¤ì— í¬í•¨í•  ë©”íƒ€ë°ì´í„°ì…ë‹ˆë‹¤

## ëŒ€í™”/ì±„íŒ… ìŠ¤ë ˆë“œ

ì–´ë–¤ run ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë“  í•˜ë‚˜ ì´ìƒì˜ ì—ì´ì „íŠ¸ê°€ ì‹¤í–‰ë  ìˆ˜ ìˆìœ¼ë©°(ë”°ë¼ì„œ í•˜ë‚˜ ì´ìƒì˜ LLM í˜¸ì¶œ), ì´ëŠ” ì±„íŒ… ëŒ€í™”ì—ì„œ í•˜ë‚˜ì˜ ë…¼ë¦¬ì  í„´ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:

1. ì‚¬ìš©ì í„´: ì‚¬ìš©ìê°€ í…ìŠ¤íŠ¸ ì…ë ¥
2. Runner ì‹¤í–‰: ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ê°€ LLM ì„ í˜¸ì¶œí•˜ê³  ë„êµ¬ë¥¼ ì‹¤í–‰í•œ í›„ ë‘ ë²ˆì§¸ ì—ì´ì „íŠ¸ë¡œ í•¸ë“œì˜¤í”„, ë‘ ë²ˆì§¸ ì—ì´ì „íŠ¸ê°€ ë” ë§ì€ ë„êµ¬ë¥¼ ì‹¤í–‰í•œ ë‹¤ìŒ ì¶œë ¥ì„ ìƒì„±

ì—ì´ì „íŠ¸ ì‹¤í–‰ì´ ëë‚˜ë©´ ì‚¬ìš©ìì—ê²Œ ë¬´ì—‡ì„ ë³´ì—¬ì¤„ì§€ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì—ì´ì „íŠ¸ê°€ ìƒì„±í•œ ëª¨ë“  ìƒˆ í•­ëª©ì„ ë³´ì—¬ì£¼ê±°ë‚˜ ìµœì¢… ì¶œë ¥ë§Œ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ëŠ ìª½ì´ë“ , ì‚¬ìš©ìê°€ í›„ì† ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆìœ¼ë©°, ì´ ê²½ìš° run ë©”ì„œë“œë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤.

### ìˆ˜ë™ ëŒ€í™” ê´€ë¦¬

ë‹¤ìŒ í„´ì˜ ì…ë ¥ì„ ì–»ê¸° ìœ„í•´ [`RunResultBase.to_input_list()`][agents.result.RunResultBase.to_input_list] ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ ìˆ˜ë™ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    thread_id = "thread_123"  # Example thread ID
    with trace(workflow_name="Conversation", group_id=thread_id):
        # First turn
        result = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
        print(result.final_output)
        # San Francisco

        # Second turn
        new_input = result.to_input_list() + [{"role": "user", "content": "What state is it in?"}]
        result = await Runner.run(agent, new_input)
        print(result.final_output)
        # California
```

### Sessions ë¥¼ í†µí•œ ìë™ ëŒ€í™” ê´€ë¦¬

ë” ê°„ë‹¨í•œ ì ‘ê·¼ìœ¼ë¡œ, [Sessions](sessions.md) ë¥¼ ì‚¬ìš©í•˜ì—¬ `.to_input_list()` ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì§€ ì•Šê³ ë„ ëŒ€í™” ê¸°ë¡ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from agents import Agent, Runner, SQLiteSession

async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    # Create session instance
    session = SQLiteSession("conversation_123")

    thread_id = "thread_123"  # Example thread ID
    with trace(workflow_name="Conversation", group_id=thread_id):
        # First turn
        result = await Runner.run(agent, "What city is the Golden Gate Bridge in?", session=session)
        print(result.final_output)
        # San Francisco

        # Second turn - agent automatically remembers previous context
        result = await Runner.run(agent, "What state is it in?", session=session)
        print(result.final_output)
        # California
```

Sessions ëŠ” ë‹¤ìŒì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤:

-   ê° ì‹¤í–‰ ì „ ëŒ€í™” ê¸°ë¡ì„ ê°€ì ¸ì˜´
-   ê° ì‹¤í–‰ í›„ ìƒˆ ë©”ì‹œì§€ë¥¼ ì €ì¥
-   ì„œë¡œ ë‹¤ë¥¸ ì„¸ì…˜ ID ì— ëŒ€í•´ ë³„ë„ì˜ ëŒ€í™”ë¥¼ ìœ ì§€

ìì„¸í•œ ë‚´ìš©ì€ [Sessions ë¬¸ì„œ](sessions.md) ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.


### ì„œë²„ ê´€ë¦¬ ëŒ€í™”

`to_input_list()` ë˜ëŠ” `Sessions` ë¡œ ë¡œì»¬ì—ì„œ ì²˜ë¦¬í•˜ëŠ” ëŒ€ì‹ , OpenAI ëŒ€í™” ìƒíƒœ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì„œë²„ ì¸¡ì—ì„œ ëŒ€í™” ìƒíƒœë¥¼ ê´€ë¦¬í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ê³¼ê±°ì˜ ëª¨ë“  ë©”ì‹œì§€ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ì‹œ ë³´ë‚´ì§€ ì•Šê³ ë„ ëŒ€í™” ê¸°ë¡ì„ ë³´ì¡´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [OpenAI Conversation state ê°€ì´ë“œ](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses) ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

OpenAI ëŠ” í„´ ê°„ ìƒíƒœë¥¼ ì¶”ì í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤:

#### 1. `conversation_id` ì‚¬ìš©

ë¨¼ì € OpenAI Conversations API ë¥¼ ì‚¬ìš©í•´ ëŒ€í™”ë¥¼ ìƒì„±í•˜ê³ , ì´í›„ì˜ ëª¨ë“  í˜¸ì¶œì—ì„œ í•´ë‹¹ ID ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
from agents import Agent, Runner
from openai import AsyncOpenAI

client = AsyncOpenAI()

async def main():
    # Create a server-managed conversation
    conversation = await client.conversations.create()
    conv_id = conversation.id    

    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    # First turn
    result1 = await Runner.run(agent, "What city is the Golden Gate Bridge in?", conversation_id=conv_id)
    print(result1.final_output)
    # San Francisco

    # Second turn reuses the same conversation_id
    result2 = await Runner.run(
        agent,
        "What state is it in?",
        conversation_id=conv_id,
    )
    print(result2.final_output)
    # California
```

#### 2. `previous_response_id` ì‚¬ìš©

ë˜ ë‹¤ë¥¸ ì˜µì…˜ì€ **response chaining** ìœ¼ë¡œ, ê° í„´ì´ ì´ì „ í„´ì˜ response ID ì— ëª…ì‹œì ìœ¼ë¡œ ì—°ê²°ë©ë‹ˆë‹¤.

```python
from agents import Agent, Runner

async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    # First turn
    result1 = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
    print(result1.final_output)
    # San Francisco

    # Second turn, chained to the previous response
    result2 = await Runner.run(
        agent,
        "What state is it in?",
        previous_response_id=result1.last_response_id,
    )
    print(result2.final_output)
    # California
```


## ì¥ê¸° ì‹¤í–‰ ì—ì´ì „íŠ¸ & íœ´ë¨¼ì¸ë”ë£¨í”„

Agents SDK ì˜ [Temporal](https://temporal.io/) í†µí•©ì„ ì‚¬ìš©í•˜ì—¬ íœ´ë¨¼ì¸ë”ë£¨í”„ ì‘ì—…ì„ í¬í•¨í•œ ë‚´êµ¬ì„± ìˆëŠ” ì¥ê¸° ì‹¤í–‰ ì›Œí¬í”Œë¡œë¥¼ ìš´ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¥ê¸° ì‹¤í–‰ ì‘ì—…ì„ ì™„ë£Œí•˜ê¸° ìœ„í•´ Temporal ê³¼ Agents SDK ê°€ í•¨ê»˜ ì‘ë™í•˜ëŠ” ë°ëª¨ë¥¼ [ì´ ë™ì˜ìƒ](https://www.youtube.com/watch?v=fFBZqzT4DD8)ì—ì„œ í™•ì¸í•˜ê³ , [ë¬¸ì„œëŠ” ì—¬ê¸°](https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents)ì—ì„œ í™•ì¸í•˜ì„¸ìš”.

## ì˜ˆì™¸

SDK ëŠ” íŠ¹ì • ê²½ìš°ì— ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤. ì „ì²´ ëª©ë¡ì€ [`agents.exceptions`][] ì— ìˆìŠµë‹ˆë‹¤. ê°œìš”ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

-   [`AgentsException`][agents.exceptions.AgentsException]: SDK ë‚´ì—ì„œ ë°œìƒí•˜ëŠ” ëª¨ë“  ì˜ˆì™¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë“  êµ¬ì²´ì ì¸ ì˜ˆì™¸ê°€ íŒŒìƒë˜ëŠ” ì¼ë°˜ì ì¸ íƒ€ì… ì—­í• ì„ í•©ë‹ˆë‹¤
-   [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded]: ì—ì´ì „íŠ¸ ì‹¤í–‰ì´ `max_turns` ì œí•œì„ ì´ˆê³¼í–ˆì„ ë•Œ `Runner.run`, `Runner.run_sync`, `Runner.run_streamed` ë©”ì„œë“œì—ì„œ ë°œìƒí•©ë‹ˆë‹¤. ì´ëŠ” ì—ì´ì „íŠ¸ê°€ ì§€ì •ëœ ìƒí˜¸ì‘ìš© í„´ ìˆ˜ ë‚´ì— ì‘ì—…ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤
-   [`ModelBehaviorError`][agents.exceptions.ModelBehaviorError]: ê¸°ë°˜ ëª¨ë¸(LLM) ì´ ì˜ˆê¸°ì¹˜ ì•Šê±°ë‚˜ ì˜ëª»ëœ ì¶œë ¥ì„ ìƒì„±í•  ë•Œ ë°œìƒí•©ë‹ˆë‹¤. ë‹¤ìŒì„ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
    -   ì˜ëª»ëœ JSON: íŠ¹íˆ íŠ¹ì • `output_type` ì´ ì •ì˜ëœ ê²½ìš°, ë„êµ¬ í˜¸ì¶œì´ë‚˜ ì§ì ‘ ì¶œë ¥ì—ì„œ ëª¨ë¸ì´ ì˜ëª»ëœ JSON êµ¬ì¡°ë¥¼ ì œê³µí•˜ëŠ” ê²½ìš°
    -   ì˜ˆê¸°ì¹˜ ì•Šì€ ë„êµ¬ ê´€ë ¨ ì‹¤íŒ¨: ëª¨ë¸ì´ ì˜ˆìƒí•œ ë°©ì‹ìœ¼ë¡œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ëª»í•œ ê²½ìš°
-   [`UserError`][agents.exceptions.UserError]: SDK ë¥¼ ì‚¬ìš©í•˜ëŠ” ì½”ë“œ ì‘ì„±ì(ë‹¹ì‹ ) ê°€ SDK ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œì¼°ì„ ë•Œ ë°œìƒí•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì˜ëª»ëœ ì½”ë“œ êµ¬í˜„, ì˜ëª»ëœ êµ¬ì„±, SDK API ì˜¤ìš©ìœ¼ë¡œ ì¸í•´ ë°œìƒí•©ë‹ˆë‹¤
-   [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered], [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered]: ê°ê° ì…ë ¥ ê°€ë“œë ˆì¼ ë˜ëŠ” ì¶œë ¥ ê°€ë“œë ˆì¼ì˜ ì¡°ê±´ì´ ì¶©ì¡±ë˜ì—ˆì„ ë•Œ ë°œìƒí•©ë‹ˆë‹¤. ì…ë ¥ ê°€ë“œë ˆì¼ì€ ì²˜ë¦¬ ì „ì— ë“¤ì–´ì˜¤ëŠ” ë©”ì‹œì§€ë¥¼ ê²€ì‚¬í•˜ê³ , ì¶œë ¥ ê°€ë“œë ˆì¼ì€ ì „ë‹¬ ì „ì— ì—ì´ì „íŠ¸ì˜ ìµœì¢… ì‘ë‹µì„ ê²€ì‚¬í•©ë‹ˆë‹¤

================
File: docs/ko/streaming.md
================
---
search:
  exclude: true
---
# ìŠ¤íŠ¸ë¦¬ë°

ìŠ¤íŠ¸ë¦¬ë°ì„ ì‚¬ìš©í•˜ë©´ ì—ì´ì „íŠ¸ ì‹¤í–‰ì´ ì§„í–‰ë˜ëŠ” ë™ì•ˆ ì—…ë°ì´íŠ¸ë¥¼ êµ¬ë…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ìµœì¢… ì‚¬ìš©ìì—ê²Œ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ì™€ ë¶€ë¶„ ì‘ë‹µì„ ë³´ì—¬ì¤„ ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.

ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´ [`Runner.run_streamed()`][agents.run.Runner.run_streamed]ë¥¼ í˜¸ì¶œí•˜ë©´ [`RunResultStreaming`][agents.result.RunResultStreaming]ì„ ë°›ìŠµë‹ˆë‹¤. `result.stream_events()`ë¥¼ í˜¸ì¶œí•˜ë©´ ì•„ë˜ì— ì„¤ëª…ëœ [`StreamEvent`][agents.stream_events.StreamEvent] ê°ì²´ì˜ ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¼ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì›ë¬¸ ì‘ë‹µ ì´ë²¤íŠ¸

[`RawResponsesStreamEvent`][agents.stream_events.RawResponsesStreamEvent]ëŠ” LLMì—ì„œ ì§ì ‘ ì „ë‹¬ë˜ëŠ” ì›ë¬¸ ì´ë²¤íŠ¸ì…ë‹ˆë‹¤. ì´ë“¤ì€ OpenAI Responses API í˜•ì‹ì´ë©°, ê° ì´ë²¤íŠ¸ì—ëŠ” íƒ€ì…(ì˜ˆ: `response.created`, `response.output_text.delta` ë“±)ê³¼ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. ìƒì„± ì¦‰ì‹œ ì‚¬ìš©ìì—ê²Œ ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ë ¤ëŠ” ê²½ìš° ìœ ìš©í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ ì½”ë“œëŠ” LLMì´ ìƒì„±í•œ í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

```python
import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner

async def main():
    agent = Agent(
        name="Joker",
        instructions="You are a helpful assistant.",
    )

    result = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            print(event.data.delta, end="", flush=True)


if __name__ == "__main__":
    asyncio.run(main())
```

## ì‹¤í–‰ í•­ëª© ì´ë²¤íŠ¸ì™€ ì—ì´ì „íŠ¸ ì´ë²¤íŠ¸

[`RunItemStreamEvent`][agents.stream_events.RunItemStreamEvent]ëŠ” ìƒìœ„ ìˆ˜ì¤€ì˜ ì´ë²¤íŠ¸ì…ë‹ˆë‹¤. í•­ëª©ì´ ì™„ì „íˆ ìƒì„±ë˜ì—ˆì„ ë•Œë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê° í† í°ì´ ì•„ë‹Œ "ë©”ì‹œì§€ ìƒì„±ë¨", "ë„êµ¬ ì‹¤í–‰ë¨" ë“±ì˜ ìˆ˜ì¤€ì—ì„œ ì§„í–‰ ìƒí™©ì„ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ, [`AgentUpdatedStreamEvent`][agents.stream_events.AgentUpdatedStreamEvent]ëŠ” í˜„ì¬ ì—ì´ì „íŠ¸ê°€ ë³€ê²½ë  ë•Œ(ì˜ˆ: í•¸ë“œì˜¤í”„ë¡œ ì¸í•œ ê²½ìš°) ì—…ë°ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ ì½”ë“œëŠ” ì›ë¬¸ ì´ë²¤íŠ¸ë¥¼ ë¬´ì‹œí•˜ê³  ì‚¬ìš©ìì—ê²Œ ì—…ë°ì´íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

```python
import asyncio
import random
from agents import Agent, ItemHelpers, Runner, function_tool

@function_tool
def how_many_jokes() -> int:
    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="Joker",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],
    )

    result = Runner.run_streamed(
        agent,
        input="Hello",
    )
    print("=== Run starting ===")

    async for event in result.stream_events():
        # We'll ignore the raw responses event deltas
        if event.type == "raw_response_event":
            continue
        # When the agent updates, print that
        elif event.type == "agent_updated_stream_event":
            print(f"Agent updated: {event.new_agent.name}")
            continue
        # When items are generated, print them
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}")
            else:
                pass  # Ignore other event types

    print("=== Run complete ===")


if __name__ == "__main__":
    asyncio.run(main())
```

================
File: docs/ko/tools.md
================
---
search:
  exclude: true
---
# ë„êµ¬

ë„êµ¬ëŠ” ì—ì´ì „íŠ¸ê°€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³ , ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³ , ì™¸ë¶€ APIë¥¼ í˜¸ì¶œí•˜ê³ , ì‹¬ì§€ì–´ ì»´í“¨í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë“± ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ í•©ë‹ˆë‹¤. Agents SDK ì—ëŠ” ì„¸ ê°€ì§€ í´ë˜ìŠ¤ì˜ ë„êµ¬ê°€ ìˆìŠµë‹ˆë‹¤:

- í˜¸ìŠ¤í‹°ë“œ íˆ´: ì´ëŠ” AI ëª¨ë¸ê³¼ í•¨ê»˜ LLM ì„œë²„ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤. OpenAI ëŠ” retrieval, ì›¹ ê²€ìƒ‰ ë° ì»´í“¨í„° ì‚¬ìš©ì„ í˜¸ìŠ¤í‹°ë“œ íˆ´ë¡œ ì œê³µí•©ë‹ˆë‹¤
- í•¨ìˆ˜ í˜¸ì¶œ: ì„ì˜ì˜ Python í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ë„êµ¬ë¡œì„œì˜ ì—ì´ì „íŠ¸: ì—ì´ì „íŠ¸ë¥¼ ë„êµ¬ë¡œ ì‚¬ìš©í•˜ì—¬, í•¸ë“œì˜¤í”„ ì—†ì´ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

## í˜¸ìŠ¤í‹°ë“œ íˆ´

[`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel]ì„ ì‚¬ìš©í•  ë•Œ OpenAI ëŠ” ëª‡ ê°€ì§€ ë‚´ì¥ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

- [`WebSearchTool`][agents.tool.WebSearchTool]: ì—ì´ì „íŠ¸ê°€ ì›¹ì„ ê²€ìƒ‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤
- [`FileSearchTool`][agents.tool.FileSearchTool]: OpenAI ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤
- [`ComputerTool`][agents.tool.ComputerTool]: ì»´í“¨í„° ì‚¬ìš© ì‘ì—…ì„ ìë™í™”í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤
- [`CodeInterpreterTool`][agents.tool.CodeInterpreterTool]: LLM ì´ ìƒŒë“œë°•ìŠ¤ í™˜ê²½ì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë„ë¡ í•©ë‹ˆë‹¤
- [`HostedMCPTool`][agents.tool.HostedMCPTool]: ì›ê²© MCP ì„œë²„ì˜ ë„êµ¬ë¥¼ ëª¨ë¸ì— ë…¸ì¶œí•©ë‹ˆë‹¤
- [`ImageGenerationTool`][agents.tool.ImageGenerationTool]: í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
- [`LocalShellTool`][agents.tool.LocalShellTool]: ë¡œì»¬ ë¨¸ì‹ ì—ì„œ ì…¸ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤

```python
from agents import Agent, FileSearchTool, Runner, WebSearchTool

agent = Agent(
    name="Assistant",
    tools=[
        WebSearchTool(),
        FileSearchTool(
            max_num_results=3,
            vector_store_ids=["VECTOR_STORE_ID"],
        ),
    ],
)

async def main():
    result = await Runner.run(agent, "Which coffee shop should I go to, taking into account my preferences and the weather today in SF?")
    print(result.final_output)
```

## í•¨ìˆ˜ ë„êµ¬

ì„ì˜ì˜ Python í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Agents SDK ê°€ ë„êµ¬ ì„¤ì •ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤:

- ë„êµ¬ ì´ë¦„ì€ Python í•¨ìˆ˜ ì´ë¦„ì´ ë©ë‹ˆë‹¤(ë˜ëŠ” ì´ë¦„ì„ ì§ì ‘ ì§€ì •í•  ìˆ˜ ìˆìŒ)
- ë„êµ¬ ì„¤ëª…ì€ í•¨ìˆ˜ì˜ ë„í¬ìŠ¤íŠ¸ë§ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤(ë˜ëŠ” ì„¤ëª…ì„ ì§ì ‘ ì§€ì •í•  ìˆ˜ ìˆìŒ)
- í•¨ìˆ˜ ì…ë ¥ì— ëŒ€í•œ ìŠ¤í‚¤ë§ˆëŠ” í•¨ìˆ˜ì˜ ì¸ìì—ì„œ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤
- ê° ì…ë ¥ì˜ ì„¤ëª…ì€ ë¹„í™œì„±í™”í•˜ì§€ ì•ŠëŠ” í•œ í•¨ìˆ˜ì˜ ë„í¬ìŠ¤íŠ¸ë§ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤

Pythonì˜ `inspect` ëª¨ë“ˆë¡œ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ë¥¼ ì¶”ì¶œí•˜ê³ , [`griffe`](https://mkdocstrings.github.io/griffe/) ë¡œ ë„í¬ìŠ¤íŠ¸ë§ì„ íŒŒì‹±í•˜ë©°, ìŠ¤í‚¤ë§ˆ ìƒì„±ì—ëŠ” `pydantic` ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
import json

from typing_extensions import TypedDict, Any

from agents import Agent, FunctionTool, RunContextWrapper, function_tool


class Location(TypedDict):
    lat: float
    long: float

@function_tool  # (1)!
async def fetch_weather(location: Location) -> str:
    # (2)!
    """Fetch the weather for a given location.

    Args:
        location: The location to fetch the weather for.
    """
    # In real life, we'd fetch the weather from a weather API
    return "sunny"


@function_tool(name_override="fetch_data")  # (3)!
def read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -> str:
    """Read the contents of a file.

    Args:
        path: The path to the file to read.
        directory: The directory to read the file from.
    """
    # In real life, we'd read the file from the file system
    return "<file contents>"


agent = Agent(
    name="Assistant",
    tools=[fetch_weather, read_file],  # (4)!
)

for tool in agent.tools:
    if isinstance(tool, FunctionTool):
        print(tool.name)
        print(tool.description)
        print(json.dumps(tool.params_json_schema, indent=2))
        print()

```

1. í•¨ìˆ˜ì˜ ì¸ìë¡œ ì–´ë–¤ Python íƒ€ì…ì´ë“  ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, í•¨ìˆ˜ëŠ” ë™ê¸° ë˜ëŠ” ë¹„ë™ê¸°ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤
2. ë„í¬ìŠ¤íŠ¸ë§ì´ ìˆìœ¼ë©´, ê·¸ ë‚´ìš©ìœ¼ë¡œ ì„¤ëª…ê³¼ ì¸ì ì„¤ëª…ì„ ìº¡ì²˜í•©ë‹ˆë‹¤
3. ì„ íƒì ìœ¼ë¡œ `context` ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤(ë°˜ë“œì‹œ ì²« ë²ˆì§¸ ì¸ì). ë˜í•œ ë„êµ¬ ì´ë¦„, ì„¤ëª…, ì‚¬ìš©í•  ë„í¬ìŠ¤íŠ¸ë§ ìŠ¤íƒ€ì¼ ë“± ì˜¤ë²„ë¼ì´ë“œë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
4. ë°ì½”ë ˆì´í„°ê°€ ì ìš©ëœ í•¨ìˆ˜ë¥¼ ë„êµ¬ ëª©ë¡ì— ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

??? note "ì¶œë ¥ì„ ë³´ë ¤ë©´ í¼ì¹˜ê¸°"

    ```
    fetch_weather
    Fetch the weather for a given location.
    {
    "$defs": {
      "Location": {
        "properties": {
          "lat": {
            "title": "Lat",
            "type": "number"
          },
          "long": {
            "title": "Long",
            "type": "number"
          }
        },
        "required": [
          "lat",
          "long"
        ],
        "title": "Location",
        "type": "object"
      }
    },
    "properties": {
      "location": {
        "$ref": "#/$defs/Location",
        "description": "The location to fetch the weather for."
      }
    },
    "required": [
      "location"
    ],
    "title": "fetch_weather_args",
    "type": "object"
    }

    fetch_data
    Read the contents of a file.
    {
    "properties": {
      "path": {
        "description": "The path to the file to read.",
        "title": "Path",
        "type": "string"
      },
      "directory": {
        "anyOf": [
          {
            "type": "string"
          },
          {
            "type": "null"
          }
        ],
        "default": null,
        "description": "The directory to read the file from.",
        "title": "Directory"
      }
    },
    "required": [
      "path"
    ],
    "title": "fetch_data_args",
    "type": "object"
    }
    ```

### í•¨ìˆ˜ ë„êµ¬ì—ì„œ ì´ë¯¸ì§€ ë˜ëŠ” íŒŒì¼ ë°˜í™˜

í…ìŠ¤íŠ¸ ì¶œë ¥ ì™¸ì—ë„, í•¨ìˆ˜ ë„êµ¬ì˜ ì¶œë ¥ìœ¼ë¡œ í•˜ë‚˜ ì´ìƒì˜ ì´ë¯¸ì§€ ë˜ëŠ” íŒŒì¼ì„ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë‹¤ìŒ ì¤‘ ì•„ë¬´ê±°ë‚˜ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- ì´ë¯¸ì§€: [`ToolOutputImage`][agents.tool.ToolOutputImage] (ë˜ëŠ” TypedDict ë²„ì „ì¸ [`ToolOutputImageDict`][agents.tool.ToolOutputImageDict])
- íŒŒì¼: [`ToolOutputFileContent`][agents.tool.ToolOutputFileContent] (ë˜ëŠ” TypedDict ë²„ì „ì¸ [`ToolOutputFileContentDict`][agents.tool.ToolOutputFileContentDict])
- í…ìŠ¤íŠ¸: ë¬¸ìì—´ ë˜ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ê°ì²´, ë˜ëŠ” [`ToolOutputText`][agents.tool.ToolOutputText] (ë˜ëŠ” TypedDict ë²„ì „ì¸ [`ToolOutputTextDict`][agents.tool.ToolOutputTextDict])

### ì»¤ìŠ¤í…€ í•¨ìˆ˜ ë„êµ¬

ë•Œë¡œëŠ” Python í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ì‚¬ìš©í•˜ê³  ì‹¶ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° [`FunctionTool`][agents.tool.FunctionTool]ì„ ì§ì ‘ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤:

- `name`
- `description`
- `params_json_schema` (ì¸ìì— ëŒ€í•œ JSON ìŠ¤í‚¤ë§ˆ)
- `on_invoke_tool` (ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ, [`ToolContext`][agents.tool_context.ToolContext]ì™€ JSON ë¬¸ìì—´ í˜•íƒœì˜ ì¸ìë¥¼ ë°›ì•„ë“¤ì´ë©° ë„êµ¬ ì¶œë ¥ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•´ì•¼ í•¨)

```python
from typing import Any

from pydantic import BaseModel

from agents import RunContextWrapper, FunctionTool



def do_some_work(data: str) -> str:
    return "done"


class FunctionArgs(BaseModel):
    username: str
    age: int


async def run_function(ctx: RunContextWrapper[Any], args: str) -> str:
    parsed = FunctionArgs.model_validate_json(args)
    return do_some_work(data=f"{parsed.username} is {parsed.age} years old")


tool = FunctionTool(
    name="process_user",
    description="Processes extracted user data",
    params_json_schema=FunctionArgs.model_json_schema(),
    on_invoke_tool=run_function,
)
```

### ìë™ ì¸ì ë° ë„í¬ìŠ¤íŠ¸ë§ íŒŒì‹±

ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´, ë„êµ¬ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•´ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ë¥¼ ìë™ìœ¼ë¡œ íŒŒì‹±í•˜ê³ , ë„êµ¬ ë° ê°œë³„ ì¸ì ì„¤ëª…ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ë„í¬ìŠ¤íŠ¸ë§ì„ íŒŒì‹±í•©ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ì°¸ê³  ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. ì‹œê·¸ë‹ˆì²˜ íŒŒì‹±ì€ `inspect` ëª¨ë“ˆì„ í†µí•´ ìˆ˜í–‰í•©ë‹ˆë‹¤. íƒ€ì… íŒíŠ¸ë¥¼ ì‚¬ìš©í•´ ì¸ìì˜ íƒ€ì…ì„ íŒŒì•…í•˜ê³ , ì „ì²´ ìŠ¤í‚¤ë§ˆë¥¼ í‘œí˜„í•˜ëŠ” Pydantic ëª¨ë¸ì„ ë™ì ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤. Python ê¸°ë³¸í˜•, Pydantic ëª¨ë¸, TypedDict ë“± ëŒ€ë¶€ë¶„ì˜ íƒ€ì…ì„ ì§€ì›í•©ë‹ˆë‹¤
2. ë„í¬ìŠ¤íŠ¸ë§ íŒŒì‹±ì—ëŠ” `griffe` ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì§€ì›ë˜ëŠ” ë„í¬ìŠ¤íŠ¸ë§ í˜•ì‹ì€ `google`, `sphinx`, `numpy` ì…ë‹ˆë‹¤. ë„í¬ìŠ¤íŠ¸ë§ í˜•ì‹ì€ ìë™ ê°ì§€ë¥¼ ì‹œë„í•˜ì§€ë§Œ ìµœì„ ì˜ ë…¸ë ¥ì— ê¸°ë°˜í•˜ë¯€ë¡œ, `function_tool` í˜¸ì¶œ ì‹œ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `use_docstring_info` ë¥¼ `False` ë¡œ ì„¤ì •í•˜ì—¬ ë„í¬ìŠ¤íŠ¸ë§ íŒŒì‹±ì„ ë¹„í™œì„±í™”í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤

ìŠ¤í‚¤ë§ˆ ì¶”ì¶œì„ ìœ„í•œ ì½”ë“œëŠ” [`agents.function_schema`][] ì— ìˆìŠµë‹ˆë‹¤.

## ë„êµ¬ë¡œì„œì˜ ì—ì´ì „íŠ¸

ì¼ë¶€ ì›Œí¬í”Œë¡œì—ì„œëŠ” ì œì–´ë¥¼ í•¸ë“œì˜¤í”„í•˜ì§€ ì•Šê³ , ì¤‘ì•™ ì—ì´ì „íŠ¸ê°€ íŠ¹í™”ëœ ì—ì´ì „íŠ¸ ë„¤íŠ¸ì›Œí¬ë¥¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•˜ëŠ” ê²ƒì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—ì´ì „íŠ¸ë¥¼ ë„êµ¬ë¡œ ëª¨ë¸ë§í•˜ì—¬ ì´ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from agents import Agent, Runner
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You translate the user's message to Spanish",
)

french_agent = Agent(
    name="French agent",
    instructions="You translate the user's message to French",
)

orchestrator_agent = Agent(
    name="orchestrator_agent",
    instructions=(
        "You are a translation agent. You use the tools given to you to translate."
        "If asked for multiple translations, you call the relevant tools."
    ),
    tools=[
        spanish_agent.as_tool(
            tool_name="translate_to_spanish",
            tool_description="Translate the user's message to Spanish",
        ),
        french_agent.as_tool(
            tool_name="translate_to_french",
            tool_description="Translate the user's message to French",
        ),
    ],
)

async def main():
    result = await Runner.run(orchestrator_agent, input="Say 'Hello, how are you?' in Spanish.")
    print(result.final_output)
```

### ë„êµ¬-ì—ì´ì „íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•

`agent.as_tool` í•¨ìˆ˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ ë„êµ¬ë¡œ ì‰½ê²Œ ì „í™˜í•˜ê¸° ìœ„í•œ í¸ì˜ ë©”ì„œë“œì…ë‹ˆë‹¤. ë‹¤ë§Œ ëª¨ë“  êµ¬ì„±ì„ ì§€ì›í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ `max_turns` ë¥¼ ì„¤ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³ ê¸‰ ì‚¬ìš© ì‚¬ë¡€ì—ì„œëŠ” ë„êµ¬ êµ¬í˜„ ë‚´ë¶€ì—ì„œ `Runner.run` ì„ ì§ì ‘ ì‚¬ìš©í•˜ì„¸ìš”:

```python
@function_tool
async def run_my_agent() -> str:
    """A tool that runs the agent with custom configs"""

    agent = Agent(name="My agent", instructions="...")

    result = await Runner.run(
        agent,
        input="...",
        max_turns=5,
        run_config=...
    )

    return str(result.final_output)
```

### ì»¤ìŠ¤í…€ ì¶œë ¥ ì¶”ì¶œ

ê²½ìš°ì— ë”°ë¼ ì¤‘ì•™ ì—ì´ì „íŠ¸ì— ë°˜í™˜í•˜ê¸° ì „ì— ë„êµ¬-ì—ì´ì „íŠ¸ì˜ ì¶œë ¥ì„ ìˆ˜ì •í•˜ê³  ì‹¶ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ìƒí™©ì— ìœ ìš©í•©ë‹ˆë‹¤:

- ì„œë¸Œ ì—ì´ì „íŠ¸ì˜ ëŒ€í™” ê¸°ë¡ì—ì„œ íŠ¹ì • ì •ë³´(ì˜ˆ: JSON í˜ì´ë¡œë“œ)ë¥¼ ì¶”ì¶œ
- ì—ì´ì „íŠ¸ì˜ ìµœì¢… ë‹µë³€ì„ ë³€í™˜ ë˜ëŠ” ì¬ì„œì‹í™”(ì˜ˆ: Markdown ì„ ì¼ë°˜ í…ìŠ¤íŠ¸ ë˜ëŠ” CSV ë¡œ ë³€í™˜)
- ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì´ ì—†ê±°ë‚˜ ì˜ëª»ëœ ê²½ìš° ì¶œë ¥ì„ ê²€ì¦í•˜ê±°ë‚˜ ëŒ€ì²´ ê°’ì„ ì œê³µ

ì´ë¥¼ ìœ„í•´ `as_tool` ë©”ì„œë“œì— `custom_output_extractor` ì¸ìë¥¼ ì œê³µí•˜ë©´ ë©ë‹ˆë‹¤:

```python
async def extract_json_payload(run_result: RunResult) -> str:
    # Scan the agentâ€™s outputs in reverse order until we find a JSON-like message from a tool call.
    for item in reversed(run_result.new_items):
        if isinstance(item, ToolCallOutputItem) and item.output.strip().startswith("{"):
            return item.output.strip()
    # Fallback to an empty JSON object if nothing was found
    return "{}"


json_tool = data_agent.as_tool(
    tool_name="get_data_json",
    tool_description="Run the data agent and return only its JSON payload",
    custom_output_extractor=extract_json_payload,
)
```

### ë„êµ¬ ì¡°ê±´ë¶€ í™œì„±í™”

ëŸ°íƒ€ì„ì— `is_enabled` ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ ë„êµ¬ë¥¼ ì¡°ê±´ë¶€ë¡œ í™œì„±í™”í•˜ê±°ë‚˜ ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì»¨í…ìŠ¤íŠ¸, ì‚¬ìš©ì ì„ í˜¸ë„ ë˜ëŠ” ëŸ°íƒ€ì„ ì¡°ê±´ì— ë”°ë¼ LLM ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¥¼ ë™ì ìœ¼ë¡œ í•„í„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import asyncio
from agents import Agent, AgentBase, Runner, RunContextWrapper
from pydantic import BaseModel

class LanguageContext(BaseModel):
    language_preference: str = "french_spanish"

def french_enabled(ctx: RunContextWrapper[LanguageContext], agent: AgentBase) -> bool:
    """Enable French for French+Spanish preference."""
    return ctx.context.language_preference == "french_spanish"

# Create specialized agents
spanish_agent = Agent(
    name="spanish_agent",
    instructions="You respond in Spanish. Always reply to the user's question in Spanish.",
)

french_agent = Agent(
    name="french_agent",
    instructions="You respond in French. Always reply to the user's question in French.",
)

# Create orchestrator with conditional tools
orchestrator = Agent(
    name="orchestrator",
    instructions=(
        "You are a multilingual assistant. You use the tools given to you to respond to users. "
        "You must call ALL available tools to provide responses in different languages. "
        "You never respond in languages yourself, you always use the provided tools."
    ),
    tools=[
        spanish_agent.as_tool(
            tool_name="respond_spanish",
            tool_description="Respond to the user's question in Spanish",
            is_enabled=True,  # Always enabled
        ),
        french_agent.as_tool(
            tool_name="respond_french",
            tool_description="Respond to the user's question in French",
            is_enabled=french_enabled,
        ),
    ],
)

async def main():
    context = RunContextWrapper(LanguageContext(language_preference="french_spanish"))
    result = await Runner.run(orchestrator, "How are you?", context=context.context)
    print(result.final_output)

asyncio.run(main())
```

`is_enabled` ë§¤ê°œë³€ìˆ˜ëŠ” ë‹¤ìŒì„ í—ˆìš©í•©ë‹ˆë‹¤:

- **ë¶ˆë¦¬ì–¸ ê°’**: `True`(í•­ìƒ í™œì„±) ë˜ëŠ” `False`(í•­ìƒ ë¹„í™œì„±)
- **í˜¸ì¶œ ê°€ëŠ¥í•œ í•¨ìˆ˜**: `(context, agent)` ë¥¼ ë°›ì•„ ë¶ˆë¦¬ì–¸ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
- **ë¹„ë™ê¸° í•¨ìˆ˜**: ë³µì¡í•œ ì¡°ê±´ ë¡œì§ì„ ìœ„í•œ ë¹„ë™ê¸° í•¨ìˆ˜

ë¹„í™œì„±í™”ëœ ë„êµ¬ëŠ” ëŸ°íƒ€ì„ì— LLM ì—ê²Œ ì™„ì „íˆ ìˆ¨ê²¨ì§€ë¯€ë¡œ ë‹¤ìŒì— ìœ ìš©í•©ë‹ˆë‹¤:

- ì‚¬ìš©ì ê¶Œí•œ ê¸°ë°˜ ê¸°ëŠ¥ ê²Œì´íŒ…
- í™˜ê²½ë³„ ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥ì„±(ê°œë°œ í™˜ê²½ vs ìš´ì˜ í™˜ê²½)
- ì„œë¡œ ë‹¤ë¥¸ ë„êµ¬ êµ¬ì„±ì„ A/B í…ŒìŠ¤íŠ¸
- ëŸ°íƒ€ì„ ìƒíƒœ ê¸°ë°˜ ë™ì  ë„êµ¬ í•„í„°ë§

## í•¨ìˆ˜ ë„êµ¬ì—ì„œì˜ ì˜¤ë¥˜ ì²˜ë¦¬

`@function_tool` ë¡œ í•¨ìˆ˜ ë„êµ¬ë¥¼ ìƒì„±í•  ë•Œ `failure_error_function` ì„ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë„êµ¬ í˜¸ì¶œì´ í¬ë˜ì‹œí•  ê²½ìš° LLM ì—ê²Œ ì˜¤ë¥˜ ì‘ë‹µì„ ì œê³µí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

- ê¸°ë³¸ì ìœ¼ë¡œ(ì•„ë¬´ê²ƒë„ ì „ë‹¬í•˜ì§€ ì•Šìœ¼ë©´) `default_tool_error_function` ì´ ì‹¤í–‰ë˜ì–´ LLM ì—ê²Œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŒì„ ì•Œë¦½ë‹ˆë‹¤
- ì‚¬ìš©ì ì •ì˜ ì˜¤ë¥˜ í•¨ìˆ˜ë¥¼ ì „ë‹¬í•˜ë©´ ê·¸ê²ƒì´ ëŒ€ì‹  ì‹¤í–‰ë˜ì–´ ê·¸ ì‘ë‹µì´ LLM ì—ê²Œ ì „ì†¡ë©ë‹ˆë‹¤
- ëª…ì‹œì ìœ¼ë¡œ `None` ì„ ì „ë‹¬í•˜ë©´, ëª¨ë“  ë„êµ¬ í˜¸ì¶œ ì˜¤ë¥˜ê°€ ë‹¤ì‹œ ë°œìƒí•˜ì—¬ í˜¸ì¶œ ì¸¡ì—ì„œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ì˜ëª»ëœ JSON ì„ ìƒì„±í•œ ê²½ìš° `ModelBehaviorError`, ì‚¬ìš©ì ì½”ë“œê°€ í¬ë˜ì‹œí•œ ê²½ìš° `UserError` ë“±ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤

```python
from agents import function_tool, RunContextWrapper
from typing import Any

def my_custom_error_function(context: RunContextWrapper[Any], error: Exception) -> str:
    """A custom function to provide a user-friendly error message."""
    print(f"A tool call failed with the following error: {error}")
    return "An internal server error occurred. Please try again later."

@function_tool(failure_error_function=my_custom_error_function)
def get_user_profile(user_id: str) -> str:
    """Fetches a user profile from a mock API.
     This function demonstrates a 'flaky' or failing API call.
    """
    if user_id == "user_123":
        return "User profile for user_123 successfully retrieved."
    else:
        raise ValueError(f"Could not retrieve profile for user_id: {user_id}. API returned an error.")

```

`FunctionTool` ê°ì²´ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ê²½ìš°, `on_invoke_tool` í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì˜¤ë¥˜ë¥¼ ì§ì ‘ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.

================
File: docs/ko/tracing.md
================
---
search:
  exclude: true
---
# íŠ¸ë ˆì´ì‹±

Agents SDKì—ëŠ” ê¸°ë³¸ ì œê³µ íŠ¸ë ˆì´ì‹±ì´ í¬í•¨ë˜ì–´ ìˆì–´ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ë°œìƒí•˜ëŠ” ì´ë²¤íŠ¸ì˜ í¬ê´„ì ì¸ ê¸°ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤: LLM ìƒì„±, ë„êµ¬ í˜¸ì¶œ, í•¸ë“œì˜¤í”„, ê°€ë“œë ˆì¼, ê·¸ë¦¬ê³  ì‚¬ìš©ì ì •ì˜ ì´ë²¤íŠ¸ê¹Œì§€ í¬í•¨ë©ë‹ˆë‹¤. [Traces ëŒ€ì‹œë³´ë“œ](https://platform.openai.com/traces)ë¥¼ ì‚¬ìš©í•´ ê°œë°œ ì¤‘ê³¼ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì›Œí¬í”Œë¡œë¥¼ ë””ë²„ê·¸, ì‹œê°í™”, ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

!!!note

    íŠ¸ë ˆì´ì‹±ì€ ê¸°ë³¸ì ìœ¼ë¡œ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íŠ¸ë ˆì´ì‹±ì„ ë¹„í™œì„±í™”í•˜ëŠ” ë°©ë²•ì€ ë‘ ê°€ì§€ì…ë‹ˆë‹¤:

    1. í™˜ê²½ ë³€ìˆ˜ `OPENAI_AGENTS_DISABLE_TRACING=1` ì„ ì„¤ì •í•´ ì „ì—­ì ìœ¼ë¡œ íŠ¸ë ˆì´ì‹±ì„ ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    2. [`agents.run.RunConfig.tracing_disabled`][] ë¥¼ `True` ë¡œ ì„¤ì •í•´ ë‹¨ì¼ ì‹¤í–‰ì— ëŒ€í•´ íŠ¸ë ˆì´ì‹±ì„ ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

***OpenAI APIë¥¼ ì‚¬ìš©í•˜ëŠ” Zero Data Retention (ZDR) ì •ì±… í•˜ì˜ ì¡°ì§ì€ íŠ¸ë ˆì´ì‹±ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.***

## íŠ¸ë ˆì´ìŠ¤ì™€ ìŠ¤íŒ¬

-   **íŠ¸ë ˆì´ìŠ¤(Traces)** ëŠ” "ì›Œí¬í”Œë¡œ"ì˜ ë‹¨ì¼ ì—”ë“œ íˆ¬ ì—”ë“œ ì‘ì—…ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìŠ¤íŒ¬ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. íŠ¸ë ˆì´ìŠ¤ì—ëŠ” ë‹¤ìŒ ì†ì„±ì´ ìˆìŠµë‹ˆë‹¤:
    -   `workflow_name`: ë…¼ë¦¬ì  ì›Œí¬í”Œë¡œ ë˜ëŠ” ì•±ì…ë‹ˆë‹¤. ì˜ˆ: "Code generation" ë˜ëŠ” "Customer service"
    -   `trace_id`: íŠ¸ë ˆì´ìŠ¤ì˜ ê³ ìœ  IDì…ë‹ˆë‹¤. ì „ë‹¬í•˜ì§€ ì•Šìœ¼ë©´ ìë™ ìƒì„±ë©ë‹ˆë‹¤. í˜•ì‹ì€ `trace_<32_alphanumeric>` ì´ì–´ì•¼ í•©ë‹ˆë‹¤
    -   `group_id`: ì„ íƒì  ê·¸ë£¹ IDë¡œ, ë™ì¼í•œ ëŒ€í™”ì—ì„œ ìƒì„±ëœ ì—¬ëŸ¬ íŠ¸ë ˆì´ìŠ¤ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì±„íŒ… ìŠ¤ë ˆë“œ IDë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    -   `disabled`: Trueì´ë©´ íŠ¸ë ˆì´ìŠ¤ê°€ ê¸°ë¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
    -   `metadata`: íŠ¸ë ˆì´ìŠ¤ì— ëŒ€í•œ ì„ íƒì  ë©”íƒ€ë°ì´í„°
-   **ìŠ¤íŒ¬(Spans)** ì€ ì‹œì‘ê³¼ ì¢…ë£Œ ì‹œê°„ì´ ìˆëŠ” ì‘ì—…ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìŠ¤íŒ¬ì—ëŠ” ë‹¤ìŒì´ ìˆìŠµë‹ˆë‹¤:
    -   `started_at` ë° `ended_at` íƒ€ì„ìŠ¤íƒ¬í”„
    -   `trace_id`: ì†í•œ íŠ¸ë ˆì´ìŠ¤ë¥¼ ë‚˜íƒ€ëƒ„
    -   `parent_id`: ì´ ìŠ¤íŒ¬ì˜ ë¶€ëª¨ ìŠ¤íŒ¬ì„ ê°€ë¦¬í‚´(ìˆëŠ” ê²½ìš°)
    -   `span_data`: ìŠ¤íŒ¬ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤. ì˜ˆ: `AgentSpanData` ëŠ” ì—ì´ì „íŠ¸ ì •ë³´, `GenerationSpanData` ëŠ” LLM ìƒì„±ì— ëŒ€í•œ ì •ë³´ë¥¼ í¬í•¨

## ê¸°ë³¸ íŠ¸ë ˆì´ì‹±

ê¸°ë³¸ì ìœ¼ë¡œ, SDKëŠ” ë‹¤ìŒì„ íŠ¸ë ˆì´ì‹±í•©ë‹ˆë‹¤:

-   ì „ì²´ `Runner.{run, run_sync, run_streamed}()` ê°€ `trace()` ë¡œ ë˜í•‘ë¨
-   ì—ì´ì „íŠ¸ê°€ ì‹¤í–‰ë  ë•Œë§ˆë‹¤ `agent_span()` ìœ¼ë¡œ ë˜í•‘ë¨
-   LLM ìƒì„±ì€ `generation_span()` ìœ¼ë¡œ ë˜í•‘ë¨
-   í•¨ìˆ˜ ë„êµ¬ í˜¸ì¶œì€ ê°ê° `function_span()` ìœ¼ë¡œ ë˜í•‘ë¨
-   ê°€ë“œë ˆì¼ì€ `guardrail_span()` ìœ¼ë¡œ ë˜í•‘ë¨
-   í•¸ë“œì˜¤í”„ëŠ” `handoff_span()` ìœ¼ë¡œ ë˜í•‘ë¨
-   ì˜¤ë””ì˜¤ ì…ë ¥(ìŒì„±â†’í…ìŠ¤íŠ¸)ì€ `transcription_span()` ìœ¼ë¡œ ë˜í•‘ë¨
-   ì˜¤ë””ì˜¤ ì¶œë ¥(í…ìŠ¤íŠ¸â†’ìŒì„±)ì€ `speech_span()` ìœ¼ë¡œ ë˜í•‘ë¨
-   ê´€ë ¨ ì˜¤ë””ì˜¤ ìŠ¤íŒ¬ì€ `speech_group_span()` ì•„ë˜ì— ë¶€ëª¨ë¡œ ë¬¶ì¼ ìˆ˜ ìˆìŒ

ê¸°ë³¸ì ìœ¼ë¡œ íŠ¸ë ˆì´ìŠ¤ì˜ ì´ë¦„ì€ "Agent workflow"ì…ë‹ˆë‹¤. `trace` ë¥¼ ì‚¬ìš©í•˜ë©´ ì´ ì´ë¦„ì„ ì„¤ì •í•  ìˆ˜ ìˆìœ¼ë©°, ë˜ëŠ” [`RunConfig`][agents.run.RunConfig] ë¡œ ì´ë¦„ê³¼ ê¸°íƒ€ ì†ì„±ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë˜í•œ, [ì‚¬ìš©ì ì •ì˜ íŠ¸ë ˆì´ìŠ¤ í”„ë¡œì„¸ì„œ](#custom-tracing-processors)ë¥¼ ì„¤ì •í•´ íŠ¸ë ˆì´ìŠ¤ë¥¼ ë‹¤ë¥¸ ëª©ì ì§€ë¡œ ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ëŒ€ì²´ ë˜ëŠ” ë³´ì¡° ëª©ì ì§€ë¡œ).

## ìƒìœ„ ë ˆë²¨ íŠ¸ë ˆì´ìŠ¤

ë•Œë¡œëŠ” ì—¬ëŸ¬ ë²ˆì˜ `run()` í˜¸ì¶œì´ ë‹¨ì¼ íŠ¸ë ˆì´ìŠ¤ì˜ ì¼ë¶€ê°€ ë˜ë„ë¡ í•˜ê³  ì‹¶ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „ì²´ ì½”ë“œë¥¼ `trace()` ë¡œ ë˜í•‘í•˜ë©´ ë©ë‹ˆë‹¤.

```python
from agents import Agent, Runner, trace

async def main():
    agent = Agent(name="Joke generator", instructions="Tell funny jokes.")

    with trace("Joke workflow"): # (1)!
        first_result = await Runner.run(agent, "Tell me a joke")
        second_result = await Runner.run(agent, f"Rate this joke: {first_result.final_output}")
        print(f"Joke: {first_result.final_output}")
        print(f"Rating: {second_result.final_output}")
```

1. `with trace()` ë¡œ `Runner.run` ì— ëŒ€í•œ ë‘ í˜¸ì¶œì„ ë˜í•‘í–ˆê¸° ë•Œë¬¸ì—, ê°œë³„ ì‹¤í–‰ì€ ë‘ ê°œì˜ íŠ¸ë ˆì´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ëŒ€ì‹  ì „ì²´ íŠ¸ë ˆì´ìŠ¤ì˜ ì¼ë¶€ê°€ ë©ë‹ˆë‹¤.

## íŠ¸ë ˆì´ìŠ¤ ìƒì„±

[`trace()`][agents.tracing.trace] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ íŠ¸ë ˆì´ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¸ë ˆì´ìŠ¤ëŠ” ì‹œì‘ê³¼ ì¢…ë£Œê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì€ ë‘ ê°€ì§€ì…ë‹ˆë‹¤:

1. ê¶Œì¥: íŠ¸ë ˆì´ìŠ¤ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜ˆ: `with trace(...) as my_trace`. ì´ë ‡ê²Œ í•˜ë©´ ì ì ˆí•œ ì‹œì ì— íŠ¸ë ˆì´ìŠ¤ê°€ ìë™ìœ¼ë¡œ ì‹œì‘ë˜ê³  ì¢…ë£Œë©ë‹ˆë‹¤
2. ìˆ˜ë™ìœ¼ë¡œ [`trace.start()`][agents.tracing.Trace.start] ì™€ [`trace.finish()`][agents.tracing.Trace.finish] ë¥¼ í˜¸ì¶œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤

í˜„ì¬ íŠ¸ë ˆì´ìŠ¤ëŠ” íŒŒì´ì¬ [`contextvar`](https://docs.python.org/3/library/contextvars.html) ë¥¼ í†µí•´ ì¶”ì ë©ë‹ˆë‹¤. ì´ëŠ” ë™ì‹œì„± í™˜ê²½ì—ì„œë„ ìë™ìœ¼ë¡œ ë™ì‘í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. íŠ¸ë ˆì´ìŠ¤ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‹œì‘/ì¢…ë£Œí•˜ëŠ” ê²½ìš°, í˜„ì¬ íŠ¸ë ˆì´ìŠ¤ë¥¼ ê°±ì‹ í•˜ë ¤ë©´ `start()`/`finish()` ì— `mark_as_current` ë° `reset_current` ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.

## ìŠ¤íŒ¬ ìƒì„±

ì—¬ëŸ¬ [`*_span()`][agents.tracing.create] ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ ìŠ¤íŒ¬ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ìŠ¤íŒ¬ì„ ìˆ˜ë™ìœ¼ë¡œ ìƒì„±í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš©ì ì •ì˜ ìŠ¤íŒ¬ ì •ë³´ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ [`custom_span()`][agents.tracing.custom_span] í•¨ìˆ˜ê°€ ì œê³µë©ë‹ˆë‹¤.

ìŠ¤íŒ¬ì€ ìë™ìœ¼ë¡œ í˜„ì¬ íŠ¸ë ˆì´ìŠ¤ì˜ ì¼ë¶€ê°€ ë˜ë©°, íŒŒì´ì¬ [`contextvar`](https://docs.python.org/3/library/contextvars.html) ë¡œ ì¶”ì ë˜ëŠ” ê°€ì¥ ê°€ê¹Œìš´ í˜„ì¬ ìŠ¤íŒ¬ ì•„ë˜ì— ì¤‘ì²©ë©ë‹ˆë‹¤.

## ë¯¼ê°í•œ ë°ì´í„°

ì¼ë¶€ ìŠ¤íŒ¬ì€ ì ì¬ì ìœ¼ë¡œ ë¯¼ê°í•œ ë°ì´í„°ë¥¼ ìº¡ì²˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

`generation_span()` ì€ LLM ìƒì„±ì˜ ì…ë ¥/ì¶œë ¥ì„ ì €ì¥í•˜ê³ , `function_span()` ì€ í•¨ìˆ˜ í˜¸ì¶œì˜ ì…ë ¥/ì¶œë ¥ì„ ì €ì¥í•©ë‹ˆë‹¤. ë¯¼ê°í•œ ë°ì´í„°ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ [`RunConfig.trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data] ë¥¼ í†µí•´ í•´ë‹¹ ë°ì´í„° ìº¡ì²˜ë¥¼ ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë§ˆì°¬ê°€ì§€ë¡œ, ì˜¤ë””ì˜¤ ìŠ¤íŒ¬ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì…ë ¥ ë° ì¶œë ¥ ì˜¤ë””ì˜¤ì— ëŒ€í•´ base64 ì¸ì½”ë”©ëœ PCM ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. [`VoicePipelineConfig.trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data] ë¥¼ êµ¬ì„±í•´ ì´ ì˜¤ë””ì˜¤ ë°ì´í„° ìº¡ì²˜ë¥¼ ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì‚¬ìš©ì ì •ì˜ íŠ¸ë ˆì´ì‹± í”„ë¡œì„¸ì„œ

íŠ¸ë ˆì´ì‹±ì˜ ìƒìœ„ ìˆ˜ì¤€ ì•„í‚¤í…ì²˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

-   ì´ˆê¸°í™” ì‹œ, íŠ¸ë ˆì´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•˜ëŠ” ì „ì—­ [`TraceProvider`][agents.tracing.setup.TraceProvider] ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
-   `TraceProvider` ë¥¼ [`BatchTraceProcessor`][agents.tracing.processors.BatchTraceProcessor] ë¡œ êµ¬ì„±í•˜ê³ , ì´ëŠ” íŠ¸ë ˆì´ìŠ¤/ìŠ¤íŒ¬ì„ ë°°ì¹˜ë¡œ [`BackendSpanExporter`][agents.tracing.processors.BackendSpanExporter] ì— ì „ì†¡í•˜ë©°, í•´ë‹¹ Exporter ê°€ ìŠ¤íŒ¬ê³¼ íŠ¸ë ˆì´ìŠ¤ë¥¼ OpenAI ë°±ì—”ë“œë¡œ ë°°ì¹˜ ì „ì†¡í•©ë‹ˆë‹¤

ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©ì ì •ì˜í•˜ì—¬ ëŒ€ì²´ ë˜ëŠ” ì¶”ê°€ ë°±ì—”ë“œë¡œ íŠ¸ë ˆì´ìŠ¤ë¥¼ ì „ì†¡í•˜ê±°ë‚˜ Exporter ë™ì‘ì„ ìˆ˜ì •í•˜ë ¤ë©´ ë‘ ê°€ì§€ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤:

1. [`add_trace_processor()`][agents.tracing.add_trace_processor] ë¥¼ ì‚¬ìš©í•˜ë©´ íŠ¸ë ˆì´ìŠ¤ì™€ ìŠ¤íŒ¬ì´ ì¤€ë¹„ë  ë•Œ ì´ë¥¼ ìˆ˜ì‹ í•˜ëŠ” **ì¶”ê°€** íŠ¸ë ˆì´ìŠ¤ í”„ë¡œì„¸ì„œë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ OpenAI ë°±ì—”ë“œë¡œ íŠ¸ë ˆì´ìŠ¤ë¥¼ ì „ì†¡í•˜ëŠ” ê²ƒì— ë”í•´ ìì²´ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
2. [`set_trace_processors()`][agents.tracing.set_trace_processors] ë¥¼ ì‚¬ìš©í•˜ë©´ ê¸°ë³¸ í”„ë¡œì„¸ì„œë¥¼ ìì‹ ì˜ íŠ¸ë ˆì´ìŠ¤ í”„ë¡œì„¸ì„œë¡œ **êµì²´** í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° OpenAI ë°±ì—”ë“œë¡œ íŠ¸ë ˆì´ìŠ¤ê°€ ì „ì†¡ë˜ì§€ ì•Šìœ¼ë©°, ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” `TracingProcessor` ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤

## ë¹„ OpenAI ëª¨ë¸ê³¼ì˜ íŠ¸ë ˆì´ì‹±

OpenAIì˜ API í‚¤ë¥¼ ë¹„ OpenAI ëª¨ë¸ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ íŠ¸ë ˆì´ì‹±ì„ ë¹„í™œì„±í™”í•˜ì§€ ì•Šê³ ë„ OpenAI Traces ëŒ€ì‹œë³´ë“œì—ì„œ ë¬´ë£Œ íŠ¸ë ˆì´ì‹±ì„ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import os
from agents import set_tracing_export_api_key, Agent, Runner
from agents.extensions.models.litellm_model import LitellmModel

tracing_api_key = os.environ["OPENAI_API_KEY"]
set_tracing_export_api_key(tracing_api_key)

model = LitellmModel(
    model="your-model-name",
    api_key="your-api-key",
)

agent = Agent(
    name="Assistant",
    model=model,
)
```

## ì°¸ê³ 
- OpenAI Traces ëŒ€ì‹œë³´ë“œì—ì„œ ë¬´ë£Œ íŠ¸ë ˆì´ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”

## ì™¸ë¶€ íŠ¸ë ˆì´ì‹± í”„ë¡œì„¸ì„œ ëª©ë¡

-   [Weights & Biases](https://weave-docs.wandb.ai/guides/integrations/openai_agents)
-   [Arize-Phoenix](https://docs.arize.com/phoenix/tracing/integrations-tracing/openai-agents-sdk)
-   [Future AGI](https://docs.futureagi.com/future-agi/products/observability/auto-instrumentation/openai_agents)
-   [MLflow (self-hosted/OSS)](https://mlflow.org/docs/latest/tracing/integrations/openai-agent)
-   [MLflow (Databricks hosted)](https://docs.databricks.com/aws/en/mlflow/mlflow-tracing#-automatic-tracing)
-   [Braintrust](https://braintrust.dev/docs/guides/traces/integrations#openai-agents-sdk)
-   [Pydantic Logfire](https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents)
-   [AgentOps](https://docs.agentops.ai/v1/integrations/agentssdk)
-   [Scorecard](https://docs.scorecard.io/docs/documentation/features/tracing#openai-agents-sdk-integration)
-   [Keywords AI](https://docs.keywordsai.co/integration/development-frameworks/openai-agent)
-   [LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
-   [Maxim AI](https://www.getmaxim.ai/docs/observe/integrations/openai-agents-sdk)
-   [Comet Opik](https://www.comet.com/docs/opik/tracing/integrations/openai_agents)
-   [Langfuse](https://langfuse.com/docs/integrations/openaiagentssdk/openai-agents)
-   [Langtrace](https://docs.langtrace.ai/supported-integrations/llm-frameworks/openai-agents-sdk)
-   [Okahu-Monocle](https://github.com/monocle2ai/monocle)
-   [Galileo](https://v2docs.galileo.ai/integrations/openai-agent-integration#openai-agent-integration)
-   [Portkey AI](https://portkey.ai/docs/integrations/agents/openai-agents)
-   [LangDB AI](https://docs.langdb.ai/getting-started/working-with-agent-frameworks/working-with-openai-agents-sdk)
-   [Agenta](https://docs.agenta.ai/observability/integrations/openai-agents)

================
File: docs/ko/usage.md
================
---
search:
  exclude: true
---
# ì‚¬ìš©ëŸ‰

Agents SDKëŠ” ê° ì‹¤í–‰(run)ì˜ í† í° ì‚¬ìš©ëŸ‰ì„ ìë™ìœ¼ë¡œ ì¶”ì í•©ë‹ˆë‹¤. ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì ‘ê·¼í•˜ì—¬ ë¹„ìš© ëª¨ë‹ˆí„°ë§, í•œë„ ì ìš© ë˜ëŠ” ë¶„ì„ ê¸°ë¡ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì¶”ì  í•­ëª©

- **requests**: ìˆ˜í–‰ëœ LLM API í˜¸ì¶œ ìˆ˜
- **input_tokens**: ì „ì†¡ëœ ì…ë ¥ í† í° ì´í•©
- **output_tokens**: ìˆ˜ì‹ ëœ ì¶œë ¥ í† í° ì´í•©
- **total_tokens**: ì…ë ¥ + ì¶œë ¥
- **details**:
  - `input_tokens_details.cached_tokens`
  - `output_tokens_details.reasoning_tokens`

## ì‹¤í–‰ì—ì„œ ì‚¬ìš©ëŸ‰ ì ‘ê·¼

`Runner.run(...)` ì´í›„ì—ëŠ” `result.context_wrapper.usage`ë¥¼ í†µí•´ ì‚¬ìš©ëŸ‰ì— ì ‘ê·¼í•©ë‹ˆë‹¤.

```python
result = await Runner.run(agent, "What's the weather in Tokyo?")
usage = result.context_wrapper.usage

print("Requests:", usage.requests)
print("Input tokens:", usage.input_tokens)
print("Output tokens:", usage.output_tokens)
print("Total tokens:", usage.total_tokens)
```

ì‚¬ìš©ëŸ‰ì€ ì‹¤í–‰ ì¤‘ ë°œìƒí•œ ëª¨ë“  ëª¨ë¸ í˜¸ì¶œ(ë„êµ¬ í˜¸ì¶œ ë° í•¸ë“œì˜¤í”„ í¬í•¨)ì„ í•©ì‚°í•´ ì§‘ê³„ë©ë‹ˆë‹¤.

### LiteLLM ëª¨ë¸ì—ì„œ ì‚¬ìš©ëŸ‰ í™œì„±í™”

LiteLLM ê³µê¸‰ìëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ëŸ‰ ë©”íŠ¸ë¦­ì„ ë³´ê³ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. [`LitellmModel`](models/litellm.md)ì„ ì‚¬ìš©í•  ë•Œ, ì—ì´ì „íŠ¸ì— `ModelSettings(include_usage=True)`ë¥¼ ì „ë‹¬í•˜ë©´ LiteLLM ì‘ë‹µì´ `result.context_wrapper.usage`ë¥¼ ì±„ìš°ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from agents import Agent, ModelSettings, Runner
from agents.extensions.models.litellm_model import LitellmModel

agent = Agent(
    name="Assistant",
    model=LitellmModel(model="your/model", api_key="..."),
    model_settings=ModelSettings(include_usage=True),
)

result = await Runner.run(agent, "What's the weather in Tokyo?")
print(result.context_wrapper.usage.total_tokens)
```

## ì„¸ì…˜ì—ì„œ ì‚¬ìš©ëŸ‰ ì ‘ê·¼

`Session`(ì˜ˆ: `SQLiteSession`)ì„ ì‚¬ìš©í•  ë•ŒëŠ” `Runner.run(...)`ì˜ ê° í˜¸ì¶œì´ í•´ë‹¹ ì‹¤í–‰ì— ëŒ€í•œ ì‚¬ìš©ëŸ‰ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì„¸ì…˜ì€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•˜ì§€ë§Œ, ê° ì‹¤í–‰ì˜ ì‚¬ìš©ëŸ‰ì€ ì„œë¡œ ë…ë¦½ì ì…ë‹ˆë‹¤.

```python
session = SQLiteSession("my_conversation")

first = await Runner.run(agent, "Hi!", session=session)
print(first.context_wrapper.usage.total_tokens)  # Usage for first run

second = await Runner.run(agent, "Can you elaborate?", session=session)
print(second.context_wrapper.usage.total_tokens)  # Usage for second run
```

ì„¸ì…˜ì€ ì‹¤í–‰ ê°„ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ì¡´í•˜ì§€ë§Œ, ê° `Runner.run()` í˜¸ì¶œì´ ë°˜í™˜í•˜ëŠ” ì‚¬ìš©ëŸ‰ ë©”íŠ¸ë¦­ì€ í•´ë‹¹ ì‹¤í–‰ë§Œì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì„¸ì…˜ì—ì„œëŠ” ì´ì „ ë©”ì‹œì§€ê°€ ê° ì‹¤í–‰ì— ì…ë ¥ìœ¼ë¡œ ë‹¤ì‹œ ì œê³µë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì´í›„ í„´ì˜ ì…ë ¥ í† í° ìˆ˜ì— ì˜í–¥ì„ ì¤ë‹ˆë‹¤.

## í›…ì—ì„œ ì‚¬ìš©ëŸ‰ í™œìš©

`RunHooks`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ê° í›…ì— ì „ë‹¬ë˜ëŠ” `context` ê°ì²´ì— `usage`ê°€ í¬í•¨ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìˆ˜ëª… ì£¼ê¸°ì˜ í•µì‹¬ ì‹œì ì—ì„œ ì‚¬ìš©ëŸ‰ì„ ë¡œê¹…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
class MyHooks(RunHooks):
    async def on_agent_end(self, context: RunContextWrapper, agent: Agent, output: Any) -> None:
        u = context.usage
        print(f"{agent.name} â†’ {u.requests} requests, {u.total_tokens} total tokens")
```

## API ì°¸ì¡°

ìì„¸í•œ API ë¬¸ì„œëŠ” ë‹¤ìŒì„ ì°¸ì¡°í•˜ì„¸ìš”.

-   [`Usage`][agents.usage.Usage] - ì‚¬ìš©ëŸ‰ ì¶”ì  ë°ì´í„° êµ¬ì¡°
-   [`RunContextWrapper`][agents.run.RunContextWrapper] - ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©ëŸ‰ì— ì ‘ê·¼
-   [`RunHooks`][agents.run.RunHooks] - ì‚¬ìš©ëŸ‰ ì¶”ì  ìˆ˜ëª… ì£¼ê¸°ì— í›… ì—°ê²°

================
File: docs/ko/visualization.md
================
---
search:
  exclude: true
---
# ì—ì´ì „íŠ¸ ì‹œê°í™”

ì—ì´ì „íŠ¸ ì‹œê°í™”ëŠ” **Graphviz**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ì™€ ê·¸ ê´€ê³„ë¥¼ êµ¬ì¡°í™”ëœ ê·¸ë˜í”½ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ ë‚´ì—ì„œ ì—ì´ì „íŠ¸, ë„êµ¬, í•¸ë“œì˜¤í”„ê°€ ì–´ë–»ê²Œ ìƒí˜¸ì‘ìš©í•˜ëŠ”ì§€ ì´í•´í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.

## ì„¤ì¹˜

ì„ íƒì  `viz` ì¢…ì†ì„± ê·¸ë£¹ì„ ì„¤ì¹˜í•˜ì„¸ìš”:

```bash
pip install "openai-agents[viz]"
```

## ê·¸ë˜í”„ ìƒì„±

`draw_graph` í•¨ìˆ˜ë¡œ ì—ì´ì „íŠ¸ ì‹œê°í™”ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìœ í–¥ ê·¸ë˜í”„ë¥¼ ë§Œë“­ë‹ˆë‹¤:

- **ì—ì´ì „íŠ¸**ëŠ” ë…¸ë€ìƒ‰ ìƒìë¡œ í‘œì‹œë©ë‹ˆë‹¤
- **MCP ì„œë²„**ëŠ” íšŒìƒ‰ ìƒìë¡œ í‘œì‹œë©ë‹ˆë‹¤
- **ë„êµ¬**ëŠ” ì´ˆë¡ìƒ‰ íƒ€ì›ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤
- **í•¸ë“œì˜¤í”„**ëŠ” í•œ ì—ì´ì „íŠ¸ì—ì„œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë¡œ í–¥í•˜ëŠ” ê°„ì„ ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤

### ì‚¬ìš© ì˜ˆì‹œ

```python
import os

from agents import Agent, function_tool
from agents.mcp.server import MCPServerStdio
from agents.extensions.visualization import draw_graph

@function_tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
)

current_dir = os.path.dirname(os.path.abspath(__file__))
samples_dir = os.path.join(current_dir, "sample_files")
mcp_server = MCPServerStdio(
    name="Filesystem Server, via npx",
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", samples_dir],
    },
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    tools=[get_weather],
    mcp_servers=[mcp_server],
)

draw_graph(triage_agent)
```

![Agent Graph](../assets/images/graph.png)

ì´ëŠ” **triage agent**ì˜ êµ¬ì¡°ì™€ í•˜ìœ„ ì—ì´ì „íŠ¸ ë° ë„êµ¬ì™€ì˜ ì—°ê²°ì„ ì‹œê°ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

## ì‹œê°í™” ì´í•´

ìƒì„±ëœ ê·¸ë˜í”„ì—ëŠ” ë‹¤ìŒì´ í¬í•¨ë©ë‹ˆë‹¤:

- ì§„ì…ì ì„ ë‚˜íƒ€ë‚´ëŠ” **ì‹œì‘ ë…¸ë“œ** (`__start__`)
- ë…¸ë€ìƒ‰ ì±„ìš°ê¸°ì˜ **ì§ì‚¬ê°í˜•**ìœ¼ë¡œ í‘œì‹œëœ ì—ì´ì „íŠ¸
- ì´ˆë¡ìƒ‰ ì±„ìš°ê¸°ì˜ **íƒ€ì›**ìœ¼ë¡œ í‘œì‹œëœ ë„êµ¬
- íšŒìƒ‰ ì±„ìš°ê¸°ì˜ **ì§ì‚¬ê°í˜•**ìœ¼ë¡œ í‘œì‹œëœ MCP ì„œë²„
- ìƒí˜¸ì‘ìš©ì„ ë‚˜íƒ€ë‚´ëŠ” ìœ í–¥ ê°„ì„ :
  - ì—ì´ì „íŠ¸ ê°„ í•¸ë“œì˜¤í”„ëŠ” **ì‹¤ì„  í™”ì‚´í‘œ**
  - ë„êµ¬ í˜¸ì¶œì€ **ì ì„  í™”ì‚´í‘œ**
  - MCP ì„œë²„ í˜¸ì¶œì€ **íŒŒì„  í™”ì‚´í‘œ**
- ì‹¤í–‰ ì¢…ë£Œ ì§€ì ì„ ë‚˜íƒ€ë‚´ëŠ” **ì¢…ë£Œ ë…¸ë“œ** (`__end__`)

**ì°¸ê³ :** MCP ì„œë²„ëŠ” ìµœì‹  ë²„ì „ì˜
`agents` íŒ¨í‚¤ì§€ì—ì„œ ë Œë”ë§ë©ë‹ˆë‹¤ (**v0.2.8**ì—ì„œ í™•ì¸ë¨). ì‹œê°í™”ì—ì„œ MCP ìƒìê°€ ë³´ì´ì§€ ì•Šìœ¼ë©´ ìµœì‹  ë¦´ë¦¬ìŠ¤ë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ì„¸ìš”.

## ê·¸ë˜í”„ ì‚¬ìš©ì ì§€ì •

### ê·¸ë˜í”„ í‘œì‹œ
ê¸°ë³¸ì ìœ¼ë¡œ `draw_graph`ëŠ” ê·¸ë˜í”„ë¥¼ ì¸ë¼ì¸ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤. ê·¸ë˜í”„ë¥¼ ë³„ë„ ì°½ì— í‘œì‹œí•˜ë ¤ë©´ ë‹¤ìŒì„ ì‘ì„±í•˜ì„¸ìš”:

```python
draw_graph(triage_agent).view()
```

### ê·¸ë˜í”„ ì €ì¥
ê¸°ë³¸ì ìœ¼ë¡œ `draw_graph`ëŠ” ê·¸ë˜í”„ë¥¼ ì¸ë¼ì¸ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤. íŒŒì¼ë¡œ ì €ì¥í•˜ë ¤ë©´ íŒŒì¼ ì´ë¦„ì„ ì§€ì •í•˜ì„¸ìš”:

```python
draw_graph(triage_agent, filename="agent_graph")
```

ê·¸ëŸ¬ë©´ ì‘ì—… ë””ë ‰í„°ë¦¬ì— `agent_graph.png`ê°€ ìƒì„±ë©ë‹ˆë‹¤.

================
File: docs/models/index.md
================
# Models

The Agents SDK comes with out-of-the-box support for OpenAI models in two flavors:

-   **Recommended**: the [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel], which calls OpenAI APIs using the new [Responses API](https://platform.openai.com/docs/api-reference/responses).
-   The [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel], which calls OpenAI APIs using the [Chat Completions API](https://platform.openai.com/docs/api-reference/chat).

## OpenAI models

When you don't specify a model when initializing an `Agent`, the default model will be used. The default is currently [`gpt-4.1`](https://platform.openai.com/docs/models/gpt-4.1), which offers a strong balance of predictability for agentic workflows and low latency.

If you want to switch to other models like [`gpt-5`](https://platform.openai.com/docs/models/gpt-5), follow the steps in the next section.

### Default OpenAI model

If you want to consistently use a specific model for all agents that do not set a custom model, set the `OPENAI_DEFAULT_MODEL` environment variable before running your agents.

```bash
export OPENAI_DEFAULT_MODEL=gpt-5
python3 my_awesome_agent.py
```

#### GPT-5 models

When you use any of GPT-5's reasoning models ([`gpt-5`](https://platform.openai.com/docs/models/gpt-5), [`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini), or [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano)) this way, the SDK applies sensible `ModelSettings` by default. Specifically, it sets both `reasoning.effort` and `verbosity` to `"low"`. If you want to build these settings yourself, call `agents.models.get_default_model_settings("gpt-5")`.

For lower latency or specific requirements, you can choose a different model and settings. To adjust the reasoning effort for the default model, pass your own `ModelSettings`:

```python
from openai.types.shared import Reasoning
from agents import Agent, ModelSettings

my_agent = Agent(
    name="My Agent",
    instructions="You're a helpful agent.",
    model_settings=ModelSettings(reasoning=Reasoning(effort="minimal"), verbosity="low")
    # If OPENAI_DEFAULT_MODEL=gpt-5 is set, passing only model_settings works.
    # It's also fine to pass a GPT-5 model name explicitly:
    # model="gpt-5",
)
```

Specifically for lower latency, using either [`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini) or [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano) model with `reasoning.effort="minimal"` will often return responses faster than the default settings. However, some built-in tools (such as file search and image generation) in Responses API do not support `"minimal"` reasoning effort, which is why this Agents SDK defaults to `"low"`.

#### Non-GPT-5 models

If you pass a nonâ€“GPT-5 model name without custom `model_settings`, the SDK reverts to generic `ModelSettings` compatible with any model.

## Non-OpenAI models

You can use most other non-OpenAI models via the [LiteLLM integration](./litellm.md). First, install the litellm dependency group:

```bash
pip install "openai-agents[litellm]"
```

Then, use any of the [supported models](https://docs.litellm.ai/docs/providers) with the `litellm/` prefix:

```python
claude_agent = Agent(model="litellm/anthropic/claude-3-5-sonnet-20240620", ...)
gemini_agent = Agent(model="litellm/gemini/gemini-2.5-flash-preview-04-17", ...)
```

### Other ways to use non-OpenAI models

You can integrate other LLM providers in 3 more ways (examples [here](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)):

1. [`set_default_openai_client`][agents.set_default_openai_client] is useful in cases where you want to globally use an instance of `AsyncOpenAI` as the LLM client. This is for cases where the LLM provider has an OpenAI compatible API endpoint, and you can set the `base_url` and `api_key`. See a configurable example in [examples/model_providers/custom_example_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py).
2. [`ModelProvider`][agents.models.interface.ModelProvider] is at the `Runner.run` level. This lets you say "use a custom model provider for all agents in this run". See a configurable example in [examples/model_providers/custom_example_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py).
3. [`Agent.model`][agents.agent.Agent.model] lets you specify the model on a specific Agent instance. This enables you to mix and match different providers for different agents. See a configurable example in [examples/model_providers/custom_example_agent.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py). An easy way to use most available models is via the [LiteLLM integration](./litellm.md).

In cases where you do not have an API key from `platform.openai.com`, we recommend disabling tracing via `set_tracing_disabled()`, or setting up a [different tracing processor](../tracing.md).

!!! note

    In these examples, we use the Chat Completions API/model, because most LLM providers don't yet support the Responses API. If your LLM provider does support it, we recommend using Responses.

## Mixing and matching models

Within a single workflow, you may want to use different models for each agent. For example, you could use a smaller, faster model for triage, while using a larger, more capable model for complex tasks. When configuring an [`Agent`][agents.Agent], you can select a specific model by either:

1. Passing the name of a model.
2. Passing any model name + a [`ModelProvider`][agents.models.interface.ModelProvider] that can map that name to a Model instance.
3. Directly providing a [`Model`][agents.models.interface.Model] implementation.

!!!note

    While our SDK supports both the [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] and the [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] shapes, we recommend using a single model shape for each workflow because the two shapes support a different set of features and tools. If your workflow requires mixing and matching model shapes, make sure that all the features you're using are available on both.

```python
from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
    model="gpt-5-mini", # (1)!
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model=OpenAIChatCompletionsModel( # (2)!
        model="gpt-5-nano",
        openai_client=AsyncOpenAI()
    ),
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    model="gpt-5",
)

async def main():
    result = await Runner.run(triage_agent, input="Hola, Â¿cÃ³mo estÃ¡s?")
    print(result.final_output)
```

1.  Sets the name of an OpenAI model directly.
2.  Provides a [`Model`][agents.models.interface.Model] implementation.

When you want to further configure the model used for an agent, you can pass [`ModelSettings`][agents.models.interface.ModelSettings], which provides optional model configuration parameters such as temperature.

```python
from agents import Agent, ModelSettings

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model="gpt-4.1",
    model_settings=ModelSettings(temperature=0.1),
)
```

Also, when you use OpenAI's Responses API, [there are a few other optional parameters](https://platform.openai.com/docs/api-reference/responses/create) (e.g., `user`, `service_tier`, and so on). If they are not available at the top level, you can use `extra_args` to pass them as well.

```python
from agents import Agent, ModelSettings

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model="gpt-4.1",
    model_settings=ModelSettings(
        temperature=0.1,
        extra_args={"service_tier": "flex", "user": "user_12345"},
    ),
)
```

## Common issues with using other LLM providers

### Tracing client error 401

If you get errors related to tracing, this is because traces are uploaded to OpenAI servers, and you don't have an OpenAI API key. You have three options to resolve this:

1. Disable tracing entirely: [`set_tracing_disabled(True)`][agents.set_tracing_disabled].
2. Set an OpenAI key for tracing: [`set_tracing_export_api_key(...)`][agents.set_tracing_export_api_key]. This API key will only be used for uploading traces, and must be from [platform.openai.com](https://platform.openai.com/).
3. Use a non-OpenAI trace processor. See the [tracing docs](../tracing.md#custom-tracing-processors).

### Responses API support

The SDK uses the Responses API by default, but most other LLM providers don't yet support it. You may see 404s or similar issues as a result. To resolve, you have two options:

1. Call [`set_default_openai_api("chat_completions")`][agents.set_default_openai_api]. This works if you are setting `OPENAI_API_KEY` and `OPENAI_BASE_URL` via environment vars.
2. Use [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel]. There are examples [here](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/).

### Structured outputs support

Some model providers don't have support for [structured outputs](https://platform.openai.com/docs/guides/structured-outputs). This sometimes results in an error that looks something like this:

```

BadRequestError: Error code: 400 - {'error': {'message': "'response_format.type' : value is not one of the allowed values ['text','json_object']", 'type': 'invalid_request_error'}}

```

This is a shortcoming of some model providers - they support JSON outputs, but don't allow you to specify the `json_schema` to use for the output. We are working on a fix for this, but we suggest relying on providers that do have support for JSON schema output, because otherwise your app will often break because of malformed JSON.

## Mixing models across providers

You need to be aware of feature differences between model providers, or you may run into errors. For example, OpenAI supports structured outputs, multimodal input, and hosted file search and web search, but many other providers don't support these features. Be aware of these limitations:

-   Don't send unsupported `tools` to providers that don't understand them
-   Filter out multimodal inputs before calling models that are text-only
-   Be aware that providers that don't support structured JSON outputs will occasionally produce invalid JSON.

================
File: docs/models/litellm.md
================
# Using any model via LiteLLM

!!! note

    The LiteLLM integration is in beta. You may run into issues with some model providers, especially smaller ones. Please report any issues via [Github issues](https://github.com/openai/openai-agents-python/issues) and we'll fix quickly.

[LiteLLM](https://docs.litellm.ai/docs/) is a library that allows you to use 100+ models via a single interface. We've added a LiteLLM integration to allow you to use any AI model in the Agents SDK.

## Setup

You'll need to ensure `litellm` is available. You can do this by installing the optional `litellm` dependency group:

```bash
pip install "openai-agents[litellm]"
```

Once done, you can use [`LitellmModel`][agents.extensions.models.litellm_model.LitellmModel] in any agent.

## Example

This is a fully working example. When you run it, you'll be prompted for a model name and API key. For example, you could enter:

-   `openai/gpt-4.1` for the model, and your OpenAI API key
-   `anthropic/claude-3-5-sonnet-20240620` for the model, and your Anthropic API key
-   etc

For a full list of models supported in LiteLLM, see the [litellm providers docs](https://docs.litellm.ai/docs/providers).

```python
from __future__ import annotations

import asyncio

from agents import Agent, Runner, function_tool, set_tracing_disabled
from agents.extensions.models.litellm_model import LitellmModel

@function_tool
def get_weather(city: str):
    print(f"[debug] getting weather for {city}")
    return f"The weather in {city} is sunny."


async def main(model: str, api_key: str):
    agent = Agent(
        name="Assistant",
        instructions="You only respond in haikus.",
        model=LitellmModel(model=model, api_key=api_key),
        tools=[get_weather],
    )

    result = await Runner.run(agent, "What's the weather in Tokyo?")
    print(result.final_output)


if __name__ == "__main__":
    # First try to get model/api key from args
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=False)
    parser.add_argument("--api-key", type=str, required=False)
    args = parser.parse_args()

    model = args.model
    if not model:
        model = input("Enter a model name for Litellm: ")

    api_key = args.api_key
    if not api_key:
        api_key = input("Enter an API key for Litellm: ")

    asyncio.run(main(model, api_key))
```

## Tracking usage data

If you want LiteLLM responses to populate the Agents SDK usage metrics, pass `ModelSettings(include_usage=True)` when creating your agent.

```python
from agents import Agent, ModelSettings
from agents.extensions.models.litellm_model import LitellmModel

agent = Agent(
    name="Assistant",
    model=LitellmModel(model="your/model", api_key="..."),
    model_settings=ModelSettings(include_usage=True),
)
```

With `include_usage=True`, LiteLLM requests report token and request counts through `result.context_wrapper.usage` just like the built-in OpenAI models.

================
File: docs/realtime/guide.md
================
# Guide

This guide provides an in-depth look at building voice-enabled AI agents using the OpenAI Agents SDK's realtime capabilities.

!!! warning "Beta feature"
Realtime agents are in beta. Expect some breaking changes as we improve the implementation.

## Overview

Realtime agents allow for conversational flows, processing audio and text inputs in real time and responding with realtime audio. They maintain persistent connections with OpenAI's Realtime API, enabling natural voice conversations with low latency and the ability to handle interruptions gracefully.

## Architecture

### Core Components

The realtime system consists of several key components:

-   **RealtimeAgent**: An agent, configured with instructions, tools and handoffs.
-   **RealtimeRunner**: Manages configuration. You can call `runner.run()` to get a session.
-   **RealtimeSession**: A single interaction session. You typically create one each time a user starts a conversation, and keep it alive until the conversation is done.
-   **RealtimeModel**: The underlying model interface (typically OpenAI's WebSocket implementation)

### Session flow

A typical realtime session follows this flow:

1. **Create your RealtimeAgent(s)** with instructions, tools and handoffs.
2. **Set up a RealtimeRunner** with the agent and configuration options
3. **Start the session** using `await runner.run()` which returns a RealtimeSession.
4. **Send audio or text messages** to the session using `send_audio()` or `send_message()`
5. **Listen for events** by iterating over the session - events include audio output, transcripts, tool calls, handoffs, and errors
6. **Handle interruptions** when users speak over the agent, which automatically stops current audio generation

The session maintains the conversation history and manages the persistent connection with the realtime model.

## Agent configuration

RealtimeAgent works similarly to the regular Agent class with some key differences. For full API details, see the [`RealtimeAgent`][agents.realtime.agent.RealtimeAgent] API reference.

Key differences from regular agents:

-   Model choice is configured at the session level, not the agent level.
-   No structured output support (`outputType` is not supported).
-   Voice can be configured per agent but cannot be changed after the first agent speaks.
-   All other features like tools, handoffs, and instructions work the same way.

## Session configuration

### Model settings

The session configuration allows you to control the underlying realtime model behavior. You can configure the model name (such as `gpt-realtime`), voice selection (alloy, echo, fable, onyx, nova, shimmer), and supported modalities (text and/or audio). Audio formats can be set for both input and output, with PCM16 being the default.

### Audio configuration

Audio settings control how the session handles voice input and output. You can configure input audio transcription using models like Whisper, set language preferences, and provide transcription prompts to improve accuracy for domain-specific terms. Turn detection settings control when the agent should start and stop responding, with options for voice activity detection thresholds, silence duration, and padding around detected speech.

## Tools and Functions

### Adding Tools

Just like regular agents, realtime agents support function tools that execute during conversations:

```python
from agents import function_tool

@function_tool
def get_weather(city: str) -> str:
    """Get current weather for a city."""
    # Your weather API logic here
    return f"The weather in {city} is sunny, 72Â°F"

@function_tool
def book_appointment(date: str, time: str, service: str) -> str:
    """Book an appointment."""
    # Your booking logic here
    return f"Appointment booked for {service} on {date} at {time}"

agent = RealtimeAgent(
    name="Assistant",
    instructions="You can help with weather and appointments.",
    tools=[get_weather, book_appointment],
)
```

## Handoffs

### Creating Handoffs

Handoffs allow transferring conversations between specialized agents.

```python
from agents.realtime import realtime_handoff

# Specialized agents
billing_agent = RealtimeAgent(
    name="Billing Support",
    instructions="You specialize in billing and payment issues.",
)

technical_agent = RealtimeAgent(
    name="Technical Support",
    instructions="You handle technical troubleshooting.",
)

# Main agent with handoffs
main_agent = RealtimeAgent(
    name="Customer Service",
    instructions="You are the main customer service agent. Hand off to specialists when needed.",
    handoffs=[
        realtime_handoff(billing_agent, tool_description="Transfer to billing support"),
        realtime_handoff(technical_agent, tool_description="Transfer to technical support"),
    ]
)
```

## Event handling

The session streams events that you can listen to by iterating over the session object. Events include audio output chunks, transcription results, tool execution start and end, agent handoffs, and errors. Key events to handle include:

-   **audio**: Raw audio data from the agent's response
-   **audio_end**: Agent finished speaking
-   **audio_interrupted**: User interrupted the agent
-   **tool_start/tool_end**: Tool execution lifecycle
-   **handoff**: Agent handoff occurred
-   **error**: Error occurred during processing

For complete event details, see [`RealtimeSessionEvent`][agents.realtime.events.RealtimeSessionEvent].

## Guardrails

Only output guardrails are supported for realtime agents. These guardrails are debounced and run periodically (not on every word) to avoid performance issues during real-time generation. The default debounce length is 100 characters, but this is configurable.

Guardrails can be attached directly to a `RealtimeAgent` or provided via the session's `run_config`. Guardrails from both sources run together.

```python
from agents.guardrail import GuardrailFunctionOutput, OutputGuardrail

def sensitive_data_check(context, agent, output):
    return GuardrailFunctionOutput(
        tripwire_triggered="password" in output,
        output_info=None,
    )

agent = RealtimeAgent(
    name="Assistant",
    instructions="...",
    output_guardrails=[OutputGuardrail(guardrail_function=sensitive_data_check)],
)
```

When a guardrail is triggered, it generates a `guardrail_tripped` event and can interrupt the agent's current response. The debounce behavior helps balance safety with real-time performance requirements. Unlike text agents, realtime agents do **not** raise an Exception when guardrails are tripped.

## Audio processing

Send audio to the session using [`session.send_audio(audio_bytes)`][agents.realtime.session.RealtimeSession.send_audio] or send text using [`session.send_message()`][agents.realtime.session.RealtimeSession.send_message].

For audio output, listen for `audio` events and play the audio data through your preferred audio library. Make sure to listen for `audio_interrupted` events to stop playback immediately and clear any queued audio when the user interrupts the agent.

## Direct model access

You can access the underlying model to add custom listeners or perform advanced operations:

```python
# Add a custom listener to the model
session.model.add_listener(my_custom_listener)
```

This gives you direct access to the [`RealtimeModel`][agents.realtime.model.RealtimeModel] interface for advanced use cases where you need lower-level control over the connection.

## Examples

For complete working examples, check out the [examples/realtime directory](https://github.com/openai/openai-agents-python/tree/main/examples/realtime) which includes demos with and without UI components.

================
File: docs/realtime/quickstart.md
================
# Quickstart

Realtime agents enable voice conversations with your AI agents using OpenAI's Realtime API. This guide walks you through creating your first realtime voice agent.

!!! warning "Beta feature"
Realtime agents are in beta. Expect some breaking changes as we improve the implementation.

## Prerequisites

-   Python 3.9 or higher
-   OpenAI API key
-   Basic familiarity with the OpenAI Agents SDK

## Installation

If you haven't already, install the OpenAI Agents SDK:

```bash
pip install openai-agents
```

## Creating your first realtime agent

### 1. Import required components

```python
import asyncio
from agents.realtime import RealtimeAgent, RealtimeRunner
```

### 2. Create a realtime agent

```python
agent = RealtimeAgent(
    name="Assistant",
    instructions="You are a helpful voice assistant. Keep your responses conversational and friendly.",
)
```

### 3. Set up the runner

```python
runner = RealtimeRunner(
    starting_agent=agent,
    config={
        "model_settings": {
            "model_name": "gpt-realtime",
            "voice": "ash",
            "modalities": ["audio"],
            "input_audio_format": "pcm16",
            "output_audio_format": "pcm16",
            "input_audio_transcription": {"model": "gpt-4o-mini-transcribe"},
            "turn_detection": {"type": "semantic_vad", "interrupt_response": True},
        }
    }
)
```

### 4. Start a session

```python
# Start the session
session = await runner.run()

async with session:
    print("Session started! The agent will stream audio responses in real-time.")
    # Process events
    async for event in session:
        try:
            if event.type == "agent_start":
                print(f"Agent started: {event.agent.name}")
            elif event.type == "agent_end":
                print(f"Agent ended: {event.agent.name}")
            elif event.type == "handoff":
                print(f"Handoff from {event.from_agent.name} to {event.to_agent.name}")
            elif event.type == "tool_start":
                print(f"Tool started: {event.tool.name}")
            elif event.type == "tool_end":
                print(f"Tool ended: {event.tool.name}; output: {event.output}")
            elif event.type == "audio_end":
                print("Audio ended")
            elif event.type == "audio":
                # Enqueue audio for callback-based playback with metadata
                # Non-blocking put; queue is unbounded, so drops wonâ€™t occur.
                pass
            elif event.type == "audio_interrupted":
                print("Audio interrupted")
                # Begin graceful fade + flush in the audio callback and rebuild jitter buffer.
            elif event.type == "error":
                print(f"Error: {event.error}")
            elif event.type == "history_updated":
                pass  # Skip these frequent events
            elif event.type == "history_added":
                pass  # Skip these frequent events
            elif event.type == "raw_model_event":
                print(f"Raw model event: {_truncate_str(str(event.data), 200)}")
            else:
                print(f"Unknown event type: {event.type}")
        except Exception as e:
            print(f"Error processing event: {_truncate_str(str(e), 200)}")

def _truncate_str(s: str, max_length: int) -> str:
    if len(s) > max_length:
        return s[:max_length] + "..."
    return s
```

## Complete example

Here's a complete working example:

```python
import asyncio
from agents.realtime import RealtimeAgent, RealtimeRunner

async def main():
    # Create the agent
    agent = RealtimeAgent(
        name="Assistant",
        instructions="You are a helpful voice assistant. Keep responses brief and conversational.",
    )
    # Set up the runner with configuration
    runner = RealtimeRunner(
        starting_agent=agent,
        config={
            "model_settings": {
                "model_name": "gpt-realtime",
                "voice": "ash",
                "modalities": ["audio"],
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "input_audio_transcription": {"model": "gpt-4o-mini-transcribe"},
                "turn_detection": {"type": "semantic_vad", "interrupt_response": True},
            }
        },
    )
    # Start the session
    session = await runner.run()

    async with session:
        print("Session started! The agent will stream audio responses in real-time.")
        # Process events
        async for event in session:
            try:
                if event.type == "agent_start":
                    print(f"Agent started: {event.agent.name}")
                elif event.type == "agent_end":
                    print(f"Agent ended: {event.agent.name}")
                elif event.type == "handoff":
                    print(f"Handoff from {event.from_agent.name} to {event.to_agent.name}")
                elif event.type == "tool_start":
                    print(f"Tool started: {event.tool.name}")
                elif event.type == "tool_end":
                    print(f"Tool ended: {event.tool.name}; output: {event.output}")
                elif event.type == "audio_end":
                    print("Audio ended")
                elif event.type == "audio":
                    # Enqueue audio for callback-based playback with metadata
                    # Non-blocking put; queue is unbounded, so drops wonâ€™t occur.
                    pass
                elif event.type == "audio_interrupted":
                    print("Audio interrupted")
                    # Begin graceful fade + flush in the audio callback and rebuild jitter buffer.
                elif event.type == "error":
                    print(f"Error: {event.error}")
                elif event.type == "history_updated":
                    pass  # Skip these frequent events
                elif event.type == "history_added":
                    pass  # Skip these frequent events
                elif event.type == "raw_model_event":
                    print(f"Raw model event: {_truncate_str(str(event.data), 200)}")
                else:
                    print(f"Unknown event type: {event.type}")
            except Exception as e:
                print(f"Error processing event: {_truncate_str(str(e), 200)}")

def _truncate_str(s: str, max_length: int) -> str:
    if len(s) > max_length:
        return s[:max_length] + "..."
    return s

if __name__ == "__main__":
    # Run the session
    asyncio.run(main())
```

## Configuration options

### Model settings

-   `model_name`: Choose from available realtime models (e.g., `gpt-realtime`)
-   `voice`: Select voice (`alloy`, `echo`, `fable`, `onyx`, `nova`, `shimmer`)
-   `modalities`: Enable text or audio (`["text"]` or `["audio"]`)

### Audio settings

-   `input_audio_format`: Format for input audio (`pcm16`, `g711_ulaw`, `g711_alaw`)
-   `output_audio_format`: Format for output audio
-   `input_audio_transcription`: Transcription configuration

### Turn detection

-   `type`: Detection method (`server_vad`, `semantic_vad`)
-   `threshold`: Voice activity threshold (0.0-1.0)
-   `silence_duration_ms`: Silence duration to detect turn end
-   `prefix_padding_ms`: Audio padding before speech

## Next steps

-   [Learn more about realtime agents](guide.md)
-   Check out working examples in the [examples/realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime) folder
-   Add tools to your agent
-   Implement handoffs between agents
-   Set up guardrails for safety

## Authentication

Make sure your OpenAI API key is set in your environment:

```bash
export OPENAI_API_KEY="your-api-key-here"
```

Or pass it directly when creating the session:

```python
session = await runner.run(model_config={"api_key": "your-api-key"})
```

================
File: docs/ref/extensions/memory/advanced_sqlite_session.md
================
# `AdvancedSQLiteSession`

::: agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession

================
File: docs/ref/extensions/memory/encrypt_session.md
================
# `EncryptedSession`

::: agents.extensions.memory.encrypt_session.EncryptedSession

================
File: docs/ref/extensions/memory/redis_session.md
================
# `RedisSession`

::: agents.extensions.memory.redis_session.RedisSession

================
File: docs/ref/extensions/memory/sqlalchemy_session.md
================
# `SQLAlchemySession`

::: agents.extensions.memory.sqlalchemy_session.SQLAlchemySession

================
File: docs/ref/extensions/models/litellm_model.md
================
# `LiteLLM Model`

::: agents.extensions.models.litellm_model

================
File: docs/ref/extensions/models/litellm_provider.md
================
# `LiteLLM Provider`

::: agents.extensions.models.litellm_provider

================
File: docs/ref/extensions/handoff_filters.md
================
# `Handoff filters`

::: agents.extensions.handoff_filters

================
File: docs/ref/extensions/handoff_prompt.md
================
# `Handoff prompt`

::: agents.extensions.handoff_prompt

    options:
        members:
            - RECOMMENDED_PROMPT_PREFIX
            - prompt_with_handoff_instructions

================
File: docs/ref/extensions/litellm.md
================
# `LiteLLM Models`

::: agents.extensions.models.litellm_model

================
File: docs/ref/extensions/visualization.md
================
# `Visualization`

::: agents.extensions.visualization

================
File: docs/ref/mcp/server.md
================
# `MCP Servers`

::: agents.mcp.server

================
File: docs/ref/mcp/util.md
================
# `MCP Util`

::: agents.mcp.util

================
File: docs/ref/memory/openai_conversations_session.md
================
# `Openai Conversations Session`

::: agents.memory.openai_conversations_session

================
File: docs/ref/memory/session.md
================
# `Session`

::: agents.memory.session

================
File: docs/ref/memory/sqlite_session.md
================
# `Sqlite Session`

::: agents.memory.sqlite_session

================
File: docs/ref/memory/util.md
================
# `Util`

::: agents.memory.util

================
File: docs/ref/models/chatcmpl_converter.md
================
# `Chatcmpl Converter`

::: agents.models.chatcmpl_converter

================
File: docs/ref/models/chatcmpl_helpers.md
================
# `Chatcmpl Helpers`

::: agents.models.chatcmpl_helpers

================
File: docs/ref/models/chatcmpl_stream_handler.md
================
# `Chatcmpl Stream Handler`

::: agents.models.chatcmpl_stream_handler

================
File: docs/ref/models/default_models.md
================
# `Default Models`

::: agents.models.default_models

================
File: docs/ref/models/fake_id.md
================
# `Fake Id`

::: agents.models.fake_id

================
File: docs/ref/models/interface.md
================
# `Model interface`

::: agents.models.interface

================
File: docs/ref/models/multi_provider.md
================
# `Multi Provider`

::: agents.models.multi_provider

================
File: docs/ref/models/openai_chatcompletions.md
================
# `OpenAI Chat Completions model`

::: agents.models.openai_chatcompletions

================
File: docs/ref/models/openai_provider.md
================
# `OpenAI Provider`

::: agents.models.openai_provider

================
File: docs/ref/models/openai_responses.md
================
# `OpenAI Responses model`

::: agents.models.openai_responses

================
File: docs/ref/realtime/agent.md
================
# `RealtimeAgent`

::: agents.realtime.agent.RealtimeAgent

================
File: docs/ref/realtime/audio_formats.md
================
# `Audio Formats`

::: agents.realtime.audio_formats

================
File: docs/ref/realtime/config.md
================
# Realtime Configuration

## Run Configuration

::: agents.realtime.config.RealtimeRunConfig

## Model Settings

::: agents.realtime.config.RealtimeSessionModelSettings

## Audio Configuration

::: agents.realtime.config.RealtimeInputAudioTranscriptionConfig
::: agents.realtime.config.RealtimeInputAudioNoiseReductionConfig
::: agents.realtime.config.RealtimeTurnDetectionConfig

## Guardrails Settings

::: agents.realtime.config.RealtimeGuardrailsSettings

## Model Configuration

::: agents.realtime.model.RealtimeModelConfig

## Tracing Configuration

::: agents.realtime.config.RealtimeModelTracingConfig

## User Input Types

::: agents.realtime.config.RealtimeUserInput
::: agents.realtime.config.RealtimeUserInputText
::: agents.realtime.config.RealtimeUserInputMessage

## Client Messages

::: agents.realtime.config.RealtimeClientMessage

## Type Aliases

::: agents.realtime.config.RealtimeModelName
::: agents.realtime.config.RealtimeAudioFormat

================
File: docs/ref/realtime/events.md
================
# Realtime Events

## Session Events

::: agents.realtime.events.RealtimeSessionEvent

## Event Types

### Agent Events
::: agents.realtime.events.RealtimeAgentStartEvent
::: agents.realtime.events.RealtimeAgentEndEvent

### Audio Events
::: agents.realtime.events.RealtimeAudio
::: agents.realtime.events.RealtimeAudioEnd
::: agents.realtime.events.RealtimeAudioInterrupted

### Tool Events
::: agents.realtime.events.RealtimeToolStart
::: agents.realtime.events.RealtimeToolEnd

### Handoff Events
::: agents.realtime.events.RealtimeHandoffEvent

### Guardrail Events
::: agents.realtime.events.RealtimeGuardrailTripped

### History Events
::: agents.realtime.events.RealtimeHistoryAdded
::: agents.realtime.events.RealtimeHistoryUpdated

### Error Events
::: agents.realtime.events.RealtimeError

### Raw Model Events
::: agents.realtime.events.RealtimeRawModelEvent

================
File: docs/ref/realtime/handoffs.md
================
# `Handoffs`

::: agents.realtime.handoffs

================
File: docs/ref/realtime/items.md
================
# `Items`

::: agents.realtime.items

================
File: docs/ref/realtime/model_events.md
================
# `Model Events`

::: agents.realtime.model_events

================
File: docs/ref/realtime/model_inputs.md
================
# `Model Inputs`

::: agents.realtime.model_inputs

================
File: docs/ref/realtime/model.md
================
# `Model`

::: agents.realtime.model

================
File: docs/ref/realtime/openai_realtime.md
================
# `Openai Realtime`

::: agents.realtime.openai_realtime

================
File: docs/ref/realtime/runner.md
================
# `RealtimeRunner`

::: agents.realtime.runner.RealtimeRunner

================
File: docs/ref/realtime/session.md
================
# `RealtimeSession`

::: agents.realtime.session.RealtimeSession

================
File: docs/ref/tracing/create.md
================
# `Creating traces/spans`

::: agents.tracing.create

================
File: docs/ref/tracing/index.md
================
# Tracing module

::: agents.tracing

================
File: docs/ref/tracing/logger.md
================
# `Logger`

::: agents.tracing.logger

================
File: docs/ref/tracing/processor_interface.md
================
# `Processor interface`

::: agents.tracing.processor_interface

================
File: docs/ref/tracing/processors.md
================
# `Processors`

::: agents.tracing.processors

================
File: docs/ref/tracing/provider.md
================
# `Provider`

::: agents.tracing.provider

================
File: docs/ref/tracing/scope.md
================
# `Scope`

::: agents.tracing.scope

================
File: docs/ref/tracing/setup.md
================
# `Setup`

::: agents.tracing.setup

================
File: docs/ref/tracing/span_data.md
================
# `Span data`

::: agents.tracing.span_data

================
File: docs/ref/tracing/spans.md
================
# `Spans`

::: agents.tracing.spans

    options:
        members:
            - Span
            - NoOpSpan
            - SpanImpl

================
File: docs/ref/tracing/traces.md
================
# `Traces`

::: agents.tracing.traces

================
File: docs/ref/tracing/util.md
================
# `Util`

::: agents.tracing.util

================
File: docs/ref/voice/models/openai_model_provider.md
================
# `OpenAI Model Provider`

::: agents.voice.models.openai_model_provider

================
File: docs/ref/voice/models/openai_provider.md
================
# `OpenAIVoiceModelProvider`

::: agents.voice.models.openai_model_provider

================
File: docs/ref/voice/models/openai_stt.md
================
# `OpenAI STT`

::: agents.voice.models.openai_stt

================
File: docs/ref/voice/models/openai_tts.md
================
# `OpenAI TTS`

::: agents.voice.models.openai_tts

================
File: docs/ref/voice/events.md
================
# `Events`

::: agents.voice.events

================
File: docs/ref/voice/exceptions.md
================
# `Exceptions`

::: agents.voice.exceptions

================
File: docs/ref/voice/imports.md
================
# `Imports`

::: agents.voice.imports

================
File: docs/ref/voice/input.md
================
# `Input`

::: agents.voice.input

================
File: docs/ref/voice/model.md
================
# `Model`

::: agents.voice.model

================
File: docs/ref/voice/pipeline_config.md
================
# `Pipeline Config`

::: agents.voice.pipeline_config

================
File: docs/ref/voice/pipeline.md
================
# `Pipeline`

::: agents.voice.pipeline

================
File: docs/ref/voice/result.md
================
# `Result`

::: agents.voice.result

================
File: docs/ref/voice/utils.md
================
# `Utils`

::: agents.voice.utils

================
File: docs/ref/voice/workflow.md
================
# `Workflow`

::: agents.voice.workflow

================
File: docs/ref/agent_output.md
================
# `Agent output`

::: agents.agent_output

================
File: docs/ref/agent.md
================
# `Agents`

::: agents.agent

================
File: docs/ref/computer.md
================
# `Computer`

::: agents.computer

================
File: docs/ref/exceptions.md
================
# `Exceptions`

::: agents.exceptions

================
File: docs/ref/function_schema.md
================
# `Function schema`

::: agents.function_schema

================
File: docs/ref/guardrail.md
================
# `Guardrails`

::: agents.guardrail

================
File: docs/ref/handoffs.md
================
# `Handoffs`

::: agents.handoffs

================
File: docs/ref/index.md
================
# Agents module

::: agents

    options:
        members:
            - set_default_openai_key
            - set_default_openai_client
            - set_default_openai_api
            - set_tracing_export_api_key
            - set_tracing_disabled
            - set_trace_processors
            - enable_verbose_stdout_logging

================
File: docs/ref/items.md
================
# `Items`

::: agents.items

================
File: docs/ref/lifecycle.md
================
# `Lifecycle`

::: agents.lifecycle

    options:
        show_source: false

================
File: docs/ref/logger.md
================
# `Logger`

::: agents.logger

================
File: docs/ref/memory.md
================
# Memory

::: agents.memory

    options:
        members:
            - Session
            - SQLiteSession
            - OpenAIConversationsSession

================
File: docs/ref/model_settings.md
================
# `Model settings`

::: agents.model_settings

================
File: docs/ref/prompts.md
================
# `Prompts`

::: agents.prompts

================
File: docs/ref/repl.md
================
# `repl`

::: agents.repl
    options:
        members:
            - run_demo_loop

================
File: docs/ref/result.md
================
# `Results`

::: agents.result

================
File: docs/ref/run_context.md
================
# `Run context`

::: agents.run_context

================
File: docs/ref/run.md
================
# `Runner`

::: agents.run

    options:
        members:
            - Runner
            - RunConfig

================
File: docs/ref/stream_events.md
================
# `Streaming events`

::: agents.stream_events

================
File: docs/ref/strict_schema.md
================
# `Strict Schema`

::: agents.strict_schema

================
File: docs/ref/tool_context.md
================
# `Tool Context`

::: agents.tool_context

================
File: docs/ref/tool_guardrails.md
================
# `Tool Guardrails`

::: agents.tool_guardrails

================
File: docs/ref/tool.md
================
# `Tools`

::: agents.tool

================
File: docs/ref/usage.md
================
# `Usage`

::: agents.usage

================
File: docs/ref/version.md
================
# `Version`

::: agents.version

================
File: docs/sessions/advanced_sqlite_session.md
================
# Advanced SQLite Sessions

`AdvancedSQLiteSession` is an enhanced version of the basic `SQLiteSession` that provides advanced conversation management capabilities including conversation branching, detailed usage analytics, and structured conversation queries.

## Features

- **Conversation branching**: Create alternative conversation paths from any user message
- **Usage tracking**: Detailed token usage analytics per turn with full JSON breakdowns
- **Structured queries**: Get conversations by turns, tool usage statistics, and more
- **Branch management**: Independent branch switching and management
- **Message structure metadata**: Track message types, tool usage, and conversation flow

## Quick start

```python
from agents import Agent, Runner
from agents.extensions.memory import AdvancedSQLiteSession

# Create agent
agent = Agent(
    name="Assistant",
    instructions="Reply very concisely.",
)

# Create an advanced session
session = AdvancedSQLiteSession(
    session_id="conversation_123",
    db_path="conversations.db",
    create_tables=True
)

# First conversation turn
result = await Runner.run(
    agent,
    "What city is the Golden Gate Bridge in?",
    session=session
)
print(result.final_output)  # "San Francisco"

# IMPORTANT: Store usage data
await session.store_run_usage(result)

# Continue conversation
result = await Runner.run(
    agent,
    "What state is it in?",
    session=session
)
print(result.final_output)  # "California"
await session.store_run_usage(result)
```

## Initialization

```python
from agents.extensions.memory import AdvancedSQLiteSession

# Basic initialization
session = AdvancedSQLiteSession(
    session_id="my_conversation",
    create_tables=True  # Auto-create advanced tables
)

# With persistent storage
session = AdvancedSQLiteSession(
    session_id="user_123",
    db_path="path/to/conversations.db",
    create_tables=True
)

# With custom logger
import logging
logger = logging.getLogger("my_app")
session = AdvancedSQLiteSession(
    session_id="session_456",
    create_tables=True,
    logger=logger
)
```

### Parameters

- `session_id` (str): Unique identifier for the conversation session
- `db_path` (str | Path): Path to SQLite database file. Defaults to `:memory:` for in-memory storage
- `create_tables` (bool): Whether to automatically create the advanced tables. Defaults to `False`
- `logger` (logging.Logger | None): Custom logger for the session. Defaults to module logger

## Usage tracking

AdvancedSQLiteSession provides detailed usage analytics by storing token usage data per conversation turn. **This is entirely dependent on the `store_run_usage` method being called after each agent run.**

### Storing usage data

```python
# After each agent run, store the usage data
result = await Runner.run(agent, "Hello", session=session)
await session.store_run_usage(result)

# This stores:
# - Total tokens used
# - Input/output token breakdown
# - Request count
# - Detailed JSON token information (if available)
```

### Retrieving usage statistics

```python
# Get session-level usage (all branches)
session_usage = await session.get_session_usage()
if session_usage:
    print(f"Total requests: {session_usage['requests']}")
    print(f"Total tokens: {session_usage['total_tokens']}")
    print(f"Input tokens: {session_usage['input_tokens']}")
    print(f"Output tokens: {session_usage['output_tokens']}")
    print(f"Total turns: {session_usage['total_turns']}")

# Get usage for specific branch
branch_usage = await session.get_session_usage(branch_id="main")

# Get usage by turn
turn_usage = await session.get_turn_usage()
for turn_data in turn_usage:
    print(f"Turn {turn_data['user_turn_number']}: {turn_data['total_tokens']} tokens")
    if turn_data['input_tokens_details']:
        print(f"  Input details: {turn_data['input_tokens_details']}")
    if turn_data['output_tokens_details']:
        print(f"  Output details: {turn_data['output_tokens_details']}")

# Get usage for specific turn
turn_2_usage = await session.get_turn_usage(user_turn_number=2)
```

## Conversation branching

One of the key features of AdvancedSQLiteSession is the ability to create conversation branches from any user message, allowing you to explore alternative conversation paths.

### Creating branches

```python
# Get available turns for branching
turns = await session.get_conversation_turns()
for turn in turns:
    print(f"Turn {turn['turn']}: {turn['content']}")
    print(f"Can branch: {turn['can_branch']}")

# Create a branch from turn 2
branch_id = await session.create_branch_from_turn(2)
print(f"Created branch: {branch_id}")

# Create a branch with custom name
branch_id = await session.create_branch_from_turn(
    2, 
    branch_name="alternative_path"
)

# Create branch by searching for content
branch_id = await session.create_branch_from_content(
    "weather", 
    branch_name="weather_focus"
)
```

### Branch management

```python
# List all branches
branches = await session.list_branches()
for branch in branches:
    current = " (current)" if branch["is_current"] else ""
    print(f"{branch['branch_id']}: {branch['user_turns']} turns, {branch['message_count']} messages{current}")

# Switch between branches
await session.switch_to_branch("main")
await session.switch_to_branch(branch_id)

# Delete a branch
await session.delete_branch(branch_id, force=True)  # force=True allows deleting current branch
```

### Branch workflow example

```python
# Original conversation
result = await Runner.run(agent, "What's the capital of France?", session=session)
await session.store_run_usage(result)

result = await Runner.run(agent, "What's the weather like there?", session=session)
await session.store_run_usage(result)

# Create branch from turn 2 (weather question)
branch_id = await session.create_branch_from_turn(2, "weather_focus")

# Continue in new branch with different question
result = await Runner.run(
    agent, 
    "What are the main tourist attractions in Paris?", 
    session=session
)
await session.store_run_usage(result)

# Switch back to main branch
await session.switch_to_branch("main")

# Continue original conversation
result = await Runner.run(
    agent, 
    "How expensive is it to visit?", 
    session=session
)
await session.store_run_usage(result)
```

## Structured queries

AdvancedSQLiteSession provides several methods for analyzing conversation structure and content.

### Conversation analysis

```python
# Get conversation organized by turns
conversation_by_turns = await session.get_conversation_by_turns()
for turn_num, items in conversation_by_turns.items():
    print(f"Turn {turn_num}: {len(items)} items")
    for item in items:
        if item["tool_name"]:
            print(f"  - {item['type']} (tool: {item['tool_name']})")
        else:
            print(f"  - {item['type']}")

# Get tool usage statistics
tool_usage = await session.get_tool_usage()
for tool_name, count, turn in tool_usage:
    print(f"{tool_name}: used {count} times in turn {turn}")

# Find turns by content
matching_turns = await session.find_turns_by_content("weather")
for turn in matching_turns:
    print(f"Turn {turn['turn']}: {turn['content']}")
```

### Message structure

The session automatically tracks message structure including:

- Message types (user, assistant, tool_call, etc.)
- Tool names for tool calls
- Turn numbers and sequence numbers
- Branch associations
- Timestamps

## Database schema

AdvancedSQLiteSession extends the basic SQLite schema with two additional tables:

### message_structure table

```sql
CREATE TABLE message_structure (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT NOT NULL,
    message_id INTEGER NOT NULL,
    branch_id TEXT NOT NULL DEFAULT 'main',
    message_type TEXT NOT NULL,
    sequence_number INTEGER NOT NULL,
    user_turn_number INTEGER,
    branch_turn_number INTEGER,
    tool_name TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (session_id) REFERENCES agent_sessions(session_id) ON DELETE CASCADE,
    FOREIGN KEY (message_id) REFERENCES agent_messages(id) ON DELETE CASCADE
);
```

### turn_usage table

```sql
CREATE TABLE turn_usage (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT NOT NULL,
    branch_id TEXT NOT NULL DEFAULT 'main',
    user_turn_number INTEGER NOT NULL,
    requests INTEGER DEFAULT 0,
    input_tokens INTEGER DEFAULT 0,
    output_tokens INTEGER DEFAULT 0,
    total_tokens INTEGER DEFAULT 0,
    input_tokens_details JSON,
    output_tokens_details JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (session_id) REFERENCES agent_sessions(session_id) ON DELETE CASCADE,
    UNIQUE(session_id, branch_id, user_turn_number)
);
```

## Complete example

Check out the [complete example](https://github.com/openai/openai-agents-python/tree/main/examples/memory/advanced_sqlite_session_example.py) for a comprehensive demonstration of all features.


## API Reference

- [`AdvancedSQLiteSession`][agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession] - Main class
- [`Session`][agents.memory.session.Session] - Base session protocol

================
File: docs/sessions/encrypted_session.md
================
# Encrypted Sessions

`EncryptedSession` provides transparent encryption for any session implementation, securing conversation data with automatic expiration of old items.

## Features

- **Transparent encryption**: Wraps any session with Fernet encryption
- **Per-session keys**: Uses HKDF key derivation for unique encryption per session
- **Automatic expiration**: Old items are silently skipped when TTL expires
- **Drop-in replacement**: Works with any existing session implementation

## Installation

Encrypted sessions require the `encrypt` extra:

```bash
pip install openai-agents[encrypt]
```

## Quick start

```python
import asyncio
from agents import Agent, Runner
from agents.extensions.memory import EncryptedSession, SQLAlchemySession

async def main():
    agent = Agent("Assistant")
    
    # Create underlying session
    underlying_session = SQLAlchemySession.from_url(
        "user-123",
        url="sqlite+aiosqlite:///:memory:",
        create_tables=True
    )
    
    # Wrap with encryption
    session = EncryptedSession(
        session_id="user-123",
        underlying_session=underlying_session,
        encryption_key="your-secret-key-here",
        ttl=600  # 10 minutes
    )
    
    result = await Runner.run(agent, "Hello", session=session)
    print(result.final_output)

if __name__ == "__main__":
    asyncio.run(main())
```

## Configuration

### Encryption key

The encryption key can be either a Fernet key or any string:

```python
from agents.extensions.memory import EncryptedSession

# Using a Fernet key (base64-encoded)
session = EncryptedSession(
    session_id="user-123",
    underlying_session=underlying_session,
    encryption_key="your-fernet-key-here",
    ttl=600
)

# Using a raw string (will be derived to a key)
session = EncryptedSession(
    session_id="user-123", 
    underlying_session=underlying_session,
    encryption_key="my-secret-password",
    ttl=600
)
```

### TTL (Time To Live)

Set how long encrypted items remain valid:

```python
# Items expire after 1 hour
session = EncryptedSession(
    session_id="user-123",
    underlying_session=underlying_session,
    encryption_key="secret",
    ttl=3600  # 1 hour in seconds
)

# Items expire after 1 day
session = EncryptedSession(
    session_id="user-123",
    underlying_session=underlying_session,
    encryption_key="secret", 
    ttl=86400  # 24 hours in seconds
)
```

## Usage with different session types

### With SQLite sessions

```python
from agents import SQLiteSession
from agents.extensions.memory import EncryptedSession

# Create encrypted SQLite session
underlying = SQLiteSession("user-123", "conversations.db")

session = EncryptedSession(
    session_id="user-123",
    underlying_session=underlying,
    encryption_key="secret-key"
)
```

### With SQLAlchemy sessions

```python
from agents.extensions.memory import EncryptedSession, SQLAlchemySession

# Create encrypted SQLAlchemy session
underlying = SQLAlchemySession.from_url(
    "user-123",
    url="postgresql+asyncpg://user:pass@localhost/db",
    create_tables=True
)

session = EncryptedSession(
    session_id="user-123",
    underlying_session=underlying,
    encryption_key="secret-key"
)
```

!!! warning "Advanced Session Features"

    When using `EncryptedSession` with advanced session implementations like `AdvancedSQLiteSession`, note that:

    - Methods like `find_turns_by_content()` won't work effectively since message content is encrypted
    - Content-based searches operate on encrypted data, limiting their effectiveness



## Key derivation

EncryptedSession uses HKDF (HMAC-based Key Derivation Function) to derive unique encryption keys per session:

- **Master key**: Your provided encryption key
- **Session salt**: The session ID
- **Info string**: `"agents.session-store.hkdf.v1"`
- **Output**: 32-byte Fernet key

This ensures that:
- Each session has a unique encryption key
- Keys cannot be derived without the master key
- Session data cannot be decrypted across different sessions

## Automatic expiration

When items exceed the TTL, they are automatically skipped during retrieval:

```python
# Items older than TTL are silently ignored
items = await session.get_items()  # Only returns non-expired items

# Expired items don't affect session behavior
result = await Runner.run(agent, "Continue conversation", session=session)
```

## API Reference

- [`EncryptedSession`][agents.extensions.memory.encrypt_session.EncryptedSession] - Main class
- [`Session`][agents.memory.session.Session] - Base session protocol

================
File: docs/sessions/index.md
================
# Sessions

The Agents SDK provides built-in session memory to automatically maintain conversation history across multiple agent runs, eliminating the need to manually handle `.to_input_list()` between turns.

Sessions stores conversation history for a specific session, allowing agents to maintain context without requiring explicit manual memory management. This is particularly useful for building chat applications or multi-turn conversations where you want the agent to remember previous interactions.

## Quick start

```python
from agents import Agent, Runner, SQLiteSession

# Create agent
agent = Agent(
    name="Assistant",
    instructions="Reply very concisely.",
)

# Create a session instance with a session ID
session = SQLiteSession("conversation_123")

# First turn
result = await Runner.run(
    agent,
    "What city is the Golden Gate Bridge in?",
    session=session
)
print(result.final_output)  # "San Francisco"

# Second turn - agent automatically remembers previous context
result = await Runner.run(
    agent,
    "What state is it in?",
    session=session
)
print(result.final_output)  # "California"

# Also works with synchronous runner
result = Runner.run_sync(
    agent,
    "What's the population?",
    session=session
)
print(result.final_output)  # "Approximately 39 million"
```

## How it works

When session memory is enabled:

1. **Before each run**: The runner automatically retrieves the conversation history for the session and prepends it to the input items.
2. **After each run**: All new items generated during the run (user input, assistant responses, tool calls, etc.) are automatically stored in the session.
3. **Context preservation**: Each subsequent run with the same session includes the full conversation history, allowing the agent to maintain context.

This eliminates the need to manually call `.to_input_list()` and manage conversation state between runs.

## Memory operations

### Basic operations

Sessions supports several operations for managing conversation history:

```python
from agents import SQLiteSession

session = SQLiteSession("user_123", "conversations.db")

# Get all items in a session
items = await session.get_items()

# Add new items to a session
new_items = [
    {"role": "user", "content": "Hello"},
    {"role": "assistant", "content": "Hi there!"}
]
await session.add_items(new_items)

# Remove and return the most recent item
last_item = await session.pop_item()
print(last_item)  # {"role": "assistant", "content": "Hi there!"}

# Clear all items from a session
await session.clear_session()
```

### Using pop_item for corrections

The `pop_item` method is particularly useful when you want to undo or modify the last item in a conversation:

```python
from agents import Agent, Runner, SQLiteSession

agent = Agent(name="Assistant")
session = SQLiteSession("correction_example")

# Initial conversation
result = await Runner.run(
    agent,
    "What's 2 + 2?",
    session=session
)
print(f"Agent: {result.final_output}")

# User wants to correct their question
assistant_item = await session.pop_item()  # Remove agent's response
user_item = await session.pop_item()  # Remove user's question

# Ask a corrected question
result = await Runner.run(
    agent,
    "What's 2 + 3?",
    session=session
)
print(f"Agent: {result.final_output}")
```

## Session types

The SDK provides several session implementations for different use cases:

### OpenAI Conversations API sessions

Use [OpenAI's Conversations API](https://platform.openai.com/docs/api-reference/conversations) through `OpenAIConversationsSession`.

```python
from agents import Agent, Runner, OpenAIConversationsSession

# Create agent
agent = Agent(
    name="Assistant",
    instructions="Reply very concisely.",
)

# Create a new conversation
session = OpenAIConversationsSession()

# Optionally resume a previous conversation by passing a conversation ID
# session = OpenAIConversationsSession(conversation_id="conv_123")

# Start conversation
result = await Runner.run(
    agent,
    "What city is the Golden Gate Bridge in?",
    session=session
)
print(result.final_output)  # "San Francisco"

# Continue the conversation
result = await Runner.run(
    agent,
    "What state is it in?",
    session=session
)
print(result.final_output)  # "California"
```

### SQLite sessions

The default, lightweight session implementation using SQLite:

```python
from agents import SQLiteSession

# In-memory database (lost when process ends)
session = SQLiteSession("user_123")

# Persistent file-based database
session = SQLiteSession("user_123", "conversations.db")

# Use the session
result = await Runner.run(
    agent,
    "Hello",
    session=session
)
```

### SQLAlchemy sessions

Production-ready sessions using any SQLAlchemy-supported database:

```python
from agents.extensions.memory import SQLAlchemySession

# Using database URL
session = SQLAlchemySession.from_url(
    "user_123",
    url="postgresql+asyncpg://user:pass@localhost/db",
    create_tables=True
)

# Using existing engine
from sqlalchemy.ext.asyncio import create_async_engine
engine = create_async_engine("postgresql+asyncpg://user:pass@localhost/db")
session = SQLAlchemySession("user_123", engine=engine, create_tables=True)
```

See [SQLAlchemy Sessions](sqlalchemy_session.md) for detailed documentation.

### Advanced SQLite sessions

Enhanced SQLite sessions with conversation branching, usage analytics, and structured queries:

```python
from agents.extensions.memory import AdvancedSQLiteSession

# Create with advanced features
session = AdvancedSQLiteSession(
    session_id="user_123",
    db_path="conversations.db",
    create_tables=True
)

# Automatic usage tracking
result = await Runner.run(agent, "Hello", session=session)
await session.store_run_usage(result)  # Track token usage

# Conversation branching
await session.create_branch_from_turn(2)  # Branch from turn 2
```

See [Advanced SQLite Sessions](advanced_sqlite_session.md) for detailed documentation.

### Encrypted sessions

Transparent encryption wrapper for any session implementation:

```python
from agents.extensions.memory import EncryptedSession, SQLAlchemySession

# Create underlying session
underlying_session = SQLAlchemySession.from_url(
    "user_123",
    url="sqlite+aiosqlite:///conversations.db",
    create_tables=True
)

# Wrap with encryption and TTL
session = EncryptedSession(
    session_id="user_123",
    underlying_session=underlying_session,
    encryption_key="your-secret-key",
    ttl=600  # 10 minutes
)

result = await Runner.run(agent, "Hello", session=session)
```

See [Encrypted Sessions](encrypted_session.md) for detailed documentation.

## Session management

### Session ID naming

Use meaningful session IDs that help you organize conversations:

-   User-based: `"user_12345"`
-   Thread-based: `"thread_abc123"`
-   Context-based: `"support_ticket_456"`

### Memory persistence

-   Use in-memory SQLite (`SQLiteSession("session_id")`) for temporary conversations
-   Use file-based SQLite (`SQLiteSession("session_id", "path/to/db.sqlite")`) for persistent conversations
-   Use SQLAlchemy-powered sessions (`SQLAlchemySession("session_id", engine=engine, create_tables=True)`) for production systems with existing databases supported by SQLAlchemy
-   Use OpenAI-hosted storage (`OpenAIConversationsSession()`) when you prefer to store history in the OpenAI Conversations API
-   Use encrypted sessions (`EncryptedSession(session_id, underlying_session, encryption_key)`) to wrap any session with transparent encryption and TTL-based expiration
-   Consider implementing custom session backends for other production systems (Redis, Django, etc.) for more advanced use cases

### Multiple sessions

```python
from agents import Agent, Runner, SQLiteSession

agent = Agent(name="Assistant")

# Different sessions maintain separate conversation histories
session_1 = SQLiteSession("user_123", "conversations.db")
session_2 = SQLiteSession("user_456", "conversations.db")

result1 = await Runner.run(
    agent,
    "Help me with my account",
    session=session_1
)
result2 = await Runner.run(
    agent,
    "What are my charges?",
    session=session_2
)
```

### Session sharing

```python
# Different agents can share the same session
support_agent = Agent(name="Support")
billing_agent = Agent(name="Billing")
session = SQLiteSession("user_123")

# Both agents will see the same conversation history
result1 = await Runner.run(
    support_agent,
    "Help me with my account",
    session=session
)
result2 = await Runner.run(
    billing_agent,
    "What are my charges?",
    session=session
)
```

## Complete example

Here's a complete example showing session memory in action:

```python
import asyncio
from agents import Agent, Runner, SQLiteSession


async def main():
    # Create an agent
    agent = Agent(
        name="Assistant",
        instructions="Reply very concisely.",
    )

    # Create a session instance that will persist across runs
    session = SQLiteSession("conversation_123", "conversation_history.db")

    print("=== Sessions Example ===")
    print("The agent will remember previous messages automatically.\n")

    # First turn
    print("First turn:")
    print("User: What city is the Golden Gate Bridge in?")
    result = await Runner.run(
        agent,
        "What city is the Golden Gate Bridge in?",
        session=session
    )
    print(f"Assistant: {result.final_output}")
    print()

    # Second turn - the agent will remember the previous conversation
    print("Second turn:")
    print("User: What state is it in?")
    result = await Runner.run(
        agent,
        "What state is it in?",
        session=session
    )
    print(f"Assistant: {result.final_output}")
    print()

    # Third turn - continuing the conversation
    print("Third turn:")
    print("User: What's the population of that state?")
    result = await Runner.run(
        agent,
        "What's the population of that state?",
        session=session
    )
    print(f"Assistant: {result.final_output}")
    print()

    print("=== Conversation Complete ===")
    print("Notice how the agent remembered the context from previous turns!")
    print("Sessions automatically handles conversation history.")


if __name__ == "__main__":
    asyncio.run(main())
```

## Custom session implementations

You can implement your own session memory by creating a class that follows the [`Session`][agents.memory.session.Session] protocol:

```python
from agents.memory.session import SessionABC
from agents.items import TResponseInputItem
from typing import List

class MyCustomSession(SessionABC):
    """Custom session implementation following the Session protocol."""

    def __init__(self, session_id: str):
        self.session_id = session_id
        # Your initialization here

    async def get_items(self, limit: int | None = None) -> List[TResponseInputItem]:
        """Retrieve conversation history for this session."""
        # Your implementation here
        pass

    async def add_items(self, items: List[TResponseInputItem]) -> None:
        """Store new items for this session."""
        # Your implementation here
        pass

    async def pop_item(self) -> TResponseInputItem | None:
        """Remove and return the most recent item from this session."""
        # Your implementation here
        pass

    async def clear_session(self) -> None:
        """Clear all items for this session."""
        # Your implementation here
        pass

# Use your custom session
agent = Agent(name="Assistant")
result = await Runner.run(
    agent,
    "Hello",
    session=MyCustomSession("my_session")
)
```

## API Reference

For detailed API documentation, see:

-   [`Session`][agents.memory.session.Session] - Protocol interface
-   [`OpenAIConversationsSession`][agents.memory.OpenAIConversationsSession] - OpenAI Conversations API implementation
-   [`SQLiteSession`][agents.memory.sqlite_session.SQLiteSession] - Basic SQLite implementation
-   [`SQLAlchemySession`][agents.extensions.memory.sqlalchemy_session.SQLAlchemySession] - SQLAlchemy-powered implementation
-   [`AdvancedSQLiteSession`][agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession] - Enhanced SQLite with branching and analytics
-   [`EncryptedSession`][agents.extensions.memory.encrypt_session.EncryptedSession] - Encrypted wrapper for any session

================
File: docs/sessions/sqlalchemy_session.md
================
# SQLAlchemy Sessions

`SQLAlchemySession` uses SQLAlchemy to provide a production-ready session implementation, allowing you to use any database supported by SQLAlchemy (PostgreSQL, MySQL, SQLite, etc.) for session storage.

## Installation

SQLAlchemy sessions require the `sqlalchemy` extra:

```bash
pip install openai-agents[sqlalchemy]
```

## Quick start

### Using database URL

The simplest way to get started:

```python
import asyncio
from agents import Agent, Runner
from agents.extensions.memory import SQLAlchemySession

async def main():
    agent = Agent("Assistant")
    
    # Create session using database URL
    session = SQLAlchemySession.from_url(
        "user-123",
        url="sqlite+aiosqlite:///:memory:",
        create_tables=True
    )
    
    result = await Runner.run(agent, "Hello", session=session)
    print(result.final_output)

if __name__ == "__main__":
    asyncio.run(main())
```

### Using existing engine

For applications with existing SQLAlchemy engines:

```python
import asyncio
from agents import Agent, Runner
from agents.extensions.memory import SQLAlchemySession
from sqlalchemy.ext.asyncio import create_async_engine

async def main():
    # Create your database engine
    engine = create_async_engine("postgresql+asyncpg://user:pass@localhost/db")
    
    agent = Agent("Assistant")
    session = SQLAlchemySession(
        "user-456",
        engine=engine,
        create_tables=True
    )
    
    result = await Runner.run(agent, "Hello", session=session)
    print(result.final_output)
    
    # Clean up
    await engine.dispose()

if __name__ == "__main__":
    asyncio.run(main())
```


## API Reference

- [`SQLAlchemySession`][agents.extensions.memory.sqlalchemy_session.SQLAlchemySession] - Main class
- [`Session`][agents.memory.session.Session] - Base session protocol

================
File: docs/voice/pipeline.md
================
# Pipelines and workflows

[`VoicePipeline`][agents.voice.pipeline.VoicePipeline] is a class that makes it easy to turn your agentic workflows into a voice app. You pass in a workflow to run, and the pipeline takes care of transcribing input audio, detecting when the audio ends, calling your workflow at the right time, and turning the workflow output back into audio.

```mermaid
graph LR
    %% Input
    A["ğŸ¤ Audio Input"]

    %% Voice Pipeline
    subgraph Voice_Pipeline [Voice Pipeline]
        direction TB
        B["Transcribe (speech-to-text)"]
        C["Your Code"]:::highlight
        D["Text-to-speech"]
        B --> C --> D
    end

    %% Output
    E["ğŸ§ Audio Output"]

    %% Flow
    A --> Voice_Pipeline
    Voice_Pipeline --> E

    %% Custom styling
    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;

```

## Configuring a pipeline

When you create a pipeline, you can set a few things:

1. The [`workflow`][agents.voice.workflow.VoiceWorkflowBase], which is the code that runs each time new audio is transcribed.
2. The [`speech-to-text`][agents.voice.model.STTModel] and [`text-to-speech`][agents.voice.model.TTSModel] models used
3. The [`config`][agents.voice.pipeline_config.VoicePipelineConfig], which lets you configure things like:
    - A model provider, which can map model names to models
    - Tracing, including whether to disable tracing, whether audio files are uploaded, the workflow name, trace IDs etc.
    - Settings on the TTS and STT models, like the prompt, language and data types used.

## Running a pipeline

You can run a pipeline via the [`run()`][agents.voice.pipeline.VoicePipeline.run] method, which lets you pass in audio input in two forms:

1. [`AudioInput`][agents.voice.input.AudioInput] is used when you have a full audio transcript, and just want to produce a result for it. This is useful in cases where you don't need to detect when a speaker is done speaking; for example, when you have pre-recorded audio or in push-to-talk apps where it's clear when the user is done speaking.
2. [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput] is used when you might need to detect when a user is done speaking. It allows you to push audio chunks as they are detected, and the voice pipeline will automatically run the agent workflow at the right time, via a process called "activity detection".

## Results

The result of a voice pipeline run is a [`StreamedAudioResult`][agents.voice.result.StreamedAudioResult]. This is an object that lets you stream events as they occur. There are a few kinds of [`VoiceStreamEvent`][agents.voice.events.VoiceStreamEvent], including:

1. [`VoiceStreamEventAudio`][agents.voice.events.VoiceStreamEventAudio], which contains a chunk of audio.
2. [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle], which informs you of lifecycle events like a turn starting or ending.
3. [`VoiceStreamEventError`][agents.voice.events.VoiceStreamEventError], is an error event.

```python

result = await pipeline.run(input)

async for event in result.stream():
    if event.type == "voice_stream_event_audio":
        # play audio
    elif event.type == "voice_stream_event_lifecycle":
        # lifecycle
    elif event.type == "voice_stream_event_error"
        # error
    ...
```

## Best practices

### Interruptions

The Agents SDK currently does not support any built-in interruptions support for [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]. Instead for every detected turn it will trigger a separate run of your workflow. If you want to handle interruptions inside your application you can listen to the [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle] events. `turn_started` will indicate that a new turn was transcribed and processing is beginning. `turn_ended` will trigger after all the audio was dispatched for a respective turn. You could use these events to mute the microphone of the speaker when the model starts a turn and unmute it after you flushed all the related audio for a turn.

================
File: docs/voice/quickstart.md
================
# Quickstart

## Prerequisites

Make sure you've followed the base [quickstart instructions](../quickstart.md) for the Agents SDK, and set up a virtual environment. Then, install the optional voice dependencies from the SDK:

```bash
pip install 'openai-agents[voice]'
```

## Concepts

The main concept to know about is a [`VoicePipeline`][agents.voice.pipeline.VoicePipeline], which is a 3 step process:

1. Run a speech-to-text model to turn audio into text.
2. Run your code, which is usually an agentic workflow, to produce a result.
3. Run a text-to-speech model to turn the result text back into audio.

```mermaid
graph LR
    %% Input
    A["ğŸ¤ Audio Input"]

    %% Voice Pipeline
    subgraph Voice_Pipeline [Voice Pipeline]
        direction TB
        B["Transcribe (speech-to-text)"]
        C["Your Code"]:::highlight
        D["Text-to-speech"]
        B --> C --> D
    end

    %% Output
    E["ğŸ§ Audio Output"]

    %% Flow
    A --> Voice_Pipeline
    Voice_Pipeline --> E

    %% Custom styling
    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;

```

## Agents

First, let's set up some Agents. This should feel familiar to you if you've built any agents with this SDK. We'll have a couple of Agents, a handoff, and a tool.

```python
import asyncio
import random

from agents import (
    Agent,
    function_tool,
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions



@function_tool
def get_weather(city: str) -> str:
    """Get the weather for a given city."""
    print(f"[debug] get_weather called with city: {city}")
    choices = ["sunny", "cloudy", "rainy", "snowy"]
    return f"The weather in {city} is {random.choice(choices)}."


spanish_agent = Agent(
    name="Spanish",
    handoff_description="A spanish speaking agent.",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. Speak in Spanish.",
    ),
    model="gpt-4.1",
)

agent = Agent(
    name="Assistant",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.",
    ),
    model="gpt-4.1",
    handoffs=[spanish_agent],
    tools=[get_weather],
)
```

## Voice pipeline

We'll set up a simple voice pipeline, using [`SingleAgentVoiceWorkflow`][agents.voice.workflow.SingleAgentVoiceWorkflow] as the workflow.

```python
from agents.voice import SingleAgentVoiceWorkflow, VoicePipeline
pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))
```

## Run the pipeline

```python
import numpy as np
import sounddevice as sd
from agents.voice import AudioInput

# For simplicity, we'll just create 3 seconds of silence
# In reality, you'd get microphone data
buffer = np.zeros(24000 * 3, dtype=np.int16)
audio_input = AudioInput(buffer=buffer)

result = await pipeline.run(audio_input)

# Create an audio player using `sounddevice`
player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
player.start()

# Play the audio stream as it comes in
async for event in result.stream():
    if event.type == "voice_stream_event_audio":
        player.write(event.data)

```

## Put it all together

```python
import asyncio
import random

import numpy as np
import sounddevice as sd

from agents import (
    Agent,
    function_tool,
    set_tracing_disabled,
)
from agents.voice import (
    AudioInput,
    SingleAgentVoiceWorkflow,
    VoicePipeline,
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions


@function_tool
def get_weather(city: str) -> str:
    """Get the weather for a given city."""
    print(f"[debug] get_weather called with city: {city}")
    choices = ["sunny", "cloudy", "rainy", "snowy"]
    return f"The weather in {city} is {random.choice(choices)}."


spanish_agent = Agent(
    name="Spanish",
    handoff_description="A spanish speaking agent.",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. Speak in Spanish.",
    ),
    model="gpt-4.1",
)

agent = Agent(
    name="Assistant",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.",
    ),
    model="gpt-4.1",
    handoffs=[spanish_agent],
    tools=[get_weather],
)


async def main():
    pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))
    buffer = np.zeros(24000 * 3, dtype=np.int16)
    audio_input = AudioInput(buffer=buffer)

    result = await pipeline.run(audio_input)

    # Create an audio player using `sounddevice`
    player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
    player.start()

    # Play the audio stream as it comes in
    async for event in result.stream():
        if event.type == "voice_stream_event_audio":
            player.write(event.data)


if __name__ == "__main__":
    asyncio.run(main())
```

If you run this example, the agent will speak to you! Check out the example in [examples/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static) to see a demo where you can speak to the agent yourself.

================
File: docs/voice/tracing.md
================
# Tracing

Just like the way [agents are traced](../tracing.md), voice pipelines are also automatically traced.

You can read the tracing doc above for basic tracing information, but you can additionally configure tracing of a pipeline via [`VoicePipelineConfig`][agents.voice.pipeline_config.VoicePipelineConfig].

Key tracing related fields are:

-   [`tracing_disabled`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]: controls whether tracing is disabled. By default, tracing is enabled.
-   [`trace_include_sensitive_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_data]: controls whether traces include potentially sensitive data, like audio transcripts. This is specifically for the voice pipeline, and not for anything that goes on inside your Workflow.
-   [`trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data]: controls whether traces include audio data.
-   [`workflow_name`][agents.voice.pipeline_config.VoicePipelineConfig.workflow_name]: The name of the trace workflow.
-   [`group_id`][agents.voice.pipeline_config.VoicePipelineConfig.group_id]: The `group_id` of the trace, which lets you link multiple traces.
-   [`trace_metadata`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]: Additional metadata to include with the trace.

================
File: docs/zh/models/index.md
================
---
search:
  exclude: true
---
# æ¨¡å‹

Agents SDK å¼€ç®±å³ç”¨åœ°æ”¯æŒä¸¤ç§ OpenAI æ¨¡å‹å½¢å¼ï¼š

-   **æ¨è**ï¼š[`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel]ï¼Œä½¿ç”¨å…¨æ–°çš„ [Responses API](https://platform.openai.com/docs/api-reference/responses) è°ƒç”¨ OpenAI æ¥å£ã€‚
-   [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel]ï¼Œä½¿ç”¨ [Chat Completions API](https://platform.openai.com/docs/api-reference/chat) è°ƒç”¨ OpenAI æ¥å£ã€‚

## OpenAI æ¨¡å‹

å½“ä½ åœ¨åˆå§‹åŒ– `Agent` æ—¶æœªæŒ‡å®šæ¨¡å‹ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹ã€‚å½“å‰é»˜è®¤æ˜¯ [`gpt-4.1`](https://platform.openai.com/docs/models/gpt-4.1)ï¼Œåœ¨æ™ºèƒ½ä½“å·¥ä½œæµçš„å¯é¢„æµ‹æ€§ä¸ä½å»¶è¿Ÿä¹‹é—´å®ç°äº†è‰¯å¥½å¹³è¡¡ã€‚

å¦‚æœä½ æƒ³åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å‹ï¼ˆå¦‚ [`gpt-5`](https://platform.openai.com/docs/models/gpt-5)ï¼‰ï¼Œè¯·æŒ‰ç…§ä¸‹ä¸€èŠ‚çš„æ­¥éª¤è¿›è¡Œã€‚

### é»˜è®¤ OpenAI æ¨¡å‹

å¦‚æœä½ å¸Œæœ›å¯¹æ‰€æœ‰æœªè®¾ç½®è‡ªå®šä¹‰æ¨¡å‹çš„æ™ºèƒ½ä½“å§‹ç»ˆä½¿ç”¨æŸä¸ªç‰¹å®šæ¨¡å‹ï¼Œè¯·åœ¨è¿è¡Œæ™ºèƒ½ä½“ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ `OPENAI_DEFAULT_MODEL`ã€‚

```bash
export OPENAI_DEFAULT_MODEL=gpt-5
python3 my_awesome_agent.py
```

#### GPT-5 æ¨¡å‹

å½“ä½ ä»¥è¿™ç§æ–¹å¼ä½¿ç”¨ä»»ä¸€ GPT-5 æ¨ç†æ¨¡å‹ï¼ˆ[`gpt-5`](https://platform.openai.com/docs/models/gpt-5)ã€[`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini) æˆ– [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano)ï¼‰æ—¶ï¼ŒSDK ä¼šé»˜è®¤åº”ç”¨åˆç†çš„ `ModelSettings`ã€‚å…·ä½“æ¥è¯´ï¼Œä¼šå°† `reasoning.effort` å’Œ `verbosity` éƒ½è®¾ç½®ä¸º `"low"`ã€‚å¦‚æœä½ æƒ³è‡ªè¡Œæ„å»ºè¿™äº›è®¾ç½®ï¼Œå¯è°ƒç”¨ `agents.models.get_default_model_settings("gpt-5")`ã€‚

ä¸ºæ›´ä½å»¶è¿Ÿæˆ–ç‰¹å®šéœ€æ±‚ï¼Œä½ å¯ä»¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹å’Œè®¾ç½®ã€‚è¦ä¸ºé»˜è®¤æ¨¡å‹è°ƒæ•´æ¨ç†å¼ºåº¦ï¼Œå¯ä¼ å…¥ä½ è‡ªå·±çš„ `ModelSettings`ï¼š

```python
from openai.types.shared import Reasoning
from agents import Agent, ModelSettings

my_agent = Agent(
    name="My Agent",
    instructions="You're a helpful agent.",
    model_settings=ModelSettings(reasoning=Reasoning(effort="minimal"), verbosity="low")
    # If OPENAI_DEFAULT_MODEL=gpt-5 is set, passing only model_settings works.
    # It's also fine to pass a GPT-5 model name explicitly:
    # model="gpt-5",
)
```

é’ˆå¯¹é™ä½å»¶è¿Ÿçš„åœºæ™¯ï¼Œä½¿ç”¨ [`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini) æˆ– [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano) å¹¶è®¾ç½® `reasoning.effort="minimal"` å¾€å¾€ä¼šæ¯”é»˜è®¤è®¾ç½®æ›´å¿«è¿”å›ç»“æœã€‚ä½†æ˜¯ï¼ŒResponses API ä¸­çš„ä¸€äº›å†…ç½®å·¥å…·ï¼ˆå¦‚ æ–‡ä»¶æ£€ç´¢ å’Œ å›¾åƒç”Ÿæˆï¼‰ä¸æ”¯æŒ `"minimal"` æ¨ç†å¼ºåº¦ï¼Œè¿™ä¹Ÿæ˜¯æœ¬ Agents SDK é»˜è®¤ä½¿ç”¨ `"low"` çš„åŸå› ã€‚

#### é GPT-5 æ¨¡å‹

å¦‚æœä½ ä¼ å…¥çš„æ˜¯é GPT-5 çš„æ¨¡å‹åç§°ä¸”æœªæä¾›è‡ªå®šä¹‰ `model_settings`ï¼ŒSDK ä¼šå›é€€åˆ°å¯ä¸ä»»æ„æ¨¡å‹å…¼å®¹çš„é€šç”¨ `ModelSettings`ã€‚

## é OpenAI æ¨¡å‹

ä½ å¯ä»¥é€šè¿‡ [LiteLLM é›†æˆ](./litellm.md) ä½¿ç”¨å¤§å¤šæ•°å…¶ä»–é OpenAI æ¨¡å‹ã€‚é¦–å…ˆï¼Œå®‰è£… litellm ä¾èµ–åˆ†ç»„ï¼š

```bash
pip install "openai-agents[litellm]"
```

ç„¶åï¼Œä½¿ç”¨ä»»ä¸€å¸¦æœ‰ `litellm/` å‰ç¼€çš„[å—æ”¯æŒæ¨¡å‹](https://docs.litellm.ai/docs/providers)ï¼š

```python
claude_agent = Agent(model="litellm/anthropic/claude-3-5-sonnet-20240620", ...)
gemini_agent = Agent(model="litellm/gemini/gemini-2.5-flash-preview-04-17", ...)
```

### ä½¿ç”¨é OpenAI æ¨¡å‹çš„å…¶ä»–æ–¹å¼

ä½ è¿˜å¯ä»¥ç”¨å¦å¤– 3 ç§æ–¹å¼é›†æˆå…¶ä»– LLM æä¾›æ–¹ï¼ˆcode examples è§[æ­¤å¤„](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)ï¼‰ï¼š

1. [`set_default_openai_client`][agents.set_default_openai_client] é€‚ç”¨äºä½ å¸Œæœ›åœ¨å…¨å±€ä½¿ç”¨æŸä¸ª `AsyncOpenAI` å®ä¾‹ä½œä¸º LLM å®¢æˆ·ç«¯çš„åœºæ™¯ã€‚é€‚åˆ LLM æä¾›æ–¹å…·å¤‡ OpenAI å…¼å®¹ API ç«¯ç‚¹ï¼Œå¹¶å¯è®¾ç½® `base_url` å’Œ `api_key` çš„æƒ…å†µã€‚å¯å‚è€ƒå¯é…ç½®ç¤ºä¾‹ï¼š[examples/model_providers/custom_example_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py)ã€‚
2. [`ModelProvider`][agents.models.interface.ModelProvider] ä½äº `Runner.run` å±‚çº§ã€‚å®ƒå…è®¸ä½ å£°æ˜â€œåœ¨æœ¬æ¬¡è¿è¡Œä¸­ä¸ºæ‰€æœ‰æ™ºèƒ½ä½“ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹æä¾›æ–¹â€ã€‚å¯å‚è€ƒå¯é…ç½®ç¤ºä¾‹ï¼š[examples/model_providers/custom_example_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py)ã€‚
3. [`Agent.model`][agents.agent.Agent.model] å…è®¸ä½ åœ¨ç‰¹å®šçš„ Agent å®ä¾‹ä¸ŠæŒ‡å®šæ¨¡å‹ã€‚è¿™æ ·ä½ å°±å¯ä»¥ä¸ºä¸åŒçš„æ™ºèƒ½ä½“æ··æ­ä¸åŒçš„æä¾›æ–¹ã€‚å¯å‚è€ƒå¯é…ç½®ç¤ºä¾‹ï¼š[examples/model_providers/custom_example_agent.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py)ã€‚ä½¿ç”¨å¤§å¤šæ•°å¯ç”¨æ¨¡å‹çš„ç®€ä¾¿æ–¹å¼æ˜¯é€šè¿‡ [LiteLLM é›†æˆ](./litellm.md)ã€‚

å¦‚æœä½ æ²¡æœ‰æ¥è‡ª `platform.openai.com` çš„ API å¯†é’¥ï¼Œå»ºè®®é€šè¿‡ `set_tracing_disabled()` ç¦ç”¨ è¿½è¸ªï¼Œæˆ–è®¾ç½®[ä¸åŒçš„ è¿½è¸ªè¿›ç¨‹](../tracing.md)ã€‚

!!! note

    åœ¨è¿™äº›ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ Chat Completions API/æ¨¡å‹ï¼Œå› ä¸ºå¤§å¤šæ•° LLM æä¾›æ–¹å°šä¸æ”¯æŒ Responses APIã€‚å¦‚æœä½ çš„ LLM æä¾›æ–¹æ”¯æŒï¼Œå»ºè®®ä½¿ç”¨ Responsesã€‚

## æ··åˆä¸æ­é…æ¨¡å‹

åœ¨å•ä¸ªå·¥ä½œæµä¸­ï¼Œä½ å¯èƒ½å¸Œæœ›ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“ä½¿ç”¨ä¸åŒçš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥ä¸ºåˆ†è¯Šä½¿ç”¨æ›´å°æ›´å¿«çš„æ¨¡å‹ï¼Œè€Œä¸ºå¤æ‚ä»»åŠ¡ä½¿ç”¨æ›´å¤§æ›´å¼ºçš„æ¨¡å‹ã€‚é…ç½® [`Agent`][agents.Agent] æ—¶ï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼é€‰æ‹©ç‰¹å®šæ¨¡å‹ï¼š

1. ä¼ å…¥æ¨¡å‹åç§°ã€‚
2. ä¼ å…¥ä»»æ„æ¨¡å‹åç§° + ä¸€ä¸ªå¯å°†è¯¥åç§°æ˜ å°„åˆ° Model å®ä¾‹çš„ [`ModelProvider`][agents.models.interface.ModelProvider]ã€‚
3. ç›´æ¥æä¾›ä¸€ä¸ª [`Model`][agents.models.interface.Model] å®ç°ã€‚

!!!note

    è™½ç„¶æˆ‘ä»¬çš„ SDK åŒæ—¶æ”¯æŒ [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] å’Œ [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] è¿™ä¸¤ç§å½¢æ€ï¼Œä½†æˆ‘ä»¬å»ºè®®åœ¨æ¯ä¸ªå·¥ä½œæµä¸­ä½¿ç”¨å•ä¸€æ¨¡å‹å½¢æ€ï¼Œå› ä¸ºä¸¤ç§å½¢æ€æ”¯æŒçš„åŠŸèƒ½å’Œå·¥å…·ä¸åŒã€‚å¦‚æœä½ çš„å·¥ä½œæµéœ€è¦æ··æ­æ¨¡å‹å½¢æ€ï¼Œè¯·ç¡®ä¿ä½ ä½¿ç”¨çš„æ‰€æœ‰åŠŸèƒ½åœ¨ä¸¤è€…ä¸Šéƒ½å¯ç”¨ã€‚

```python
from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
    model="gpt-5-mini", # (1)!
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model=OpenAIChatCompletionsModel( # (2)!
        model="gpt-5-nano",
        openai_client=AsyncOpenAI()
    ),
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    model="gpt-5",
)

async def main():
    result = await Runner.run(triage_agent, input="Hola, Â¿cÃ³mo estÃ¡s?")
    print(result.final_output)
```

1.  ç›´æ¥è®¾ç½® OpenAI æ¨¡å‹çš„åç§°ã€‚
2.  æä¾›ä¸€ä¸ª [`Model`][agents.models.interface.Model] å®ç°ã€‚

å½“ä½ å¸Œæœ›è¿›ä¸€æ­¥é…ç½®æŸä¸ªæ™ºèƒ½ä½“æ‰€ç”¨çš„æ¨¡å‹æ—¶ï¼Œå¯ä»¥ä¼ å…¥ [`ModelSettings`][agents.models.interface.ModelSettings]ï¼Œç”¨äºæä¾›è¯¸å¦‚ temperature ç­‰å¯é€‰æ¨¡å‹é…ç½®å‚æ•°ã€‚

```python
from agents import Agent, ModelSettings

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model="gpt-4.1",
    model_settings=ModelSettings(temperature=0.1),
)
```

å¦å¤–ï¼Œå½“ä½ ä½¿ç”¨ OpenAI çš„ Responses API æ—¶ï¼Œ[è¿˜æœ‰ä¸€äº›å…¶ä»–å¯é€‰å‚æ•°](https://platform.openai.com/docs/api-reference/responses/create)ï¼ˆä¾‹å¦‚ `user`ã€`service_tier` ç­‰ï¼‰ã€‚å¦‚æœå®ƒä»¬ä¸åœ¨é¡¶å±‚å¯ç”¨ï¼Œä½ å¯ä»¥é€šè¿‡ `extra_args` ä¼ å…¥ã€‚

```python
from agents import Agent, ModelSettings

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model="gpt-4.1",
    model_settings=ModelSettings(
        temperature=0.1,
        extra_args={"service_tier": "flex", "user": "user_12345"},
    ),
)
```

## ä½¿ç”¨å…¶ä»– LLM æä¾›æ–¹çš„å¸¸è§é—®é¢˜

### è¿½è¸ªå®¢æˆ·ç«¯é”™è¯¯ 401

å¦‚æœä½ é‡åˆ°ä¸è¿½è¸ªç›¸å…³çš„é”™è¯¯ï¼Œè¿™æ˜¯å› ä¸ºè¿½è¸ªæ•°æ®ä¼šä¸Šä¼ åˆ° OpenAI æœåŠ¡ï¼Œè€Œä½ æ²¡æœ‰ OpenAI API å¯†é’¥ã€‚ä½ æœ‰ä¸‰ç§æ–¹å¼è§£å†³ï¼š

1. å®Œå…¨ç¦ç”¨è¿½è¸ªï¼š[`set_tracing_disabled(True)`][agents.set_tracing_disabled]ã€‚
2. ä¸ºè¿½è¸ªè®¾ç½®ä¸€ä¸ª OpenAI å¯†é’¥ï¼š[`set_tracing_export_api_key(...)`][agents.set_tracing_export_api_key]ã€‚è¯¥ API å¯†é’¥ä»…ç”¨äºä¸Šä¼ è¿½è¸ªï¼Œä¸”å¿…é¡»æ¥è‡ª [platform.openai.com](https://platform.openai.com/)ã€‚
3. ä½¿ç”¨é OpenAI çš„è¿½è¸ªè¿›ç¨‹ã€‚å‚è§[è¿½è¸ªæ–‡æ¡£](../tracing.md#custom-tracing-processors)ã€‚

### Responses API æ”¯æŒ

SDK é»˜è®¤ä½¿ç”¨ Responses APIï¼Œä½†å¤§å¤šæ•°å…¶ä»– LLM æä¾›æ–¹å°šä¸æ”¯æŒã€‚å› æ­¤ä½ å¯èƒ½ä¼šé‡åˆ° 404 æˆ–ç±»ä¼¼é—®é¢˜ã€‚ä½ æœ‰ä¸¤ä¸ªé€‰é¡¹ï¼š

1. è°ƒç”¨ [`set_default_openai_api("chat_completions")`][agents.set_default_openai_api]ã€‚å¦‚æœä½ é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®äº† `OPENAI_API_KEY` å’Œ `OPENAI_BASE_URL`ï¼Œè¿™å°†ç”Ÿæ•ˆã€‚
2. ä½¿ç”¨ [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel]ã€‚code examples è§[æ­¤å¤„](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)ã€‚

### structured outputs æ”¯æŒ

ä¸€äº›æ¨¡å‹æä¾›æ–¹ä¸æ”¯æŒ [structured outputs](https://platform.openai.com/docs/guides/structured-outputs)ã€‚è¿™æœ‰æ—¶ä¼šå¯¼è‡´ç±»ä¼¼å¦‚ä¸‹çš„é”™è¯¯ï¼š

```

BadRequestError: Error code: 400 - {'error': {'message': "'response_format.type' : value is not one of the allowed values ['text','json_object']", 'type': 'invalid_request_error'}}

```

è¿™æ˜¯æŸäº›æ¨¡å‹æä¾›æ–¹çš„ç¼ºé™·â€”â€”å®ƒä»¬æ”¯æŒ JSON è¾“å‡ºï¼Œä½†ä¸å…è®¸ä½ æŒ‡å®šç”¨äºè¾“å‡ºçš„ `json_schema`ã€‚æˆ‘ä»¬æ­£åœ¨åŠªåŠ›ä¿®å¤æ­¤é—®é¢˜ï¼Œä½†å»ºè®®ä¼˜å…ˆä¾èµ–æ”¯æŒ JSON schema è¾“å‡ºçš„æä¾›æ–¹ï¼Œå¦åˆ™ä½ çš„åº”ç”¨å¸¸ä¼šå›  JSON æ ¼å¼é”™è¯¯è€Œå‡ºé”™ã€‚

## è·¨æä¾›æ–¹æ··ç”¨æ¨¡å‹

ä½ éœ€è¦äº†è§£å„æ¨¡å‹æä¾›æ–¹ä¹‹é—´çš„åŠŸèƒ½å·®å¼‚ï¼Œå¦åˆ™å¯èƒ½ä¼šé‡åˆ°é”™è¯¯ã€‚ä¾‹å¦‚ï¼ŒOpenAI æ”¯æŒ structured outputsã€å¤šæ¨¡æ€è¾“å…¥ï¼Œä»¥åŠæ‰˜ç®¡çš„æ–‡ä»¶æ£€ç´¢å’Œ ç½‘ç»œæ£€ç´¢ï¼Œä½†è®¸å¤šå…¶ä»–æä¾›æ–¹ä¸æ”¯æŒè¿™äº›åŠŸèƒ½ã€‚è¯·æ³¨æ„ä»¥ä¸‹é™åˆ¶ï¼š

-   ä¸è¦å‘ä¸æ”¯æŒçš„æä¾›æ–¹å‘é€å…¶æ— æ³•ç†è§£çš„ `tools`
-   åœ¨è°ƒç”¨ä»…æ”¯æŒæ–‡æœ¬çš„æ¨¡å‹å‰ï¼Œè¿‡æ»¤æ‰å¤šæ¨¡æ€è¾“å…¥
-   æ³¨æ„ä¸æ”¯æŒç»“æ„åŒ– JSON è¾“å‡ºçš„æä¾›æ–¹å¶å°”ä¼šäº§ç”Ÿæ— æ•ˆçš„ JSONã€‚

================
File: docs/zh/models/litellm.md
================
---
search:
  exclude: true
---
# é€šè¿‡ LiteLLM ä½¿ç”¨ä»»æ„æ¨¡å‹

!!! note

    LiteLLM é›†æˆç›®å‰å¤„äºæµ‹è¯•ç‰ˆã€‚ä½ åœ¨ä½¿ç”¨æŸäº›æ¨¡å‹æä¾›å•†ï¼ˆå°¤å…¶æ˜¯è§„æ¨¡è¾ƒå°çš„ï¼‰æ—¶å¯èƒ½ä¼šé‡åˆ°é—®é¢˜ã€‚è¯·é€šè¿‡ [Github issues](https://github.com/openai/openai-agents-python/issues) åé¦ˆé—®é¢˜ï¼Œæˆ‘ä»¬ä¼šå°½å¿«ä¿®å¤ã€‚

[LiteLLM](https://docs.litellm.ai/docs/) æ˜¯ä¸€ä¸ªåº“ï¼Œå…è®¸ä½ é€šè¿‡ç»Ÿä¸€æ¥å£ä½¿ç”¨ 100 å¤šç§æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ Agents SDK ä¸­åŠ å…¥äº† LiteLLM é›†æˆï¼Œä½¿ä½ å¯ä»¥ä½¿ç”¨ä»»æ„ AI æ¨¡å‹ã€‚

## è®¾ç½®

ä½ éœ€è¦ç¡®ä¿ `litellm` å¯ç”¨ã€‚å¯ä»¥é€šè¿‡å®‰è£…å¯é€‰çš„ `litellm` ä¾èµ–ç»„æ¥å®Œæˆï¼š

```bash
pip install "openai-agents[litellm]"
```

å®Œæˆåï¼Œä½ å¯ä»¥åœ¨ä»»æ„æ™ºèƒ½ä½“ä¸­ä½¿ç”¨ [`LitellmModel`][agents.extensions.models.litellm_model.LitellmModel]ã€‚

## ç¤ºä¾‹

è¿™æ˜¯ä¸€ä¸ªå¯ç›´æ¥è¿è¡Œçš„ç¤ºä¾‹ã€‚è¿è¡Œæ—¶ä¼šæç¤ºä½ è¾“å…¥æ¨¡å‹åç§°å’Œ API keyã€‚æ¯”å¦‚ï¼Œä½ å¯ä»¥è¾“å…¥ï¼š

- `openai/gpt-4.1` ä½œä¸ºæ¨¡å‹ï¼Œä»¥åŠä½ çš„ OpenAI API key
- `anthropic/claude-3-5-sonnet-20240620` ä½œä¸ºæ¨¡å‹ï¼Œä»¥åŠä½ çš„ Anthropic API key
- ç­‰ç­‰

æœ‰å…³ LiteLLM æ”¯æŒçš„å®Œæ•´æ¨¡å‹åˆ—è¡¨ï¼Œè¯·å‚è§ [litellm providers æ–‡æ¡£](https://docs.litellm.ai/docs/providers)ã€‚

```python
from __future__ import annotations

import asyncio

from agents import Agent, Runner, function_tool, set_tracing_disabled
from agents.extensions.models.litellm_model import LitellmModel

@function_tool
def get_weather(city: str):
    print(f"[debug] getting weather for {city}")
    return f"The weather in {city} is sunny."


async def main(model: str, api_key: str):
    agent = Agent(
        name="Assistant",
        instructions="You only respond in haikus.",
        model=LitellmModel(model=model, api_key=api_key),
        tools=[get_weather],
    )

    result = await Runner.run(agent, "What's the weather in Tokyo?")
    print(result.final_output)


if __name__ == "__main__":
    # First try to get model/api key from args
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=False)
    parser.add_argument("--api-key", type=str, required=False)
    args = parser.parse_args()

    model = args.model
    if not model:
        model = input("Enter a model name for Litellm: ")

    api_key = args.api_key
    if not api_key:
        api_key = input("Enter an API key for Litellm: ")

    asyncio.run(main(model, api_key))
```

## ä½¿ç”¨æ•°æ®è¿½è¸ª

å¦‚æœä½ å¸Œæœ› LiteLLM çš„å“åº”å¡«å…… Agents SDK çš„ä½¿ç”¨æŒ‡æ ‡ï¼Œåœ¨åˆ›å»ºæ™ºèƒ½ä½“æ—¶ä¼ å…¥ `ModelSettings(include_usage=True)`ã€‚

```python
from agents import Agent, ModelSettings
from agents.extensions.models.litellm_model import LitellmModel

agent = Agent(
    name="Assistant",
    model=LitellmModel(model="your/model", api_key="..."),
    model_settings=ModelSettings(include_usage=True),
)
```

å¯ç”¨ `include_usage=True` åï¼ŒLiteLLM è¯·æ±‚ä¼šé€šè¿‡ `result.context_wrapper.usage` æŠ¥å‘Š token å’Œè¯·æ±‚è®¡æ•°ï¼Œä¸å†…ç½®çš„ OpenAI æ¨¡å‹ä¸€è‡´ã€‚

================
File: docs/zh/realtime/guide.md
================
---
search:
  exclude: true
---
# æŒ‡å—

æœ¬æŒ‡å—æ·±å…¥ä»‹ç»å¦‚ä½•ä½¿ç”¨ OpenAI Agents SDK çš„å®æ—¶åŠŸèƒ½æ„å»ºæ”¯æŒè¯­éŸ³çš„ AI æ™ºèƒ½ä½“ã€‚

!!! warning "æµ‹è¯•ç‰ˆåŠŸèƒ½"
å®æ—¶æ™ºèƒ½ä½“ç›®å‰å¤„äºæµ‹è¯•é˜¶æ®µã€‚éšç€æˆ‘ä»¬æ”¹è¿›å®ç°ï¼Œå¯èƒ½ä¼šæœ‰ä¸å…¼å®¹çš„å˜æ›´ã€‚

## æ¦‚è¿°

å®æ—¶æ™ºèƒ½ä½“æ”¯æŒå¯¹è¯å¼æµç¨‹ï¼Œèƒ½å¤Ÿå®æ—¶å¤„ç†éŸ³é¢‘ä¸æ–‡æœ¬è¾“å…¥ï¼Œå¹¶ä»¥å®æ—¶éŸ³é¢‘è¿›è¡Œå“åº”ã€‚å®ƒä»¬ä¸ OpenAI çš„ Realtime API ä¿æŒæŒä¹…è¿æ¥ï¼Œä»è€Œå®ç°ä½å»¶è¿Ÿçš„è‡ªç„¶è¯­éŸ³å¯¹è¯ï¼Œå¹¶èƒ½å¤Ÿä¼˜é›…åœ°å¤„ç†æ‰“æ–­ã€‚

## æ¶æ„

### æ ¸å¿ƒç»„ä»¶

å®æ—¶ç³»ç»Ÿç”±ä»¥ä¸‹å…³é”®ç»„ä»¶ç»„æˆï¼š

-   **RealtimeAgent**: ä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œå¯é…ç½® instructionsã€tools å’Œ handoffsã€‚
-   **RealtimeRunner**: ç®¡ç†é…ç½®ã€‚ä½ å¯ä»¥è°ƒç”¨ `runner.run()` è·å–ä¼šè¯ã€‚
-   **RealtimeSession**: å•æ¬¡äº¤äº’ä¼šè¯ã€‚é€šå¸¸åœ¨æ¯æ¬¡ç”¨æˆ·å¼€å§‹å¯¹è¯æ—¶åˆ›å»ºä¸€ä¸ªï¼Œå¹¶åœ¨å¯¹è¯ç»“æŸå‰ä¿æŒå­˜æ´»ã€‚
-   **RealtimeModel**: åº•å±‚æ¨¡å‹æ¥å£ï¼ˆé€šå¸¸ä¸º OpenAI çš„ WebSocket å®ç°ï¼‰

### ä¼šè¯æµç¨‹

å…¸å‹çš„å®æ—¶ä¼šè¯æµç¨‹å¦‚ä¸‹ï¼š

1. **åˆ›å»º RealtimeAgent**ï¼Œé…ç½® instructionsã€tools å’Œ handoffsã€‚
2. **è®¾ç½® RealtimeRunner**ï¼Œæä¾›æ™ºèƒ½ä½“å’Œé…ç½®é€‰é¡¹
3. **å¯åŠ¨ä¼šè¯**ï¼Œä½¿ç”¨ `await runner.run()`ï¼Œè¿”å›ä¸€ä¸ª RealtimeSessionã€‚
4. **å‘é€éŸ³é¢‘æˆ–æ–‡æœ¬æ¶ˆæ¯** åˆ°ä¼šè¯ï¼Œä½¿ç”¨ `send_audio()` æˆ– `send_message()`
5. **ç›‘å¬äº‹ä»¶**ï¼Œé€šè¿‡è¿­ä»£ä¼šè¯å¯¹è±¡è·å–äº‹ä»¶â€”â€”åŒ…æ‹¬éŸ³é¢‘è¾“å‡ºã€è½¬å†™ã€å·¥å…·è°ƒç”¨ã€ä»»åŠ¡è½¬ç§»å’Œé”™è¯¯
6. **å¤„ç†æ‰“æ–­**ï¼Œå½“ç”¨æˆ·æ‰“æ–­æ™ºèƒ½ä½“è¯´è¯æ—¶ï¼Œä¼šè‡ªåŠ¨åœæ­¢å½“å‰çš„éŸ³é¢‘ç”Ÿæˆ

ä¼šè¯ä¼šç»´æŠ¤å¯¹è¯å†å²ï¼Œå¹¶ç®¡ç†ä¸å®æ—¶æ¨¡å‹çš„æŒä¹…è¿æ¥ã€‚

## æ™ºèƒ½ä½“é…ç½®

RealtimeAgent ä¸å¸¸è§„ Agent ç±»ä¼¼ï¼Œä½†æœ‰ä¸€äº›å…³é”®å·®å¼‚ã€‚å®Œæ•´ API è¯¦æƒ…å‚è§ [`RealtimeAgent`][agents.realtime.agent.RealtimeAgent] API å‚è€ƒã€‚

ä¸å¸¸è§„æ™ºèƒ½ä½“çš„ä¸»è¦å·®å¼‚ï¼š

-   æ¨¡å‹é€‰æ‹©åœ¨ä¼šè¯çº§åˆ«è¿›è¡Œé…ç½®ï¼Œè€Œä¸æ˜¯æ™ºèƒ½ä½“çº§åˆ«ã€‚
-   ä¸æ”¯æŒ structured outputï¼ˆä¸æ”¯æŒ `outputType`ï¼‰ã€‚
-   è¯­éŸ³å¯æŒ‰æ™ºèƒ½ä½“é…ç½®ï¼Œä½†åœ¨ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“å¼€å§‹è¯´è¯åä¸å¯æ›´æ”¹ã€‚
-   å…¶å®ƒç‰¹æ€§å¦‚å·¥å…·ã€ä»»åŠ¡è½¬ç§»å’Œ instructions çš„å·¥ä½œæ–¹å¼ç›¸åŒã€‚

## ä¼šè¯é…ç½®

### æ¨¡å‹è®¾ç½®

ä¼šè¯é…ç½®å…è®¸ä½ æ§åˆ¶åº•å±‚å®æ—¶æ¨¡å‹çš„è¡Œä¸ºã€‚ä½ å¯ä»¥é…ç½®æ¨¡å‹åç§°ï¼ˆå¦‚ `gpt-realtime`ï¼‰ã€è¯­éŸ³é€‰æ‹©ï¼ˆalloyã€echoã€fableã€onyxã€novaã€shimmerï¼‰ä»¥åŠæ”¯æŒçš„æ¨¡æ€ï¼ˆæ–‡æœ¬å’Œ/æˆ–éŸ³é¢‘ï¼‰ã€‚éŸ³é¢‘æ ¼å¼å¯åˆ†åˆ«é’ˆå¯¹è¾“å…¥ä¸è¾“å‡ºè®¾ç½®ï¼Œé»˜è®¤æ˜¯ PCM16ã€‚

### éŸ³é¢‘é…ç½®

éŸ³é¢‘è®¾ç½®æ§åˆ¶ä¼šè¯å¦‚ä½•å¤„ç†è¯­éŸ³è¾“å…¥ä¸è¾“å‡ºã€‚ä½ å¯ä»¥ä½¿ç”¨å¦‚ Whisper çš„æ¨¡å‹è¿›è¡Œè¾“å…¥éŸ³é¢‘è½¬å†™ã€è®¾ç½®è¯­è¨€åå¥½ï¼Œå¹¶æä¾›è½¬å†™æç¤ºä»¥æå‡é¢†åŸŸæœ¯è¯­çš„å‡†ç¡®æ€§ã€‚è½®æ¬¡æ£€æµ‹è®¾ç½®æ§åˆ¶æ™ºèƒ½ä½“ä½•æ—¶å¼€å§‹å’Œåœæ­¢å“åº”ï¼ŒåŒ…æ‹¬è¯­éŸ³æ´»åŠ¨æ£€æµ‹é˜ˆå€¼ã€é™éŸ³æ—¶é•¿ä»¥åŠå¯¹æ£€æµ‹åˆ°çš„è¯­éŸ³æ·»åŠ å‰åç¼“å†²ã€‚

## å·¥å…·ä¸å‡½æ•°

### æ·»åŠ å·¥å…·

ä¸å¸¸è§„æ™ºèƒ½ä½“ç›¸åŒï¼Œå®æ—¶æ™ºèƒ½ä½“æ”¯æŒåœ¨å¯¹è¯ä¸­æ‰§è¡Œçš„ function toolsï¼š

```python
from agents import function_tool

@function_tool
def get_weather(city: str) -> str:
    """Get current weather for a city."""
    # Your weather API logic here
    return f"The weather in {city} is sunny, 72Â°F"

@function_tool
def book_appointment(date: str, time: str, service: str) -> str:
    """Book an appointment."""
    # Your booking logic here
    return f"Appointment booked for {service} on {date} at {time}"

agent = RealtimeAgent(
    name="Assistant",
    instructions="You can help with weather and appointments.",
    tools=[get_weather, book_appointment],
)
```

## ä»»åŠ¡è½¬ç§»

### åˆ›å»ºä»»åŠ¡è½¬ç§»

ä»»åŠ¡è½¬ç§»å…è®¸åœ¨ä¸“é—¨åŒ–æ™ºèƒ½ä½“ä¹‹é—´è½¬ç§»å¯¹è¯ã€‚

```python
from agents.realtime import realtime_handoff

# Specialized agents
billing_agent = RealtimeAgent(
    name="Billing Support",
    instructions="You specialize in billing and payment issues.",
)

technical_agent = RealtimeAgent(
    name="Technical Support",
    instructions="You handle technical troubleshooting.",
)

# Main agent with handoffs
main_agent = RealtimeAgent(
    name="Customer Service",
    instructions="You are the main customer service agent. Hand off to specialists when needed.",
    handoffs=[
        realtime_handoff(billing_agent, tool_description="Transfer to billing support"),
        realtime_handoff(technical_agent, tool_description="Transfer to technical support"),
    ]
)
```

## äº‹ä»¶å¤„ç†

ä¼šè¯ä¼šä»¥æµå¼æ–¹å¼å‘é€äº‹ä»¶ï¼Œä½ å¯ä»¥é€šè¿‡è¿­ä»£ä¼šè¯å¯¹è±¡æ¥ç›‘å¬ã€‚äº‹ä»¶åŒ…æ‹¬éŸ³é¢‘è¾“å‡ºåˆ†ç‰‡ã€è½¬å†™ç»“æœã€å·¥å…·æ‰§è¡Œå¼€å§‹å’Œç»“æŸã€æ™ºèƒ½ä½“ä»»åŠ¡è½¬ç§»ä»¥åŠé”™è¯¯ã€‚éœ€è¦é‡ç‚¹å¤„ç†çš„äº‹ä»¶åŒ…æ‹¬ï¼š

-   **audio**: æ¥è‡ªæ™ºèƒ½ä½“å“åº”çš„åŸå§‹éŸ³é¢‘æ•°æ®
-   **audio_end**: æ™ºèƒ½ä½“å®Œæˆå‘å£°
-   **audio_interrupted**: ç”¨æˆ·æ‰“æ–­äº†æ™ºèƒ½ä½“
-   **tool_start/tool_end**: å·¥å…·æ‰§è¡Œçš„ç”Ÿå‘½å‘¨æœŸ
-   **handoff**: å‘ç”Ÿæ™ºèƒ½ä½“ä»»åŠ¡è½¬ç§»
-   **error**: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯

å®Œæ•´äº‹ä»¶è¯¦æƒ…å‚è§ [`RealtimeSessionEvent`][agents.realtime.events.RealtimeSessionEvent]ã€‚

## å®‰å…¨é˜²æŠ¤æªæ–½

å®æ—¶æ™ºèƒ½ä½“ä»…æ”¯æŒè¾“å‡ºå®‰å…¨é˜²æŠ¤æªæ–½ã€‚è¿™äº›é˜²æŠ¤ä»¥å»æŠ–åŠ¨æ–¹å¼å‘¨æœŸæ€§è¿è¡Œï¼ˆéæ¯ä¸ªè¯è§¦å‘ï¼‰ï¼Œä»¥é¿å…å®æ—¶ç”Ÿæˆæ—¶çš„æ€§èƒ½é—®é¢˜ã€‚é»˜è®¤å»æŠ–é•¿åº¦ä¸º 100 ä¸ªå­—ç¬¦ï¼Œå¯é…ç½®ã€‚

å®‰å…¨é˜²æŠ¤æªæ–½å¯ä»¥ç›´æ¥é™„åŠ åˆ° `RealtimeAgent`ï¼Œæˆ–é€šè¿‡ä¼šè¯çš„ `run_config` æä¾›ã€‚ä¸¤ä¸ªæ¥æºçš„é˜²æŠ¤ä¼šå…±åŒç”Ÿæ•ˆã€‚

```python
from agents.guardrail import GuardrailFunctionOutput, OutputGuardrail

def sensitive_data_check(context, agent, output):
    return GuardrailFunctionOutput(
        tripwire_triggered="password" in output,
        output_info=None,
    )

agent = RealtimeAgent(
    name="Assistant",
    instructions="...",
    output_guardrails=[OutputGuardrail(guardrail_function=sensitive_data_check)],
)
```

å½“å®‰å…¨é˜²æŠ¤è¢«è§¦å‘æ—¶ï¼Œä¼šç”Ÿæˆ `guardrail_tripped` äº‹ä»¶ï¼Œå¹¶å¯ä¸­æ–­æ™ºèƒ½ä½“å½“å‰çš„å“åº”ã€‚å»æŠ–åŠ¨è¡Œä¸ºæœ‰åŠ©äºåœ¨å®‰å…¨æ€§ä¸å®æ—¶æ€§èƒ½è¦æ±‚ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä¸æ–‡æœ¬æ™ºèƒ½ä½“ä¸åŒï¼Œå®æ—¶æ™ºèƒ½ä½“åœ¨é˜²æŠ¤è¢«è§¦å‘æ—¶ä¸ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚

## éŸ³é¢‘å¤„ç†

é€šè¿‡ [`session.send_audio(audio_bytes)`][agents.realtime.session.RealtimeSession.send_audio] å‘ä¼šè¯å‘é€éŸ³é¢‘ï¼Œæˆ–é€šè¿‡ [`session.send_message()`][agents.realtime.session.RealtimeSession.send_message] å‘é€æ–‡æœ¬ã€‚

å¯¹äºéŸ³é¢‘è¾“å‡ºï¼Œç›‘å¬ `audio` äº‹ä»¶å¹¶é€šè¿‡ä½ åå¥½çš„éŸ³é¢‘åº“æ’­æ”¾éŸ³é¢‘æ•°æ®ã€‚åŠ¡å¿…ç›‘å¬ `audio_interrupted` äº‹ä»¶ï¼Œä»¥ä¾¿åœ¨ç”¨æˆ·æ‰“æ–­æ™ºèƒ½ä½“æ—¶ç«‹å³åœæ­¢æ’­æ”¾å¹¶æ¸…ç©ºä»»ä½•å·²æ’é˜Ÿçš„éŸ³é¢‘ã€‚

## ç›´æ¥è®¿é—®æ¨¡å‹

ä½ å¯ä»¥è®¿é—®åº•å±‚æ¨¡å‹ä»¥æ·»åŠ è‡ªå®šä¹‰ç›‘å¬å™¨æˆ–æ‰§è¡Œé«˜çº§æ“ä½œï¼š

```python
# Add a custom listener to the model
session.model.add_listener(my_custom_listener)
```

è¿™ä½¿ä½ èƒ½å¤Ÿç›´æ¥è®¿é—® [`RealtimeModel`][agents.realtime.model.RealtimeModel] æ¥å£ï¼Œä»¥æ»¡è¶³éœ€è¦å¯¹è¿æ¥è¿›è¡Œæ›´åº•å±‚æ§åˆ¶çš„é«˜çº§ç”¨ä¾‹ã€‚

## ç¤ºä¾‹

å®Œæ•´å¯è¿è¡Œçš„ç¤ºä¾‹è¯·æŸ¥çœ‹ [examples/realtime ç›®å½•](https://github.com/openai/openai-agents-python/tree/main/examples/realtime)ï¼Œå…¶ä¸­åŒ…å«å¸¦æœ‰å’Œä¸å¸¦ UI ç»„ä»¶çš„æ¼”ç¤ºã€‚

================
File: docs/zh/realtime/quickstart.md
================
---
search:
  exclude: true
---
# å¿«é€Ÿå¼€å§‹

Realtime æ™ºèƒ½ä½“è®©ä½ çš„ AI æ™ºèƒ½ä½“é€šè¿‡ OpenAI çš„ Realtime API è¿›è¡Œè¯­éŸ³å¯¹è¯ã€‚æœ¬æŒ‡å—å°†å¸¦ä½ åˆ›å»ºç¬¬ä¸€ä¸ªå®æ—¶è¯­éŸ³æ™ºèƒ½ä½“ã€‚

!!! warning "æµ‹è¯•ç‰ˆåŠŸèƒ½"
Realtime æ™ºèƒ½ä½“ç›®å‰ä¸ºæµ‹è¯•ç‰ˆã€‚éšç€å®ç°ä¸æ–­æ”¹è¿›ï¼Œå¯èƒ½ä¼šå‡ºç°ä¸å…¼å®¹çš„å˜æ›´ã€‚

## å‰ç½®æ¡ä»¶

- Python 3.9 æˆ–æ›´é«˜ç‰ˆæœ¬
- OpenAI API key
- å¯¹ OpenAI Agents SDK æœ‰åŸºæœ¬äº†è§£

## å®‰è£…

å¦‚æœå°šæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… OpenAI Agents SDKï¼š

```bash
pip install openai-agents
```

## åˆ›å»ºä½ çš„ç¬¬ä¸€ä¸ªå®æ—¶æ™ºèƒ½ä½“

### 1. å¯¼å…¥æ‰€éœ€ç»„ä»¶

```python
import asyncio
from agents.realtime import RealtimeAgent, RealtimeRunner
```

### 2. åˆ›å»ºä¸€ä¸ªå®æ—¶æ™ºèƒ½ä½“

```python
agent = RealtimeAgent(
    name="Assistant",
    instructions="You are a helpful voice assistant. Keep your responses conversational and friendly.",
)
```

### 3. è®¾ç½®è¿è¡Œå™¨

```python
runner = RealtimeRunner(
    starting_agent=agent,
    config={
        "model_settings": {
            "model_name": "gpt-realtime",
            "voice": "ash",
            "modalities": ["audio"],
            "input_audio_format": "pcm16",
            "output_audio_format": "pcm16",
            "input_audio_transcription": {"model": "gpt-4o-mini-transcribe"},
            "turn_detection": {"type": "semantic_vad", "interrupt_response": True},
        }
    }
)
```

### 4. å¯åŠ¨ä¼šè¯

```python
# Start the session
session = await runner.run()

async with session:
    print("Session started! The agent will stream audio responses in real-time.")
    # Process events
    async for event in session:
        try:
            if event.type == "agent_start":
                print(f"Agent started: {event.agent.name}")
            elif event.type == "agent_end":
                print(f"Agent ended: {event.agent.name}")
            elif event.type == "handoff":
                print(f"Handoff from {event.from_agent.name} to {event.to_agent.name}")
            elif event.type == "tool_start":
                print(f"Tool started: {event.tool.name}")
            elif event.type == "tool_end":
                print(f"Tool ended: {event.tool.name}; output: {event.output}")
            elif event.type == "audio_end":
                print("Audio ended")
            elif event.type == "audio":
                # Enqueue audio for callback-based playback with metadata
                # Non-blocking put; queue is unbounded, so drops wonâ€™t occur.
                pass
            elif event.type == "audio_interrupted":
                print("Audio interrupted")
                # Begin graceful fade + flush in the audio callback and rebuild jitter buffer.
            elif event.type == "error":
                print(f"Error: {event.error}")
            elif event.type == "history_updated":
                pass  # Skip these frequent events
            elif event.type == "history_added":
                pass  # Skip these frequent events
            elif event.type == "raw_model_event":
                print(f"Raw model event: {_truncate_str(str(event.data), 200)}")
            else:
                print(f"Unknown event type: {event.type}")
        except Exception as e:
            print(f"Error processing event: {_truncate_str(str(e), 200)}")

def _truncate_str(s: str, max_length: int) -> str:
    if len(s) > max_length:
        return s[:max_length] + "..."
    return s
```

## å®Œæ•´ç¤ºä¾‹

ä¸‹é¢æ˜¯ä¸€ä¸ªå¯è¿è¡Œçš„å®Œæ•´ç¤ºä¾‹ï¼š

```python
import asyncio
from agents.realtime import RealtimeAgent, RealtimeRunner

async def main():
    # Create the agent
    agent = RealtimeAgent(
        name="Assistant",
        instructions="You are a helpful voice assistant. Keep responses brief and conversational.",
    )
    # Set up the runner with configuration
    runner = RealtimeRunner(
        starting_agent=agent,
        config={
            "model_settings": {
                "model_name": "gpt-realtime",
                "voice": "ash",
                "modalities": ["audio"],
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "input_audio_transcription": {"model": "gpt-4o-mini-transcribe"},
                "turn_detection": {"type": "semantic_vad", "interrupt_response": True},
            }
        },
    )
    # Start the session
    session = await runner.run()

    async with session:
        print("Session started! The agent will stream audio responses in real-time.")
        # Process events
        async for event in session:
            try:
                if event.type == "agent_start":
                    print(f"Agent started: {event.agent.name}")
                elif event.type == "agent_end":
                    print(f"Agent ended: {event.agent.name}")
                elif event.type == "handoff":
                    print(f"Handoff from {event.from_agent.name} to {event.to_agent.name}")
                elif event.type == "tool_start":
                    print(f"Tool started: {event.tool.name}")
                elif event.type == "tool_end":
                    print(f"Tool ended: {event.tool.name}; output: {event.output}")
                elif event.type == "audio_end":
                    print("Audio ended")
                elif event.type == "audio":
                    # Enqueue audio for callback-based playback with metadata
                    # Non-blocking put; queue is unbounded, so drops wonâ€™t occur.
                    pass
                elif event.type == "audio_interrupted":
                    print("Audio interrupted")
                    # Begin graceful fade + flush in the audio callback and rebuild jitter buffer.
                elif event.type == "error":
                    print(f"Error: {event.error}")
                elif event.type == "history_updated":
                    pass  # Skip these frequent events
                elif event.type == "history_added":
                    pass  # Skip these frequent events
                elif event.type == "raw_model_event":
                    print(f"Raw model event: {_truncate_str(str(event.data), 200)}")
                else:
                    print(f"Unknown event type: {event.type}")
            except Exception as e:
                print(f"Error processing event: {_truncate_str(str(e), 200)}")

def _truncate_str(s: str, max_length: int) -> str:
    if len(s) > max_length:
        return s[:max_length] + "..."
    return s

if __name__ == "__main__":
    # Run the session
    asyncio.run(main())
```

## é…ç½®é€‰é¡¹

### æ¨¡å‹è®¾ç½®

- `model_name`: ä»å¯ç”¨çš„å®æ—¶æ¨¡å‹ä¸­é€‰æ‹©ï¼ˆä¾‹å¦‚ï¼Œ`gpt-realtime`ï¼‰
- `voice`: é€‰æ‹©è¯­éŸ³ï¼ˆ`alloy`ã€`echo`ã€`fable`ã€`onyx`ã€`nova`ã€`shimmer`ï¼‰
- `modalities`: å¯ç”¨æ–‡æœ¬æˆ–éŸ³é¢‘ï¼ˆ`["text"]` æˆ– `["audio"]`ï¼‰

### éŸ³é¢‘è®¾ç½®

- `input_audio_format`: è¾“å…¥éŸ³é¢‘æ ¼å¼ï¼ˆ`pcm16`ã€`g711_ulaw`ã€`g711_alaw`ï¼‰
- `output_audio_format`: è¾“å‡ºéŸ³é¢‘æ ¼å¼
- `input_audio_transcription`: è½¬å†™é…ç½®

### è½®æ¬¡æ£€æµ‹

- `type`: æ£€æµ‹æ–¹å¼ï¼ˆ`server_vad`ã€`semantic_vad`ï¼‰
- `threshold`: è¯­éŸ³æ´»åŠ¨é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰
- `silence_duration_ms`: ç”¨äºæ£€æµ‹è½®æ¬¡ç»“æŸçš„é™éŸ³æ—¶é•¿
- `prefix_padding_ms`: è¯­éŸ³å¼€å§‹å‰çš„éŸ³é¢‘å¡«å……

## åç»­æ­¥éª¤

- [è¿›ä¸€æ­¥äº†è§£ Realtime æ™ºèƒ½ä½“](guide.md)
- æŸ¥çœ‹ [examples/realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime) æ–‡ä»¶å¤¹ä¸­çš„å¯è¿è¡Œç¤ºä¾‹
- ä¸ºä½ çš„æ™ºèƒ½ä½“æ·»åŠ å·¥å…·
- å®ç°æ™ºèƒ½ä½“ä¹‹é—´çš„ä»»åŠ¡è½¬ç§»
- è®¾ç½®å®‰å…¨é˜²æŠ¤æªæ–½ä»¥ä¿éšœå®‰å…¨

## èº«ä»½éªŒè¯

ç¡®ä¿å·²åœ¨ç¯å¢ƒä¸­è®¾ç½®ä½ çš„ OpenAI API keyï¼š

```bash
export OPENAI_API_KEY="your-api-key-here"
```

æˆ–åœ¨åˆ›å»ºä¼šè¯æ—¶ç›´æ¥ä¼ å…¥ï¼š

```python
session = await runner.run(model_config={"api_key": "your-api-key"})
```

================
File: docs/zh/voice/pipeline.md
================
---
search:
  exclude: true
---
# æµæ°´çº¿ä¸å·¥ä½œæµ

[`VoicePipeline`][agents.voice.pipeline.VoicePipeline] æ˜¯ä¸€ä¸ªç±»ï¼Œå¯è½»æ¾å°†ä½ çš„æ™ºèƒ½ä½“å·¥ä½œæµå˜æˆè¯­éŸ³åº”ç”¨ã€‚ä½ ä¼ å…¥è¦è¿è¡Œçš„å·¥ä½œæµï¼Œæµæ°´çº¿ä¼šè´Ÿè´£è½¬å†™è¾“å…¥éŸ³é¢‘ã€æ£€æµ‹éŸ³é¢‘ç»“æŸæ—¶é—´ã€åœ¨æ°å½“æ—¶æœºè°ƒç”¨ä½ çš„å·¥ä½œæµï¼Œå¹¶å°†å·¥ä½œæµè¾“å‡ºè½¬æ¢å›éŸ³é¢‘ã€‚

```mermaid
graph LR
    %% Input
    A["ğŸ¤ Audio Input"]

    %% Voice Pipeline
    subgraph Voice_Pipeline [Voice Pipeline]
        direction TB
        B["Transcribe (speech-to-text)"]
        C["Your Code"]:::highlight
        D["Text-to-speech"]
        B --> C --> D
    end

    %% Output
    E["ğŸ§ Audio Output"]

    %% Flow
    A --> Voice_Pipeline
    Voice_Pipeline --> E

    %% Custom styling
    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;

```

## é…ç½®æµæ°´çº¿

åˆ›å»ºæµæ°´çº¿æ—¶ï¼Œä½ å¯ä»¥è®¾ç½®ä»¥ä¸‹å†…å®¹ï¼š

1. [`workflow`][agents.voice.workflow.VoiceWorkflowBase]ï¼šæ¯æ¬¡æœ‰æ–°éŸ³é¢‘è¢«è½¬å†™æ—¶è¿è¡Œçš„ä»£ç ã€‚
2. ä½¿ç”¨çš„ [`speech-to-text`][agents.voice.model.STTModel] å’Œ [`text-to-speech`][agents.voice.model.TTSModel] æ¨¡å‹
3. [`config`][agents.voice.pipeline_config.VoicePipelineConfig]ï¼šç”¨äºé…ç½®ä»¥ä¸‹å†…å®¹ï¼š
    - æ¨¡å‹æä¾›è€…ï¼Œå¯å°†æ¨¡å‹åç§°æ˜ å°„åˆ°å…·ä½“æ¨¡å‹
    - è¿½è¸ªï¼ŒåŒ…æ‹¬æ˜¯å¦ç¦ç”¨è¿½è¸ªã€æ˜¯å¦ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ã€å·¥ä½œæµåç§°ã€è¿½è¸ª ID ç­‰
    - TTS ä¸ STT æ¨¡å‹è®¾ç½®ï¼Œå¦‚æç¤ºè¯ã€è¯­è¨€å’Œä½¿ç”¨çš„æ•°æ®ç±»å‹

## è¿è¡Œæµæ°´çº¿

ä½ å¯ä»¥é€šè¿‡ [`run()`][agents.voice.pipeline.VoicePipeline.run] æ–¹æ³•è¿è¡Œæµæ°´çº¿ï¼Œå¹¶ä»¥ä¸¤ç§å½¢å¼ä¹‹ä¸€ä¼ å…¥éŸ³é¢‘è¾“å…¥ï¼š

1. [`AudioInput`][agents.voice.input.AudioInput]ï¼šå½“ä½ å·²æœ‰å®Œæ•´ä¸€æ®µéŸ³é¢‘ï¼Œåªæƒ³ä¸ºå…¶ç”Ÿæˆç»“æœæ—¶ä½¿ç”¨ã€‚é€‚ç”¨äºæ— éœ€æ£€æµ‹è¯´è¯è€…ä½•æ—¶ç»“æŸçš„åœºæ™¯ï¼›ä¾‹å¦‚ï¼Œé¢„å½•éŸ³é¢‘ï¼Œæˆ–åœ¨æŒ‰é”®è¯´è¯åº”ç”¨ä¸­ç”¨æˆ·ç»“æŸè¯´è¯çš„æ—¶æœºæ˜¯æ˜ç¡®çš„ã€‚
2. [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]ï¼šå½“ä½ éœ€è¦æ£€æµ‹ç”¨æˆ·ä½•æ—¶è¯´å®Œæ—¶ä½¿ç”¨ã€‚å®ƒå…è®¸ä½ åœ¨æ£€æµ‹åˆ°éŸ³é¢‘å—æ—¶é€æ­¥æ¨é€ï¼Œè¯­éŸ³æµæ°´çº¿ä¼šé€šè¿‡â€œactivity detectionï¼ˆæ´»åŠ¨æ£€æµ‹ï¼‰â€åœ¨æ°å½“æ—¶æœºè‡ªåŠ¨è¿è¡Œæ™ºèƒ½ä½“å·¥ä½œæµã€‚

## ç»“æœ

è¯­éŸ³æµæ°´çº¿è¿è¡Œçš„ç»“æœæ˜¯ä¸€ä¸ª [`StreamedAudioResult`][agents.voice.result.StreamedAudioResult]ã€‚è¿™æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå…è®¸ä½ åœ¨äº‹ä»¶å‘ç”Ÿæ—¶è¿›è¡Œæµå¼ä¼ è¾“ã€‚[`VoiceStreamEvent`][agents.voice.events.VoiceStreamEvent] æœ‰å‡ ç§ç±»å‹ï¼ŒåŒ…æ‹¬ï¼š

1. [`VoiceStreamEventAudio`][agents.voice.events.VoiceStreamEventAudio]ï¼šåŒ…å«ä¸€æ®µéŸ³é¢‘æ•°æ®ã€‚
2. [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle]ï¼šå‘ŠçŸ¥å›åˆå¼€å§‹æˆ–ç»“æŸç­‰ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ã€‚
3. [`VoiceStreamEventError`][agents.voice.events.VoiceStreamEventError]ï¼šé”™è¯¯äº‹ä»¶ã€‚

```python

result = await pipeline.run(input)

async for event in result.stream():
    if event.type == "voice_stream_event_audio":
        # play audio
    elif event.type == "voice_stream_event_lifecycle":
        # lifecycle
    elif event.type == "voice_stream_event_error"
        # error
    ...
```

## æœ€ä½³å®è·µ

### ä¸­æ–­

Agents SDK ç›®å‰å°šä¸æ”¯æŒå¯¹ [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput] çš„ä»»ä½•å†…ç½®ä¸­æ–­åŠŸèƒ½ã€‚ç›¸åï¼Œå¯¹äºæ£€æµ‹åˆ°çš„æ¯ä¸ªå›åˆï¼Œå®ƒéƒ½ä¼šè§¦å‘ä½ çš„å·¥ä½œæµçš„ä¸€æ¬¡ç‹¬ç«‹è¿è¡Œã€‚å¦‚æœä½ å¸Œæœ›åœ¨åº”ç”¨å†…å¤„ç†ä¸­æ–­ï¼Œå¯ä»¥ç›‘å¬ [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle] äº‹ä»¶ã€‚`turn_started` è¡¨ç¤ºæ–°çš„å›åˆå·²è¢«è½¬å†™å¹¶å¼€å§‹å¤„ç†ï¼›`turn_ended` ä¼šåœ¨ç›¸åº”å›åˆçš„æ‰€æœ‰éŸ³é¢‘éƒ½å·²åˆ†å‘åè§¦å‘ã€‚ä½ å¯ä»¥åˆ©ç”¨è¿™äº›äº‹ä»¶ï¼Œåœ¨æ¨¡å‹å¼€å§‹ä¸€ä¸ªå›åˆæ—¶å°†è¯´è¯è€…çš„éº¦å…‹é£é™éŸ³ï¼Œå¹¶åœ¨ä½ å‘é€å®Œè¯¥å›åˆçš„æ‰€æœ‰ç›¸å…³éŸ³é¢‘åå–æ¶ˆé™éŸ³ã€‚

================
File: docs/zh/voice/quickstart.md
================
---
search:
  exclude: true
---
# å¿«é€Ÿå¼€å§‹

## å…ˆå†³æ¡ä»¶

è¯·ç¡®ä¿ä½ å·²æŒ‰ç…§ Agents SDK çš„åŸºç¡€[å¿«é€Ÿå¼€å§‹è¯´æ˜](../quickstart.md)å®Œæˆè®¾ç½®ï¼Œå¹¶åˆ›å»ºäº†è™šæ‹Ÿç¯å¢ƒã€‚ç„¶åï¼Œä» SDK å®‰è£…å¯é€‰çš„è¯­éŸ³ç›¸å…³ä¾èµ–é¡¹ï¼š

```bash
pip install 'openai-agents[voice]'
```

## æ¦‚å¿µ

ä¸»è¦æ¦‚å¿µæ˜¯ä¸€ä¸ª [`VoicePipeline`][agents.voice.pipeline.VoicePipeline]ï¼Œå®ƒåŒ…å« 3 ä¸ªæ­¥éª¤ï¼š

1. è¿è¡Œè¯­éŸ³è½¬æ–‡æœ¬æ¨¡å‹å°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡æœ¬ã€‚
2. è¿è¡Œä½ çš„ä»£ç ï¼ˆé€šå¸¸æ˜¯æ™ºèƒ½ä½“å·¥ä½œæµï¼‰ä»¥ç”Ÿæˆç»“æœã€‚
3. è¿è¡Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹å°†ç»“æœæ–‡æœ¬è½¬æ¢å›éŸ³é¢‘ã€‚

```mermaid
graph LR
    %% Input
    A["ğŸ¤ Audio Input"]

    %% Voice Pipeline
    subgraph Voice_Pipeline [Voice Pipeline]
        direction TB
        B["Transcribe (speech-to-text)"]
        C["Your Code"]:::highlight
        D["Text-to-speech"]
        B --> C --> D
    end

    %% Output
    E["ğŸ§ Audio Output"]

    %% Flow
    A --> Voice_Pipeline
    Voice_Pipeline --> E

    %% Custom styling
    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;

```

## æ™ºèƒ½ä½“

é¦–å…ˆï¼Œè®©æˆ‘ä»¬è®¾ç½®ä¸€äº›æ™ºèƒ½ä½“ã€‚å¦‚æœä½ æ›¾ä½¿ç”¨æ­¤ SDK æ„å»ºè¿‡æ™ºèƒ½ä½“ï¼Œè¿™éƒ¨åˆ†ä¼šå¾ˆç†Ÿæ‚‰ã€‚æˆ‘ä»¬å°†åˆ›å»ºå‡ ä¸ªæ™ºèƒ½ä½“ã€ä¸€ä¸ªä»»åŠ¡è½¬ç§»ï¼Œä»¥åŠä¸€ä¸ªå·¥å…·ã€‚

```python
import asyncio
import random

from agents import (
    Agent,
    function_tool,
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions



@function_tool
def get_weather(city: str) -> str:
    """Get the weather for a given city."""
    print(f"[debug] get_weather called with city: {city}")
    choices = ["sunny", "cloudy", "rainy", "snowy"]
    return f"The weather in {city} is {random.choice(choices)}."


spanish_agent = Agent(
    name="Spanish",
    handoff_description="A spanish speaking agent.",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. Speak in Spanish.",
    ),
    model="gpt-4.1",
)

agent = Agent(
    name="Assistant",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.",
    ),
    model="gpt-4.1",
    handoffs=[spanish_agent],
    tools=[get_weather],
)
```

## è¯­éŸ³æµæ°´çº¿

æˆ‘ä»¬å°†è®¾ç½®ä¸€ä¸ªç®€å•çš„è¯­éŸ³æµæ°´çº¿ï¼Œä½¿ç”¨ [`SingleAgentVoiceWorkflow`][agents.voice.workflow.SingleAgentVoiceWorkflow] ä½œä¸ºå·¥ä½œæµã€‚

```python
from agents.voice import SingleAgentVoiceWorkflow, VoicePipeline
pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))
```

## æµæ°´çº¿è¿è¡Œ

```python
import numpy as np
import sounddevice as sd
from agents.voice import AudioInput

# For simplicity, we'll just create 3 seconds of silence
# In reality, you'd get microphone data
buffer = np.zeros(24000 * 3, dtype=np.int16)
audio_input = AudioInput(buffer=buffer)

result = await pipeline.run(audio_input)

# Create an audio player using `sounddevice`
player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
player.start()

# Play the audio stream as it comes in
async for event in result.stream():
    if event.type == "voice_stream_event_audio":
        player.write(event.data)

```

## æ•´åˆ

```python
import asyncio
import random

import numpy as np
import sounddevice as sd

from agents import (
    Agent,
    function_tool,
    set_tracing_disabled,
)
from agents.voice import (
    AudioInput,
    SingleAgentVoiceWorkflow,
    VoicePipeline,
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions


@function_tool
def get_weather(city: str) -> str:
    """Get the weather for a given city."""
    print(f"[debug] get_weather called with city: {city}")
    choices = ["sunny", "cloudy", "rainy", "snowy"]
    return f"The weather in {city} is {random.choice(choices)}."


spanish_agent = Agent(
    name="Spanish",
    handoff_description="A spanish speaking agent.",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. Speak in Spanish.",
    ),
    model="gpt-4.1",
)

agent = Agent(
    name="Assistant",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.",
    ),
    model="gpt-4.1",
    handoffs=[spanish_agent],
    tools=[get_weather],
)


async def main():
    pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))
    buffer = np.zeros(24000 * 3, dtype=np.int16)
    audio_input = AudioInput(buffer=buffer)

    result = await pipeline.run(audio_input)

    # Create an audio player using `sounddevice`
    player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
    player.start()

    # Play the audio stream as it comes in
    async for event in result.stream():
        if event.type == "voice_stream_event_audio":
            player.write(event.data)


if __name__ == "__main__":
    asyncio.run(main())
```

å¦‚æœä½ è¿è¡Œè¿™ä¸ªç¤ºä¾‹ï¼Œæ™ºèƒ½ä½“ä¼šå¯¹ä½ è¯´è¯ï¼æŸ¥çœ‹ [examples/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static) ç¤ºä¾‹ï¼Œä½“éªŒä¸€ä¸ªä½ å¯ä»¥äº²è‡ªä¸æ™ºèƒ½ä½“å¯¹è¯çš„æ¼”ç¤ºã€‚

================
File: docs/zh/voice/tracing.md
================
---
search:
  exclude: true
---
# è¿½è¸ª

ä¸[æ™ºèƒ½ä½“æ˜¯å¦‚ä½•è¢«è¿½è¸ªçš„](../tracing.md)ç›¸åŒï¼Œè¯­éŸ³æµæ°´çº¿ä¹Ÿä¼šè¢«è‡ªåŠ¨è¿½è¸ªã€‚

ä½ å¯ä»¥å‚è€ƒä¸Šé¢çš„è¿½è¸ªæ–‡æ¡£è·å–åŸºç¡€ä¿¡æ¯ï¼Œæ­¤å¤–è¿˜å¯ä»¥é€šè¿‡[`VoicePipelineConfig`][agents.voice.pipeline_config.VoicePipelineConfig]å¯¹æµæ°´çº¿çš„è¿½è¸ªè¿›è¡Œé…ç½®ã€‚

ä¸è¿½è¸ªç›¸å…³çš„å…³é”®å­—æ®µåŒ…æ‹¬ï¼š

- [`tracing_disabled`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]ï¼šæ§åˆ¶æ˜¯å¦ç¦ç”¨è¿½è¸ªã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿½è¸ªæ˜¯å¯ç”¨çš„ã€‚
- [`trace_include_sensitive_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_data]ï¼šæ§åˆ¶è¿½è¸ªä¸­æ˜¯å¦åŒ…å«æ½œåœ¨çš„æ•æ„Ÿæ•°æ®ï¼Œä¾‹å¦‚éŸ³é¢‘è½¬å½•æ–‡æœ¬ã€‚æ­¤è®¾ç½®ä»…é€‚ç”¨äºè¯­éŸ³æµæ°´çº¿ï¼Œä¸é€‚ç”¨äºä½ çš„ Workflow å†…éƒ¨å‘ç”Ÿçš„ä»»ä½•äº‹æƒ…ã€‚
- [`trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data]ï¼šæ§åˆ¶è¿½è¸ªä¸­æ˜¯å¦åŒ…å«éŸ³é¢‘æ•°æ®ã€‚
- [`workflow_name`][agents.voice.pipeline_config.VoicePipelineConfig.workflow_name]ï¼šè¿½è¸ªå·¥ä½œæµçš„åç§°ã€‚
- [`group_id`][agents.voice.pipeline_config.VoicePipelineConfig.group_id]ï¼šè¿½è¸ªçš„`group_id`ï¼Œå¯ç”¨äºå…³è”å¤šä¸ªè¿½è¸ªã€‚
- [`trace_metadata`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]ï¼šéœ€è¦éšè¿½è¸ªä¸€å¹¶åŒ…å«çš„é™„åŠ å…ƒæ•°æ®ã€‚

================
File: docs/zh/agents.md
================
---
search:
  exclude: true
---
# æ™ºèƒ½ä½“

æ™ºèƒ½ä½“æ˜¯ä½ åº”ç”¨ä¸­çš„æ ¸å¿ƒæ„å»ºå—ã€‚ä¸€ä¸ªæ™ºèƒ½ä½“æ˜¯é…ç½®äº† instructions å’Œå·¥å…·çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚

## åŸºæœ¬é…ç½®

ä½ æœ€å¸¸ä¸ºæ™ºèƒ½ä½“é…ç½®çš„å±æ€§åŒ…æ‹¬ï¼š

- `name`: æ ‡è¯†ä½ çš„æ™ºèƒ½ä½“çš„å¿…å¡«å­—ç¬¦ä¸²ã€‚
- `instructions`: ä¹Ÿç§°ä¸ºå¼€å‘è€…æ¶ˆæ¯æˆ– system promptï¼ˆç³»ç»Ÿæç¤ºè¯ï¼‰ã€‚
- `model`: ä½¿ç”¨å“ªä¸ª LLMï¼Œä»¥åŠå¯é€‰çš„ `model_settings`ï¼Œç”¨äºé…ç½®å¦‚ temperatureã€top_p ç­‰æ¨¡å‹è°ƒä¼˜å‚æ•°ã€‚
- `tools`: æ™ºèƒ½ä½“å¯ç”¨äºå®Œæˆä»»åŠ¡çš„å·¥å…·ã€‚

```python
from agents import Agent, ModelSettings, function_tool

@function_tool
def get_weather(city: str) -> str:
    """returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Haiku agent",
    instructions="Always respond in haiku form",
    model="gpt-5-nano",
    tools=[get_weather],
)
```

## ä¸Šä¸‹æ–‡

æ™ºèƒ½ä½“åœ¨å…¶ `context` ç±»å‹ä¸Šæ˜¯æ³›å‹çš„ã€‚Context æ˜¯ä¸€ç§ä¾èµ–æ³¨å…¥å·¥å…·ï¼šå®ƒæ˜¯ä¸€ä¸ªä½ åˆ›å»ºå¹¶ä¼ ç»™ `Runner.run()` çš„å¯¹è±¡ï¼Œä¼šä¼ é€’ç»™æ¯ä¸ªæ™ºèƒ½ä½“ã€å·¥å…·ã€ä»»åŠ¡è½¬ç§»ç­‰ï¼Œç”¨ä½œæœ¬æ¬¡è¿è¡Œçš„ä¾èµ–ä¸çŠ¶æ€é›†åˆã€‚ä½ å¯ä»¥å°†ä»»æ„ Python å¯¹è±¡ä½œä¸º context æä¾›ã€‚

```python
@dataclass
class UserContext:
    name: str
    uid: str
    is_pro_user: bool

    async def fetch_purchases() -> list[Purchase]:
        return ...

agent = Agent[UserContext](
    ...,
)
```

## è¾“å‡ºç±»å‹

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ™ºèƒ½ä½“äº§å‡ºçº¯æ–‡æœ¬ï¼ˆå³ `str`ï¼‰è¾“å‡ºã€‚å¦‚æœä½ å¸Œæœ›æ™ºèƒ½ä½“äº§å‡ºç‰¹å®šç±»å‹çš„è¾“å‡ºï¼Œä½ å¯ä»¥ä½¿ç”¨ `output_type` å‚æ•°ã€‚å¸¸è§çš„åšæ³•æ˜¯ä½¿ç”¨ [Pydantic](https://docs.pydantic.dev/) å¯¹è±¡ï¼Œä½†æˆ‘ä»¬æ”¯æŒä»»ä½•å¯ç”± Pydantic çš„ [TypeAdapter](https://docs.pydantic.dev/latest/api/type_adapter/) åŒ…è£…çš„ç±»å‹â€”â€”dataclassesã€åˆ—è¡¨ã€TypedDict ç­‰ã€‚

```python
from pydantic import BaseModel
from agents import Agent


class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

agent = Agent(
    name="Calendar extractor",
    instructions="Extract calendar events from text",
    output_type=CalendarEvent,
)
```

!!! note

    å½“ä½ ä¼ å…¥ `output_type` æ—¶ï¼Œè¿™ä¼šå‘Šè¯‰æ¨¡å‹ä½¿ç”¨ [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) è€Œéå¸¸è§„çš„çº¯æ–‡æœ¬å“åº”ã€‚

## å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡æ¨¡å¼

è®¾è®¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæœ‰å¾ˆå¤šæ–¹å¼ï¼Œä½†æˆ‘ä»¬å¸¸è§åˆ°ä¸¤ç§å¹¿æ³›é€‚ç”¨çš„æ¨¡å¼ï¼š

1. ç®¡ç†è€…ï¼ˆæ™ºèƒ½ä½“ä½œä¸ºå·¥å…·ï¼‰ï¼šä¸€ä¸ªä¸­å¿ƒç®¡ç†è€…/ç¼–æ’è€…å°†ä¸“ä¸šçš„å­æ™ºèƒ½ä½“ä½œä¸ºå·¥å…·è°ƒç”¨ï¼Œå¹¶ä¿æŒå¯¹è¯æ§åˆ¶æƒã€‚
2. ä»»åŠ¡è½¬ç§»ï¼šå¯¹ç­‰çš„æ™ºèƒ½ä½“å°†æ§åˆ¶æƒè½¬äº¤ç»™ä¸€ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼Œç”±å…¶æ¥ç®¡å¯¹è¯ã€‚è¿™æ˜¯å»ä¸­å¿ƒåŒ–çš„ã€‚

æ›´å¤šç»†èŠ‚å‚è§[æˆ‘ä»¬çš„æ„å»ºæ™ºèƒ½ä½“å®è·µæŒ‡å—](https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf)ã€‚

### ç®¡ç†è€…ï¼ˆæ™ºèƒ½ä½“ä½œä¸ºå·¥å…·ï¼‰

`customer_facing_agent` è´Ÿè´£æ‰€æœ‰ç”¨æˆ·äº¤äº’ï¼Œå¹¶è°ƒç”¨ä»¥å·¥å…·å½¢å¼æš´éœ²çš„ä¸“ä¸šå­æ™ºèƒ½ä½“ã€‚è¯¦è§ [tools](tools.md#agents-as-tools) æ–‡æ¡£ã€‚

```python
from agents import Agent

booking_agent = Agent(...)
refund_agent = Agent(...)

customer_facing_agent = Agent(
    name="Customer-facing agent",
    instructions=(
        "Handle all direct user communication. "
        "Call the relevant tools when specialized expertise is needed."
    ),
    tools=[
        booking_agent.as_tool(
            tool_name="booking_expert",
            tool_description="Handles booking questions and requests.",
        ),
        refund_agent.as_tool(
            tool_name="refund_expert",
            tool_description="Handles refund questions and requests.",
        )
    ],
)
```

### ä»»åŠ¡è½¬ç§»

ä»»åŠ¡è½¬ç§»æ˜¯æ™ºèƒ½ä½“å¯å§”æ´¾ç»™çš„å­æ™ºèƒ½ä½“ã€‚å½“å‘ç”Ÿä»»åŠ¡è½¬ç§»æ—¶ï¼Œè¢«å§”æ´¾çš„æ™ºèƒ½ä½“ä¼šæ¥æ”¶å¯¹è¯å†å²å¹¶æ¥ç®¡å¯¹è¯ã€‚æ­¤æ¨¡å¼æ”¯æŒæ¨¡å—åŒ–ã€ä¸“ç²¾çš„æ™ºèƒ½ä½“ï¼Œä½¿å…¶åœ¨å•ä¸€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚è¯¦è§ [handoffs](handoffs.md) æ–‡æ¡£ã€‚

```python
from agents import Agent

booking_agent = Agent(...)
refund_agent = Agent(...)

triage_agent = Agent(
    name="Triage agent",
    instructions=(
        "Help the user with their questions. "
        "If they ask about booking, hand off to the booking agent. "
        "If they ask about refunds, hand off to the refund agent."
    ),
    handoffs=[booking_agent, refund_agent],
)
```

## åŠ¨æ€ instructions

å¤šæ•°æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥åœ¨åˆ›å»ºæ™ºèƒ½ä½“æ—¶æä¾› instructionsã€‚ä¸è¿‡ï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡å‡½æ•°æä¾›åŠ¨æ€ instructionsã€‚è¯¥å‡½æ•°å°†æ¥æ”¶æ™ºèƒ½ä½“ä¸ contextï¼Œå¹¶ä¸”å¿…é¡»è¿”å›æç¤ºè¯ã€‚å¸¸è§„ä¸ `async` å‡½æ•°å‡è¢«æ¥å—ã€‚

```python
def dynamic_instructions(
    context: RunContextWrapper[UserContext], agent: Agent[UserContext]
) -> str:
    return f"The user's name is {context.context.name}. Help them with their questions."


agent = Agent[UserContext](
    name="Triage agent",
    instructions=dynamic_instructions,
)
```

## ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ï¼ˆhooksï¼‰

æœ‰æ—¶ä½ å¸Œæœ›è§‚å¯Ÿæ™ºèƒ½ä½“çš„ç”Ÿå‘½å‘¨æœŸã€‚ä¾‹å¦‚ï¼Œä½ å¯èƒ½å¸Œæœ›è®°å½•äº‹ä»¶ï¼Œæˆ–åœ¨æŸäº›äº‹ä»¶å‘ç”Ÿæ—¶é¢„å–æ•°æ®ã€‚ä½ å¯ä»¥é€šè¿‡ `hooks` å±æ€§æŒ‚æ¥åˆ°æ™ºèƒ½ä½“çš„ç”Ÿå‘½å‘¨æœŸã€‚å­ç±»åŒ– [`AgentHooks`][agents.lifecycle.AgentHooks]ï¼Œå¹¶é‡å†™ä½ æ„Ÿå…´è¶£çš„æ–¹æ³•ã€‚

## å®‰å…¨é˜²æŠ¤æªæ–½

å®‰å…¨é˜²æŠ¤æªæ–½å…è®¸ä½ åœ¨æ™ºèƒ½ä½“è¿è¡Œçš„åŒæ—¶å¯¹ç”¨æˆ·è¾“å…¥è¿›è¡Œæ£€æŸ¥/éªŒè¯ï¼Œå¹¶åœ¨æ™ºèƒ½ä½“äº§ç”Ÿè¾“å‡ºåå¯¹å…¶è¿›è¡Œæ£€æŸ¥/éªŒè¯ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥å¯¹ç”¨æˆ·è¾“å…¥å’Œæ™ºèƒ½ä½“è¾“å‡ºè¿›è¡Œç›¸å…³æ€§ç­›æŸ¥ã€‚è¯¦è§ [guardrails](guardrails.md) æ–‡æ¡£ã€‚

## å…‹éš†/å¤åˆ¶æ™ºèƒ½ä½“

é€šè¿‡åœ¨æ™ºèƒ½ä½“ä¸Šä½¿ç”¨ `clone()` æ–¹æ³•ï¼Œä½ å¯ä»¥å¤åˆ¶ä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œå¹¶å¯é€‰åœ°æ›´æ”¹ä»»æ„ä½ æƒ³ä¿®æ”¹çš„å±æ€§ã€‚

```python
pirate_agent = Agent(
    name="Pirate",
    instructions="Write like a pirate",
    model="gpt-4.1",
)

robot_agent = pirate_agent.clone(
    name="Robot",
    instructions="Write like a robot",
)
```

## å¼ºåˆ¶å·¥å…·ä½¿ç”¨

æä¾›å·¥å…·åˆ—è¡¨å¹¶ä¸æ€»æ„å‘³ç€ LLM ä¼šä½¿ç”¨å·¥å…·ã€‚ä½ å¯ä»¥é€šè¿‡è®¾ç½® [`ModelSettings.tool_choice`][agents.model_settings.ModelSettings.tool_choice] æ¥å¼ºåˆ¶ä½¿ç”¨å·¥å…·ã€‚å¯ç”¨å€¼ä¸ºï¼š

1. `auto`ï¼šå…è®¸ LLM è‡ªè¡Œå†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·ã€‚
2. `required`ï¼šè¦æ±‚ LLM å¿…é¡»ä½¿ç”¨æŸä¸ªå·¥å…·ï¼ˆä½†å®ƒå¯ä»¥æ™ºèƒ½åœ°é€‰æ‹©å“ªä¸ªå·¥å…·ï¼‰ã€‚
3. `none`ï¼šè¦æ±‚ LLM ä¸ä½¿ç”¨å·¥å…·ã€‚
4. è®¾ç½®ç‰¹å®šå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ `my_tool`ï¼šè¦æ±‚ LLM ä½¿ç”¨è¯¥ç‰¹å®šå·¥å…·ã€‚

```python
from agents import Agent, Runner, function_tool, ModelSettings

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Weather Agent",
    instructions="Retrieve weather details.",
    tools=[get_weather],
    model_settings=ModelSettings(tool_choice="get_weather")
)
```

## å·¥å…·ä½¿ç”¨è¡Œä¸º

`Agent` é…ç½®ä¸­çš„ `tool_use_behavior` å‚æ•°æ§åˆ¶å¦‚ä½•å¤„ç†å·¥å…·è¾“å‡ºï¼š

- `"run_llm_again"`ï¼šé»˜è®¤å€¼ã€‚è¿è¡Œå·¥å…·åï¼Œç”± LLM å¤„ç†ç»“æœä»¥ç”Ÿæˆæœ€ç»ˆå“åº”ã€‚
- `"stop_on_first_tool"`ï¼šç¬¬ä¸€æ¬¡å·¥å…·è°ƒç”¨çš„è¾“å‡ºç›´æ¥ä½œä¸ºæœ€ç»ˆå“åº”ï¼Œä¸å†è¿›è¡Œåç»­çš„ LLM å¤„ç†ã€‚

```python
from agents import Agent, Runner, function_tool, ModelSettings

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Weather Agent",
    instructions="Retrieve weather details.",
    tools=[get_weather],
    tool_use_behavior="stop_on_first_tool"
)
```

- `StopAtTools(stop_at_tool_names=[...])`ï¼šå¦‚æœè°ƒç”¨äº†ä»»ä¸€æŒ‡å®šå·¥å…·åˆ™åœæ­¢ï¼Œå¹¶ä½¿ç”¨å…¶è¾“å‡ºä½œä¸ºæœ€ç»ˆå“åº”ã€‚

```python
from agents import Agent, Runner, function_tool
from agents.agent import StopAtTools

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

@function_tool
def sum_numbers(a: int, b: int) -> int:
    """Adds two numbers."""
    return a + b

agent = Agent(
    name="Stop At Stock Agent",
    instructions="Get weather or sum numbers.",
    tools=[get_weather, sum_numbers],
    tool_use_behavior=StopAtTools(stop_at_tool_names=["get_weather"])
)
```

- `ToolsToFinalOutputFunction`ï¼šè‡ªå®šä¹‰å‡½æ•°ï¼Œç”¨äºå¤„ç†å·¥å…·ç»“æœå¹¶å†³å®šæ˜¯åœæ­¢è¿˜æ˜¯ç»§ç»­ç”± LLM å¤„ç†ã€‚

```python
from agents import Agent, Runner, function_tool, FunctionToolResult, RunContextWrapper
from agents.agent import ToolsToFinalOutputResult
from typing import List, Any

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

def custom_tool_handler(
    context: RunContextWrapper[Any],
    tool_results: List[FunctionToolResult]
) -> ToolsToFinalOutputResult:
    """Processes tool results to decide final output."""
    for result in tool_results:
        if result.output and "sunny" in result.output:
            return ToolsToFinalOutputResult(
                is_final_output=True,
                final_output=f"Final weather: {result.output}"
            )
    return ToolsToFinalOutputResult(
        is_final_output=False,
        final_output=None
    )

agent = Agent(
    name="Weather Agent",
    instructions="Retrieve weather details.",
    tools=[get_weather],
    tool_use_behavior=custom_tool_handler
)
```

!!! note

    ä¸ºé˜²æ­¢æ— é™å¾ªç¯ï¼Œæ¡†æ¶ä¼šåœ¨ä¸€æ¬¡å·¥å…·è°ƒç”¨åè‡ªåŠ¨å°† `tool_choice` é‡ç½®ä¸º "auto"ã€‚å¯é€šè¿‡ [`agent.reset_tool_choice`][agents.agent.Agent.reset_tool_choice] é…ç½®æ­¤è¡Œä¸ºã€‚äº§ç”Ÿæ— é™å¾ªç¯çš„åŸå› æ˜¯å·¥å…·ç»“æœä¼šè¢«å‘é€ç»™ LLMï¼Œè€Œç”±äº `tool_choice`ï¼ŒLLM ä¼šå†æ¬¡ç”Ÿæˆå·¥å…·è°ƒç”¨ï¼Œå¦‚æ­¤å¾€å¤ã€‚

================
File: docs/zh/config.md
================
---
search:
  exclude: true
---
# é…ç½® SDK

## API å¯†é’¥ä¸å®¢æˆ·ç«¯

é»˜è®¤æƒ…å†µä¸‹ï¼ŒSDK åœ¨è¢«å¯¼å…¥åä¼šç«‹å³ä½¿ç”¨ `OPENAI_API_KEY` ç¯å¢ƒå˜é‡æ¥å¤„ç† LLM è¯·æ±‚å’Œè¿½è¸ªã€‚å¦‚æœä½ æ— æ³•åœ¨åº”ç”¨å¯åŠ¨å‰è®¾ç½®è¯¥ç¯å¢ƒå˜é‡ï¼Œå¯ä»¥ä½¿ç”¨ [set_default_openai_key()][agents.set_default_openai_key] å‡½æ•°æ¥è®¾ç½®å¯†é’¥ã€‚

```python
from agents import set_default_openai_key

set_default_openai_key("sk-...")
```

æˆ–è€…ï¼Œä½ ä¹Ÿå¯ä»¥é…ç½®è¦ä½¿ç”¨çš„ OpenAI å®¢æˆ·ç«¯ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒSDK ä¼šåˆ›å»ºä¸€ä¸ª `AsyncOpenAI` å®ä¾‹ï¼Œå¹¶ä½¿ç”¨æ¥è‡ªç¯å¢ƒå˜é‡æˆ–ä¸Šè¿°é»˜è®¤å¯†é’¥çš„ API keyã€‚ä½ å¯ä»¥é€šè¿‡ [set_default_openai_client()][agents.set_default_openai_client] å‡½æ•°æ›´æ”¹è¿™ä¸€è¡Œä¸ºã€‚

```python
from openai import AsyncOpenAI
from agents import set_default_openai_client

custom_client = AsyncOpenAI(base_url="...", api_key="...")
set_default_openai_client(custom_client)
```

æœ€åï¼Œä½ ä¹Ÿå¯ä»¥è‡ªå®šä¹‰æ‰€ä½¿ç”¨çš„ OpenAI APIã€‚é»˜è®¤ä½¿ç”¨ OpenAI Responses APIã€‚ä½ å¯ä»¥é€šè¿‡ [set_default_openai_api()][agents.set_default_openai_api] å‡½æ•°æ”¹ä¸ºä½¿ç”¨ Chat Completions APIã€‚

```python
from agents import set_default_openai_api

set_default_openai_api("chat_completions")
```

## è¿½è¸ª

è¿½è¸ªé»˜è®¤å¼€å¯ã€‚é»˜è®¤ä¼šä½¿ç”¨ä¸Šè¿°éƒ¨åˆ†çš„ OpenAI API keyï¼ˆå³ç¯å¢ƒå˜é‡æˆ–ä½ è®¾ç½®çš„é»˜è®¤å¯†é’¥ï¼‰ã€‚ä½ å¯ä»¥é€šè¿‡ [`set_tracing_export_api_key`][agents.set_tracing_export_api_key] å‡½æ•°ä¸“é—¨è®¾ç½®ç”¨äºè¿½è¸ªçš„ API keyã€‚

```python
from agents import set_tracing_export_api_key

set_tracing_export_api_key("sk-...")
```

ä½ ä¹Ÿå¯ä»¥é€šè¿‡ [`set_tracing_disabled()`][agents.set_tracing_disabled] å‡½æ•°å®Œå…¨ç¦ç”¨è¿½è¸ªã€‚

```python
from agents import set_tracing_disabled

set_tracing_disabled(True)
```

## è°ƒè¯•æ—¥å¿—

è¯¥ SDK æä¾›äº†ä¸¤ä¸ªæœªè®¾ç½®ä»»ä½•å¤„ç†å™¨çš„ Python æ—¥å¿—è®°å½•å™¨ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿™æ„å‘³ç€è­¦å‘Šå’Œé”™è¯¯ä¼šå‘é€åˆ° `stdout`ï¼Œè€Œå…¶ä»–æ—¥å¿—ä¼šè¢«æŠ‘åˆ¶ã€‚

è¦å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼Œè¯·ä½¿ç”¨ [`enable_verbose_stdout_logging()`][agents.enable_verbose_stdout_logging] å‡½æ•°ã€‚

```python
from agents import enable_verbose_stdout_logging

enable_verbose_stdout_logging()
```

æˆ–è€…ï¼Œä½ å¯ä»¥é€šè¿‡æ·»åŠ å¤„ç†å™¨ã€è¿‡æ»¤å™¨ã€æ ¼å¼åŒ–å™¨ç­‰è‡ªå®šä¹‰æ—¥å¿—ã€‚æ›´å¤šå†…å®¹å¯å‚é˜… [Python logging æŒ‡å—](https://docs.python.org/3/howto/logging.html)ã€‚

```python
import logging

logger = logging.getLogger("openai.agents") # or openai.agents.tracing for the Tracing logger

# To make all logs show up
logger.setLevel(logging.DEBUG)
# To make info and above show up
logger.setLevel(logging.INFO)
# To make warning and above show up
logger.setLevel(logging.WARNING)
# etc

# You can customize this as needed, but this will output to `stderr` by default
logger.addHandler(logging.StreamHandler())
```

### æ—¥å¿—ä¸­çš„æ•æ„Ÿæ•°æ®

æŸäº›æ—¥å¿—å¯èƒ½åŒ…å«æ•æ„Ÿæ•°æ®ï¼ˆä¾‹å¦‚ç”¨æˆ·æ•°æ®ï¼‰ã€‚å¦‚æœä½ æƒ³ç¦ç”¨è¿™äº›æ•°æ®çš„æ—¥å¿—è®°å½•ï¼Œè¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ã€‚

ç¦ç”¨è®°å½• LLM çš„è¾“å…¥å’Œè¾“å‡ºï¼š

```bash
export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1
```

ç¦ç”¨è®°å½•å·¥å…·çš„è¾“å…¥å’Œè¾“å‡ºï¼š

```bash
export OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1
```

================
File: docs/zh/context.md
================
---
search:
  exclude: true
---
# ä¸Šä¸‹æ–‡ç®¡ç†

â€œä¸Šä¸‹æ–‡â€æ˜¯ä¸€ä¸ªå«ä¹‰å¹¿æ³›çš„æœ¯è¯­ã€‚ä½ å¯èƒ½å…³å¿ƒä¸¤å¤§ç±»ä¸Šä¸‹æ–‡ï¼š

1. ä»£ç æœ¬åœ°å¯ç”¨çš„ä¸Šä¸‹æ–‡ï¼šè¿™æ˜¯å·¥å…·å‡½æ•°è¿è¡Œæ—¶ã€`on_handoff` ç­‰å›è°ƒã€ç”Ÿå‘½å‘¨æœŸé’©å­ç­‰å¯èƒ½éœ€è¦çš„æ•°æ®å’Œä¾èµ–ã€‚
2. LLM å¯ç”¨çš„ä¸Šä¸‹æ–‡ï¼šè¿™æ˜¯ LLM åœ¨ç”Ÿæˆå“åº”æ—¶èƒ½çœ‹åˆ°çš„æ•°æ®ã€‚

## æœ¬åœ°ä¸Šä¸‹æ–‡

è¿™é€šè¿‡ [`RunContextWrapper`][agents.run_context.RunContextWrapper] ç±»ä»¥åŠå…¶ä¸­çš„ [`context`][agents.run_context.RunContextWrapper.context] å±æ€§æ¥è¡¨ç¤ºã€‚å…¶å·¥ä½œæ–¹å¼ä¸ºï¼š

1. ä½ åˆ›å»ºä»»æ„ Python å¯¹è±¡ã€‚å¸¸è§åšæ³•æ˜¯ä½¿ç”¨ dataclass æˆ– Pydantic å¯¹è±¡ã€‚
2. å°†è¯¥å¯¹è±¡ä¼ é€’ç»™å„ç§è¿è¡Œæ–¹æ³•ï¼ˆä¾‹å¦‚ `Runner.run(..., **context=whatever**)`ï¼‰ã€‚
3. æ‰€æœ‰å·¥å…·è°ƒç”¨ã€ç”Ÿå‘½å‘¨æœŸé’©å­ç­‰éƒ½ä¼šæ¥æ”¶ä¸€ä¸ªåŒ…è£…å¯¹è±¡ `RunContextWrapper[T]`ï¼Œå…¶ä¸­ `T` è¡¨ç¤ºä½ çš„ä¸Šä¸‹æ–‡å¯¹è±¡ç±»å‹ï¼Œä½ å¯ä»¥é€šè¿‡ `wrapper.context` è®¿é—®ã€‚

éœ€è¦ç‰¹åˆ«æ³¨æ„çš„å…³é”®ç‚¹ï¼šå¯¹äºæŸæ¬¡ agent è¿è¡Œï¼Œæ‰€æœ‰çš„æ™ºèƒ½ä½“ã€å·¥å…·å‡½æ•°ã€ç”Ÿå‘½å‘¨æœŸç­‰éƒ½å¿…é¡»ä½¿ç”¨ç›¸åŒçš„ä¸Šä¸‹æ–‡â€œç±»å‹â€ã€‚

ä½ å¯ä»¥å°†ä¸Šä¸‹æ–‡ç”¨äºå¦‚ä¸‹ç”¨é€”ï¼š

-   ä½ çš„è¿è¡Œæ‰€éœ€çš„æƒ…å¢ƒæ•°æ®ï¼ˆä¾‹å¦‚ç”¨æˆ·å/uid æˆ–å…³äºç”¨æˆ·çš„å…¶ä»–ä¿¡æ¯ï¼‰
-   ä¾èµ–é¡¹ï¼ˆä¾‹å¦‚æ—¥å¿—å¯¹è±¡ã€æ•°æ®è·å–å™¨ç­‰ï¼‰
-   å¸®åŠ©å‡½æ•°

!!! danger "æ³¨æ„"

    ä¸Šä¸‹æ–‡å¯¹è±¡**ä¸ä¼š**å‘é€ç»™ LLMã€‚å®ƒçº¯ç²¹æ˜¯ä¸€ä¸ªæœ¬åœ°å¯¹è±¡ï¼Œä½ å¯ä»¥è¯»å–ã€å†™å…¥å¹¶åœ¨å…¶ä¸Šè°ƒç”¨æ–¹æ³•ã€‚

```python
import asyncio
from dataclasses import dataclass

from agents import Agent, RunContextWrapper, Runner, function_tool

@dataclass
class UserInfo:  # (1)!
    name: str
    uid: int

@function_tool
async def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str:  # (2)!
    """Fetch the age of the user. Call this function to get user's age information."""
    return f"The user {wrapper.context.name} is 47 years old"

async def main():
    user_info = UserInfo(name="John", uid=123)

    agent = Agent[UserInfo](  # (3)!
        name="Assistant",
        tools=[fetch_user_age],
    )

    result = await Runner.run(  # (4)!
        starting_agent=agent,
        input="What is the age of the user?",
        context=user_info,
    )

    print(result.final_output)  # (5)!
    # The user John is 47 years old.

if __name__ == "__main__":
    asyncio.run(main())
```

1. è¿™æ˜¯ä¸Šä¸‹æ–‡å¯¹è±¡ã€‚æ­¤å¤„æˆ‘ä»¬ä½¿ç”¨äº† dataclassï¼Œä½†ä½ å¯ä»¥ä½¿ç”¨ä»»ä½•ç±»å‹ã€‚
2. è¿™æ˜¯ä¸€ä¸ªå·¥å…·ã€‚å¯ä»¥çœ‹åˆ°å®ƒæ¥æ”¶ `RunContextWrapper[UserInfo]`ã€‚å·¥å…·å®ç°ä¼šä»ä¸Šä¸‹æ–‡ä¸­è¯»å–ã€‚
3. æˆ‘ä»¬ç”¨æ³›å‹ `UserInfo` æ ‡æ³¨è¯¥æ™ºèƒ½ä½“ï¼Œä»¥ä¾¿ç±»å‹æ£€æŸ¥å™¨èƒ½æ•æ‰é”™è¯¯ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å°è¯•ä¼ å…¥ä¸€ä¸ªä½¿ç”¨ä¸åŒä¸Šä¸‹æ–‡ç±»å‹çš„å·¥å…·ï¼‰ã€‚
4. é€šè¿‡ `run` å‡½æ•°ä¼ å…¥ä¸Šä¸‹æ–‡ã€‚
5. æ™ºèƒ½ä½“æ­£ç¡®è°ƒç”¨å·¥å…·å¹¶è·å¾—å¹´é¾„ã€‚

---

### è¿›é˜¶ï¼š`ToolContext`

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½å¸Œæœ›è®¿é—®æ­£åœ¨æ‰§è¡Œçš„å·¥å…·çš„é¢å¤–å…ƒæ•°æ®â€”â€”ä¾‹å¦‚å…¶åç§°ã€è°ƒç”¨ IDï¼Œæˆ–åŸå§‹å‚æ•°å­—ç¬¦ä¸²ã€‚  
ä¸ºæ­¤ï¼Œä½ å¯ä»¥ä½¿ç”¨æ‰©å±•è‡ª `RunContextWrapper` çš„ [`ToolContext`][agents.tool_context.ToolContext] ç±»ã€‚

```python
from typing import Annotated
from pydantic import BaseModel, Field
from agents import Agent, Runner, function_tool
from agents.tool_context import ToolContext

class WeatherContext(BaseModel):
    user_id: str

class Weather(BaseModel):
    city: str = Field(description="The city name")
    temperature_range: str = Field(description="The temperature range in Celsius")
    conditions: str = Field(description="The weather conditions")

@function_tool
def get_weather(ctx: ToolContext[WeatherContext], city: Annotated[str, "The city to get the weather for"]) -> Weather:
    print(f"[debug] Tool context: (name: {ctx.tool_name}, call_id: {ctx.tool_call_id}, args: {ctx.tool_arguments})")
    return Weather(city=city, temperature_range="14-20C", conditions="Sunny with wind.")

agent = Agent(
    name="Weather Agent",
    instructions="You are a helpful agent that can tell the weather of a given city.",
    tools=[get_weather],
)
```

`ToolContext` æä¾›ä¸ `RunContextWrapper` ç›¸åŒçš„ `.context` å±æ€§ï¼Œ  
å¹¶é¢å¤–åŒ…å«å½“å‰å·¥å…·è°ƒç”¨çš„ç‰¹å®šå­—æ®µï¼š

- `tool_name` â€“ è¢«è°ƒç”¨å·¥å…·çš„åç§°  
- `tool_call_id` â€“ æ­¤æ¬¡å·¥å…·è°ƒç”¨çš„å”¯ä¸€æ ‡è¯†ç¬¦  
- `tool_arguments` â€“ ä¼ ç»™å·¥å…·çš„åŸå§‹å‚æ•°å­—ç¬¦ä¸²  

å½“ä½ åœ¨æ‰§è¡ŒæœŸé—´éœ€è¦å·¥å…·çº§å…ƒæ•°æ®æ—¶è¯·ä½¿ç”¨ `ToolContext`ã€‚  
å¯¹äºåœ¨æ™ºèƒ½ä½“ä¸å·¥å…·ä¹‹é—´å…±äº«ä¸€èˆ¬ä¸Šä¸‹æ–‡ï¼Œ`RunContextWrapper` å·²ç»è¶³å¤Ÿã€‚

---

## æ™ºèƒ½ä½“/LLM ä¸Šä¸‹æ–‡

å½“è°ƒç”¨ LLM æ—¶ï¼Œå®ƒèƒ½çœ‹åˆ°çš„**å”¯ä¸€**æ•°æ®æ¥è‡ªå¯¹è¯å†å²ã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœä½ æƒ³è®©ä¸€äº›æ–°æ•°æ®å¯¹ LLM å¯ç”¨ï¼Œå¿…é¡»ä»¥ä¸€ç§èƒ½å°†å…¶æ”¾å…¥è¯¥å†å²çš„æ–¹å¼æä¾›ã€‚ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç§æ–¹æ³•ï¼š

1. ä½ å¯ä»¥æŠŠå®ƒåŠ å…¥æ™ºèƒ½ä½“çš„ `instructions`ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºâ€œç³»ç»Ÿæç¤ºè¯â€æˆ–â€œå¼€å‘è€…æ¶ˆæ¯â€ã€‚ç³»ç»Ÿæç¤ºè¯å¯ä»¥æ˜¯é™æ€å­—ç¬¦ä¸²ï¼Œä¹Ÿå¯ä»¥æ˜¯æ¥æ”¶ä¸Šä¸‹æ–‡å¹¶è¾“å‡ºå­—ç¬¦ä¸²çš„åŠ¨æ€å‡½æ•°ã€‚è¿™å¯¹äºå§‹ç»ˆæœ‰ç”¨çš„ä¿¡æ¯å¾ˆå¸¸è§ï¼ˆä¾‹å¦‚ç”¨æˆ·åæˆ–å½“å‰æ—¥æœŸï¼‰ã€‚
2. åœ¨è°ƒç”¨ `Runner.run` å‡½æ•°æ—¶å°†å…¶æ·»åŠ åˆ° `input` ä¸­ã€‚è¿™ä¸ `instructions` çš„ç­–ç•¥ç±»ä¼¼ï¼Œä½†å…è®¸ä½ æ·»åŠ åœ¨[æŒ‡ä»¤é“¾](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command)ä¸­å±‚çº§æ›´ä½çš„æ¶ˆæ¯ã€‚
3. é€šè¿‡ å·¥å…·è°ƒç”¨ æš´éœ²å®ƒã€‚è¿™å¯¹æŒ‰éœ€ä¸Šä¸‹æ–‡å¾ˆæœ‰ç”¨â€”â€”LLM å†³å®šä½•æ—¶éœ€è¦æŸäº›æ•°æ®ï¼Œå¹¶å¯è°ƒç”¨å·¥å…·æ¥è·å–è¿™äº›æ•°æ®ã€‚
4. ä½¿ç”¨æ£€ç´¢æˆ– ç½‘ç»œæ£€ç´¢ã€‚è¿™äº›æ˜¯ç‰¹æ®Šçš„å·¥å…·ï¼Œèƒ½å¤Ÿä»æ–‡ä»¶æˆ–æ•°æ®åº“ï¼ˆæ£€ç´¢ï¼‰ï¼Œæˆ–ä»ç½‘ç»œï¼ˆç½‘ç»œæ£€ç´¢ï¼‰ä¸­è·å–ç›¸å…³æ•°æ®ã€‚è¿™æœ‰åŠ©äºç”¨ç›¸å…³çš„æƒ…å¢ƒæ•°æ®æ¥â€œæ”¯æ’‘â€æ¨¡å‹çš„å“åº”ã€‚

================
File: docs/zh/examples.md
================
---
search:
  exclude: true
---
# ä»£ç ç¤ºä¾‹

åœ¨[repo](https://github.com/openai/openai-agents-python/tree/main/examples) çš„ examples éƒ¨åˆ†æŸ¥çœ‹å¤šç§ SDK ç¤ºä¾‹å®ç°ã€‚è¿™äº›ç¤ºä¾‹æŒ‰å¤šä¸ªç›®å½•ç»„ç»‡ï¼Œå±•ç¤ºä¸åŒçš„æ¨¡å¼ä¸èƒ½åŠ›ã€‚

## ç›®å½•

-   **[agent_patterns](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns):**
    æ­¤ç›®å½•ä¸­çš„ç¤ºä¾‹å±•ç¤ºå¸¸è§çš„æ™ºèƒ½ä½“è®¾è®¡æ¨¡å¼ï¼Œä¾‹å¦‚

    -   ç¡®å®šæ€§å·¥ä½œæµ
    -   å°†æ™ºèƒ½ä½“ä½œä¸ºå·¥å…·
    -   å¹¶è¡Œæ™ºèƒ½ä½“æ‰§è¡Œ
    -   æ¡ä»¶å¼å·¥å…·ä½¿ç”¨
    -   è¾“å…¥/è¾“å‡ºå®‰å…¨é˜²æŠ¤æªæ–½
    -   LLM æ‹…ä»»è¯„å®¡
    -   è·¯ç”±
    -   æµå¼ä¼ è¾“å®‰å…¨é˜²æŠ¤æªæ–½

-   **[basic](https://github.com/openai/openai-agents-python/tree/main/examples/basic):**
    è¿™äº›ç¤ºä¾‹å±•ç¤º SDK çš„åŸºç¡€èƒ½åŠ›ï¼Œä¾‹å¦‚

    -   Hello World ç¤ºä¾‹ï¼ˆé»˜è®¤æ¨¡å‹ã€GPT-5ã€å¼€æ”¾æƒé‡æ¨¡å‹ï¼‰
    -   æ™ºèƒ½ä½“ç”Ÿå‘½å‘¨æœŸç®¡ç†
    -   åŠ¨æ€ç³»ç»Ÿæç¤ºè¯
    -   æµå¼ä¼ è¾“è¾“å‡ºï¼ˆæ–‡æœ¬ã€itemsã€å‡½æ•°è°ƒç”¨å‚æ•°ï¼‰
    -   æç¤ºæ¨¡æ¿
    -   æ–‡ä»¶å¤„ç†ï¼ˆæœ¬åœ°ä¸è¿œç¨‹ã€å›¾åƒä¸ PDFï¼‰
    -   ä½¿ç”¨è¿½è¸ª
    -   éä¸¥æ ¼è¾“å‡ºç±»å‹
    -   å…ˆå‰å“åº” ID çš„ä½¿ç”¨

-   **[customer_service](https://github.com/openai/openai-agents-python/tree/main/examples/customer_service):**
    èˆªç©ºå…¬å¸å®¢æˆ·æœåŠ¡ç³»ç»Ÿç¤ºä¾‹ã€‚

-   **[financial_research_agent](https://github.com/openai/openai-agents-python/tree/main/examples/financial_research_agent):**
    ä¸€ä¸ªé‡‘èç ”ç©¶æ™ºèƒ½ä½“ï¼Œæ¼”ç¤ºä½¿ç”¨æ™ºèƒ½ä½“å’Œå·¥å…·è¿›è¡Œé‡‘èæ•°æ®åˆ†æçš„ç»“æ„åŒ–ç ”ç©¶å·¥ä½œæµã€‚

-   **[handoffs](https://github.com/openai/openai-agents-python/tree/main/examples/handoffs):**
    æŸ¥çœ‹å¸¦æ¶ˆæ¯è¿‡æ»¤çš„æ™ºèƒ½ä½“ä»»åŠ¡è½¬ç§»çš„å®ç”¨ç¤ºä¾‹ã€‚

-   **[hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp):**
    æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ‰˜ç®¡çš„ MCP (Model Context Protocol) è¿æ¥å™¨ä¸å®¡æ‰¹çš„ç¤ºä¾‹ã€‚

-   **[mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp):**
    äº†è§£å¦‚ä½•ä½¿ç”¨ MCP (Model Context Protocol) æ„å»ºæ™ºèƒ½ä½“ï¼ŒåŒ…æ‹¬ï¼š

    -   æ–‡ä»¶ç³»ç»Ÿç¤ºä¾‹
    -   Git ç¤ºä¾‹
    -   MCP æç¤ºæœåŠ¡ç¤ºä¾‹
    -   SSE (Server-Sent Events) ç¤ºä¾‹
    -   å¯æµå¼çš„ HTTP ç¤ºä¾‹

-   **[memory](https://github.com/openai/openai-agents-python/tree/main/examples/memory):**
    ä¸åŒæ™ºèƒ½ä½“è®°å¿†å®ç°çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š

    -   SQLite ä¼šè¯å­˜å‚¨
    -   é«˜çº§ SQLite ä¼šè¯å­˜å‚¨
    -   Redis ä¼šè¯å­˜å‚¨
    -   SQLAlchemy ä¼šè¯å­˜å‚¨
    -   åŠ å¯†çš„ä¼šè¯å­˜å‚¨
    -   OpenAI ä¼šè¯å­˜å‚¨

-   **[model_providers](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers):**
    äº†è§£å¦‚ä½•åœ¨ SDK ä¸­ä½¿ç”¨é OpenAI æ¨¡å‹ï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰æä¾›æ–¹ä¸ LiteLLM é›†æˆã€‚

-   **[realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime):**
    å±•ç¤ºå¦‚ä½•ä½¿ç”¨ SDK æ„å»ºå®æ—¶ä½“éªŒçš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š

    -   Web åº”ç”¨
    -   å‘½ä»¤è¡Œç•Œé¢
    -   ä¸ Twilio é›†æˆ

-   **[reasoning_content](https://github.com/openai/openai-agents-python/tree/main/examples/reasoning_content):**
    æ¼”ç¤ºå¦‚ä½•å¤„ç†æ¨ç†å†…å®¹ä¸ structured outputs çš„ç¤ºä¾‹ã€‚

-   **[research_bot](https://github.com/openai/openai-agents-python/tree/main/examples/research_bot):**
    ä¸€ä¸ªç®€å•çš„æ·±åº¦ç ”ç©¶å…‹éš†ï¼Œæ¼”ç¤ºå¤æ‚çš„å¤šæ™ºèƒ½ä½“ç ”ç©¶å·¥ä½œæµã€‚

-   **[tools](https://github.com/openai/openai-agents-python/tree/main/examples/tools):**
    äº†è§£å¦‚ä½•å®ç°ç”±OpenAIæ‰˜ç®¡çš„å·¥å…·ï¼Œä¾‹å¦‚ï¼š

    -   ç½‘ç»œæ£€ç´¢ä»¥åŠå¸¦ç­›é€‰çš„ç½‘ç»œæ£€ç´¢
    -   æ–‡ä»¶æ£€ç´¢
    -   Code Interpreter
    -   è®¡ç®—æœºæ“ä½œ
    -   å›¾åƒç”Ÿæˆ

-   **[voice](https://github.com/openai/openai-agents-python/tree/main/examples/voice):**
    æŸ¥çœ‹è¯­éŸ³æ™ºèƒ½ä½“ç¤ºä¾‹ï¼Œä½¿ç”¨æˆ‘ä»¬çš„ TTS ä¸ STT æ¨¡å‹ï¼ŒåŒ…æ‹¬æµå¼è¯­éŸ³ç¤ºä¾‹ã€‚

================
File: docs/zh/guardrails.md
================
---
search:
  exclude: true
---
# å®‰å…¨é˜²æŠ¤æªæ–½

å®‰å…¨é˜²æŠ¤æªæ–½ä¸æ‚¨çš„æ™ºèƒ½ä½“å¹¶è¡Œè¿è¡Œï¼Œä½¿æ‚¨èƒ½å¤Ÿå¯¹ç”¨æˆ·è¾“å…¥è¿›è¡Œæ£€æŸ¥ä¸æ ¡éªŒã€‚æ¯”å¦‚ï¼Œå‡è®¾æ‚¨æœ‰ä¸€ä¸ªæ™ºèƒ½ä½“ä½¿ç”¨éå¸¸æ™ºèƒ½ï¼ˆå› æ­¤ä¹Ÿæ›´æ…¢/æ›´æ˜‚è´µï¼‰çš„æ¨¡å‹æ¥å¤„ç†å®¢æˆ·è¯·æ±‚ã€‚æ‚¨ä¸å¸Œæœ›æ¶æ„ç”¨æˆ·è¦æ±‚æ¨¡å‹å¸®åŠ©ä»–ä»¬å®Œæˆæ•°å­¦ä½œä¸šã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå¿«é€Ÿ/å»‰ä»·çš„æ¨¡å‹æ¥è¿è¡Œå®‰å…¨é˜²æŠ¤æªæ–½ã€‚å¦‚æœè¯¥å®‰å…¨é˜²æŠ¤æªæ–½æ£€æµ‹åˆ°æ¶æ„ä½¿ç”¨ï¼Œå®ƒå¯ä»¥ç«‹å³æŠ›å‡ºé”™è¯¯ï¼Œä»è€Œé˜»æ­¢æ˜‚è´µçš„æ¨¡å‹è¿è¡Œï¼ŒèŠ‚çœæ—¶é—´/æˆæœ¬ã€‚

å®‰å…¨é˜²æŠ¤æªæ–½æœ‰ä¸¤ç§ç±»å‹ï¼š

1. è¾“å…¥å®‰å…¨é˜²æŠ¤æªæ–½è¿è¡Œåœ¨åˆå§‹ç”¨æˆ·è¾“å…¥ä¸Š
2. è¾“å‡ºå®‰å…¨é˜²æŠ¤æªæ–½è¿è¡Œåœ¨æœ€ç»ˆæ™ºèƒ½ä½“è¾“å‡ºä¸Š

## è¾“å…¥å®‰å…¨é˜²æŠ¤æªæ–½

è¾“å…¥å®‰å…¨é˜²æŠ¤æªæ–½åˆ† 3 æ­¥è¿è¡Œï¼š

1. é¦–å…ˆï¼Œå®‰å…¨é˜²æŠ¤æªæ–½æ¥æ”¶ä¸æ™ºèƒ½ä½“ç›¸åŒçš„è¾“å…¥ã€‚
2. æ¥ç€ï¼Œè¿è¡Œå®‰å…¨é˜²æŠ¤å‡½æ•°ä»¥ç”Ÿæˆä¸€ä¸ª [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]ï¼Œç„¶åå°†å…¶åŒ…è£…ä¸º [`InputGuardrailResult`][agents.guardrail.InputGuardrailResult]
3. æœ€åï¼Œæˆ‘ä»¬æ£€æŸ¥ [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] æ˜¯å¦ä¸º trueã€‚å¦‚æœä¸º trueï¼Œå°†æŠ›å‡º [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered] å¼‚å¸¸ï¼Œä»¥ä¾¿æ‚¨é€‚å½“å›åº”ç”¨æˆ·æˆ–å¤„ç†è¯¥å¼‚å¸¸ã€‚

!!! Note

    è¾“å…¥å®‰å…¨é˜²æŠ¤æªæ–½æ—¨åœ¨å¯¹ç”¨æˆ·è¾“å…¥è¿›è¡Œå¤„ç†ï¼Œå› æ­¤åªæœ‰å½“æŸä¸ªæ™ºèƒ½ä½“æ˜¯ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“æ—¶ï¼Œè¯¥æ™ºèƒ½ä½“çš„å®‰å…¨é˜²æŠ¤æªæ–½æ‰ä¼šè¿è¡Œã€‚æ‚¨å¯èƒ½ä¼šé—®ï¼Œä¸ºä»€ä¹ˆ `guardrails` å±æ€§åœ¨æ™ºèƒ½ä½“ä¸Šï¼Œè€Œä¸æ˜¯ä¼ ç»™ `Runner.run`ï¼Ÿè¿™æ˜¯å› ä¸ºå®‰å…¨é˜²æŠ¤æªæ–½é€šå¸¸ä¸å…·ä½“çš„æ™ºèƒ½ä½“ç›¸å…³â€”â€”ä¸åŒçš„æ™ºèƒ½ä½“ä¼šè¿è¡Œä¸åŒçš„å®‰å…¨é˜²æŠ¤æªæ–½ï¼ŒæŠŠä»£ç æ”¾åœ¨ä¸€èµ·æœ‰åŠ©äºå¯è¯»æ€§ã€‚

## è¾“å‡ºå®‰å…¨é˜²æŠ¤æªæ–½

è¾“å‡ºå®‰å…¨é˜²æŠ¤æªæ–½åˆ† 3 æ­¥è¿è¡Œï¼š

1. é¦–å…ˆï¼Œå®‰å…¨é˜²æŠ¤æªæ–½æ¥æ”¶ç”±æ™ºèƒ½ä½“ç”Ÿæˆçš„è¾“å‡ºã€‚
2. æ¥ç€ï¼Œè¿è¡Œå®‰å…¨é˜²æŠ¤å‡½æ•°ä»¥ç”Ÿæˆä¸€ä¸ª [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]ï¼Œç„¶åå°†å…¶åŒ…è£…ä¸º [`OutputGuardrailResult`][agents.guardrail.OutputGuardrailResult]
3. æœ€åï¼Œæˆ‘ä»¬æ£€æŸ¥ [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] æ˜¯å¦ä¸º trueã€‚å¦‚æœä¸º trueï¼Œå°†æŠ›å‡º [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered] å¼‚å¸¸ï¼Œä»¥ä¾¿æ‚¨é€‚å½“å›åº”ç”¨æˆ·æˆ–å¤„ç†è¯¥å¼‚å¸¸ã€‚

!!! Note

    è¾“å‡ºå®‰å…¨é˜²æŠ¤æªæ–½æ—¨åœ¨å¯¹æœ€ç»ˆçš„æ™ºèƒ½ä½“è¾“å‡ºè¿›è¡Œå¤„ç†ï¼Œå› æ­¤åªæœ‰å½“æŸä¸ªæ™ºèƒ½ä½“æ˜¯æœ€åä¸€ä¸ªæ™ºèƒ½ä½“æ—¶ï¼Œè¯¥æ™ºèƒ½ä½“çš„å®‰å…¨é˜²æŠ¤æªæ–½æ‰ä¼šè¿è¡Œã€‚ä¸è¾“å…¥å®‰å…¨é˜²æŠ¤æªæ–½ç±»ä¼¼ï¼Œæˆ‘ä»¬è¿™æ ·åšæ˜¯å› ä¸ºå®‰å…¨é˜²æŠ¤æªæ–½é€šå¸¸ä¸å…·ä½“çš„æ™ºèƒ½ä½“ç›¸å…³â€”â€”ä¸åŒçš„æ™ºèƒ½ä½“ä¼šè¿è¡Œä¸åŒçš„å®‰å…¨é˜²æŠ¤æªæ–½ï¼ŒæŠŠä»£ç æ”¾åœ¨ä¸€èµ·æœ‰åŠ©äºå¯è¯»æ€§ã€‚

## è§¦å‘çº¿

å¦‚æœè¾“å…¥æˆ–è¾“å‡ºæœªé€šè¿‡å®‰å…¨é˜²æŠ¤æªæ–½ï¼Œå®‰å…¨é˜²æŠ¤æªæ–½å¯ä»¥é€šè¿‡è§¦å‘çº¿ï¼ˆtripwireï¼‰å‘å‡ºä¿¡å·ã€‚ä¸€æ—¦æˆ‘ä»¬å‘ç°æŸä¸ªå®‰å…¨é˜²æŠ¤æªæ–½è§¦å‘äº†è§¦å‘çº¿ï¼Œæˆ‘ä»¬ä¼šç«‹å³æŠ›å‡º `{Input,Output}GuardrailTripwireTriggered` å¼‚å¸¸å¹¶ä¸­æ­¢æ™ºèƒ½ä½“çš„æ‰§è¡Œã€‚

## å®ç°å®‰å…¨é˜²æŠ¤æªæ–½

æ‚¨éœ€è¦æä¾›ä¸€ä¸ªå‡½æ•°æ¥æ¥æ”¶è¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ª [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡åœ¨åº•å±‚è¿è¡Œä¸€ä¸ªæ™ºèƒ½ä½“æ¥å®ç°è¿™ä¸€ç‚¹ã€‚

```python
from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    InputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    TResponseInputItem,
    input_guardrail,
)

class MathHomeworkOutput(BaseModel):
    is_math_homework: bool
    reasoning: str

guardrail_agent = Agent( # (1)!
    name="Guardrail check",
    instructions="Check if the user is asking you to do their math homework.",
    output_type=MathHomeworkOutput,
)


@input_guardrail
async def math_guardrail( # (2)!
    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, input, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output, # (3)!
        tripwire_triggered=result.final_output.is_math_homework,
    )


agent = Agent(  # (4)!
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    input_guardrails=[math_guardrail],
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except InputGuardrailTripwireTriggered:
        print("Math homework guardrail tripped")
```

1. æˆ‘ä»¬ä¼šåœ¨å®‰å…¨é˜²æŠ¤å‡½æ•°ä¸­ä½¿ç”¨è¿™ä¸ªæ™ºèƒ½ä½“ã€‚
2. è¿™æ˜¯æ¥æ”¶æ™ºèƒ½ä½“è¾“å…¥/ä¸Šä¸‹æ–‡å¹¶è¿”å›ç»“æœçš„å®‰å…¨é˜²æŠ¤å‡½æ•°ã€‚
3. æˆ‘ä»¬å¯ä»¥åœ¨å®‰å…¨é˜²æŠ¤ç»“æœä¸­åŒ…å«é¢å¤–ä¿¡æ¯ã€‚
4. è¿™æ˜¯å®šä¹‰å·¥ä½œæµçš„å®é™…æ™ºèƒ½ä½“ã€‚

è¾“å‡ºå®‰å…¨é˜²æŠ¤æªæ–½ä¸ä¹‹ç±»ä¼¼ã€‚

```python
from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    OutputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    output_guardrail,
)
class MessageOutput(BaseModel): # (1)!
    response: str

class MathOutput(BaseModel): # (2)!
    reasoning: str
    is_math: bool

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the output includes any math.",
    output_type=MathOutput,
)

@output_guardrail
async def math_guardrail(  # (3)!
    ctx: RunContextWrapper, agent: Agent, output: MessageOutput
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, output.response, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output,
        tripwire_triggered=result.final_output.is_math,
    )

agent = Agent( # (4)!
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    output_guardrails=[math_guardrail],
    output_type=MessageOutput,
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except OutputGuardrailTripwireTriggered:
        print("Math output guardrail tripped")
```

1. è¿™æ˜¯å®é™…æ™ºèƒ½ä½“çš„è¾“å‡ºç±»å‹ã€‚
2. è¿™æ˜¯å®‰å…¨é˜²æŠ¤æªæ–½çš„è¾“å‡ºç±»å‹ã€‚
3. è¿™æ˜¯æ¥æ”¶æ™ºèƒ½ä½“è¾“å‡ºå¹¶è¿”å›ç»“æœçš„å®‰å…¨é˜²æŠ¤å‡½æ•°ã€‚
4. è¿™æ˜¯å®šä¹‰å·¥ä½œæµçš„å®é™…æ™ºèƒ½ä½“ã€‚

================
File: docs/zh/handoffs.md
================
---
search:
  exclude: true
---
# ä»»åŠ¡è½¬ç§»

ä»»åŠ¡è½¬ç§»å…è®¸ä¸€ä¸ªæ™ºèƒ½ä½“å°†ä»»åŠ¡å§”æ´¾ç»™å¦ä¸€ä¸ªæ™ºèƒ½ä½“ã€‚è¿™åœ¨ä¸åŒæ™ºèƒ½ä½“å„è‡ªä¸“é•¿ä¸åŒé¢†åŸŸçš„åœºæ™¯ä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå®¢æˆ·æ”¯æŒåº”ç”¨å¯èƒ½æœ‰åˆ†åˆ«å¤„ç†è®¢å•çŠ¶æ€ã€é€€æ¬¾ã€å¸¸è§é—®é¢˜ç­‰ä»»åŠ¡çš„æ™ºèƒ½ä½“ã€‚

åœ¨ LLM çœ‹æ¥ï¼Œä»»åŠ¡è½¬ç§»è¡¨ç°ä¸ºä¸€ä¸ªå·¥å…·ã€‚å› æ­¤ï¼Œå¦‚æœæœ‰ä¸€ä¸ªåˆ°åä¸º `Refund Agent` çš„æ™ºèƒ½ä½“çš„ä»»åŠ¡è½¬ç§»ï¼Œè¯¥å·¥å…·ä¼šè¢«å‘½åä¸º `transfer_to_refund_agent`ã€‚

## åˆ›å»ºä»»åŠ¡è½¬ç§»

æ‰€æœ‰æ™ºèƒ½ä½“éƒ½æœ‰ä¸€ä¸ª [`handoffs`][agents.agent.Agent.handoffs] å‚æ•°ï¼Œå®ƒå¯ä»¥ç›´æ¥æ¥å—ä¸€ä¸ª `Agent`ï¼Œæˆ–è€…æ¥å—ä¸€ä¸ªç”¨äºè‡ªå®šä¹‰ä»»åŠ¡è½¬ç§»çš„ `Handoff` å¯¹è±¡ã€‚

ä½ å¯ä»¥ä½¿ç”¨ Agents SDK æä¾›çš„ [`handoff()`][agents.handoffs.handoff] å‡½æ•°æ¥åˆ›å»ºä»»åŠ¡è½¬ç§»ã€‚è¯¥å‡½æ•°å…è®¸ä½ æŒ‡å®šè¦è½¬ç§»åˆ°çš„æ™ºèƒ½ä½“ï¼Œå¹¶æä¾›å¯é€‰çš„è¦†ç›–é¡¹å’Œè¾“å…¥è¿‡æ»¤å™¨ã€‚

### åŸºæœ¬ç”¨æ³•

ä»¥ä¸‹å±•ç¤ºå¦‚ä½•åˆ›å»ºä¸€ä¸ªç®€å•çš„ä»»åŠ¡è½¬ç§»ï¼š

```python
from agents import Agent, handoff

billing_agent = Agent(name="Billing agent")
refund_agent = Agent(name="Refund agent")

# (1)!
triage_agent = Agent(name="Triage agent", handoffs=[billing_agent, handoff(refund_agent)])
```

1. ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨æ™ºèƒ½ä½“ï¼ˆå¦‚ `billing_agent`ï¼‰ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ `handoff()` å‡½æ•°ã€‚

### é€šè¿‡ `handoff()` å‡½æ•°è‡ªå®šä¹‰ä»»åŠ¡è½¬ç§»

[`handoff()`][agents.handoffs.handoff] å‡½æ•°å…è®¸ä½ è¿›è¡Œè‡ªå®šä¹‰ã€‚

- `agent`: è¦å°†ä»»åŠ¡è½¬ç§»åˆ°çš„æ™ºèƒ½ä½“ã€‚
- `tool_name_override`: é»˜è®¤ä½¿ç”¨ `Handoff.default_tool_name()`ï¼Œè§£æä¸º `transfer_to_<agent_name>`ã€‚ä½ å¯ä»¥è¦†ç›–å®ƒã€‚
- `tool_description_override`: è¦†ç›–æ¥è‡ª `Handoff.default_tool_description()` çš„é»˜è®¤å·¥å…·æè¿°ã€‚
- `on_handoff`: å½“ä»»åŠ¡è½¬ç§»è¢«è°ƒç”¨æ—¶æ‰§è¡Œçš„å›è°ƒå‡½æ•°ã€‚å¯ç”¨äºåœ¨ä½ çŸ¥é“ä»»åŠ¡è½¬ç§»è¢«è°ƒç”¨æ—¶ç«‹å³å¯åŠ¨ä¸€äº›æ•°æ®æŠ“å–ç­‰æ“ä½œã€‚è¯¥å‡½æ•°æ¥æ”¶æ™ºèƒ½ä½“ä¸Šä¸‹æ–‡ï¼Œå¹¶å¯é€‰æ¥æ”¶ LLM ç”Ÿæˆçš„è¾“å…¥ã€‚è¾“å…¥æ•°æ®ç”± `input_type` å‚æ•°æ§åˆ¶ã€‚
- `input_type`: ä»»åŠ¡è½¬ç§»æ‰€æœŸæœ›çš„è¾“å…¥ç±»å‹ï¼ˆå¯é€‰ï¼‰ã€‚
- `input_filter`: å…è®¸ä½ è¿‡æ»¤ä¸‹ä¸€ä¸ªæ™ºèƒ½ä½“æ¥æ”¶çš„è¾“å…¥ã€‚è¯¦è§ä¸‹æ–‡ã€‚
- `is_enabled`: ä»»åŠ¡è½¬ç§»æ˜¯å¦å¯ç”¨ã€‚å¯ä»¥æ˜¯å¸ƒå°”å€¼æˆ–è¿”å›å¸ƒå°”å€¼çš„å‡½æ•°ï¼Œä½¿ä½ å¯ä»¥åœ¨è¿è¡Œæ—¶åŠ¨æ€å¯ç”¨æˆ–ç¦ç”¨ä»»åŠ¡è½¬ç§»ã€‚

```python
from agents import Agent, handoff, RunContextWrapper

def on_handoff(ctx: RunContextWrapper[None]):
    print("Handoff called")

agent = Agent(name="My agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    tool_name_override="custom_handoff_tool",
    tool_description_override="Custom description",
)
```

## ä»»åŠ¡è½¬ç§»è¾“å…¥

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¸Œæœ› LLM åœ¨è°ƒç”¨ä»»åŠ¡è½¬ç§»æ—¶æä¾›ä¸€äº›æ•°æ®ã€‚æ¯”å¦‚ï¼Œè®¾æƒ³ä¸€ä¸ªåˆ°â€œå‡çº§å¤„ç†ï¼ˆEscalationï¼‰æ™ºèƒ½ä½“â€çš„ä»»åŠ¡è½¬ç§»ã€‚ä½ å¯èƒ½å¸Œæœ›æä¾›ä¸€ä¸ªåŸå› ï¼Œä»¥ä¾¿è¿›è¡Œæ—¥å¿—è®°å½•ã€‚

```python
from pydantic import BaseModel

from agents import Agent, handoff, RunContextWrapper

class EscalationData(BaseModel):
    reason: str

async def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):
    print(f"Escalation agent called with reason: {input_data.reason}")

agent = Agent(name="Escalation agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    input_type=EscalationData,
)
```

## è¾“å…¥è¿‡æ»¤å™¨

å½“å‘ç”Ÿä»»åŠ¡è½¬ç§»æ—¶ï¼Œå°±å¥½åƒæ–°çš„æ™ºèƒ½ä½“æ¥ç®¡äº†å¯¹è¯ï¼Œå¹¶èƒ½çœ‹åˆ°æ•´ä¸ªå…ˆå‰çš„å¯¹è¯å†å²ã€‚å¦‚æœä½ æƒ³æ”¹å˜è¿™ä¸€ç‚¹ï¼Œå¯ä»¥è®¾ç½®ä¸€ä¸ª [`input_filter`][agents.handoffs.Handoff.input_filter]ã€‚è¾“å…¥è¿‡æ»¤å™¨æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒé€šè¿‡ [`HandoffInputData`][agents.handoffs.HandoffInputData] æ¥æ”¶ç°æœ‰è¾“å…¥ï¼Œå¹¶ä¸”å¿…é¡»è¿”å›ä¸€ä¸ªæ–°çš„ `HandoffInputData`ã€‚

æœ‰ä¸€äº›å¸¸è§æ¨¡å¼ï¼ˆä¾‹å¦‚ä»å†å²è®°å½•ä¸­ç§»é™¤æ‰€æœ‰å·¥å…·è°ƒç”¨ï¼‰ï¼Œå·²åœ¨ [`agents.extensions.handoff_filters`][] ä¸­ä¸ºä½ å®ç°ã€‚

```python
from agents import Agent, handoff
from agents.extensions import handoff_filters

agent = Agent(name="FAQ agent")

handoff_obj = handoff(
    agent=agent,
    input_filter=handoff_filters.remove_all_tools, # (1)!
)
```

1. å½“è°ƒç”¨ `FAQ agent` æ—¶ï¼Œè¿™å°†è‡ªåŠ¨ä»å†å²è®°å½•ä¸­ç§»é™¤æ‰€æœ‰å·¥å…·ã€‚

## æ¨èæç¤ºè¯

ä¸ºç¡®ä¿ LLM èƒ½æ­£ç¡®ç†è§£ä»»åŠ¡è½¬ç§»ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ä½ çš„æ™ºèƒ½ä½“ä¸­åŒ…å«å…³äºä»»åŠ¡è½¬ç§»çš„ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨ [`agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX`][] ä¸­æä¾›äº†ä¸€ä¸ªå»ºè®®å‰ç¼€ï¼Œæˆ–è€…ä½ å¯ä»¥è°ƒç”¨ [`agents.extensions.handoff_prompt.prompt_with_handoff_instructions`][] å°†æ¨èæ•°æ®è‡ªåŠ¨æ·»åŠ åˆ°ä½ çš„æç¤ºè¯ä¸­ã€‚

```python
from agents import Agent
from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX

billing_agent = Agent(
    name="Billing agent",
    instructions=f"""{RECOMMENDED_PROMPT_PREFIX}
    <Fill in the rest of your prompt here>.""",
)
```

================
File: docs/zh/index.md
================
---
search:
  exclude: true
---
# OpenAI Agents SDK

[OpenAI Agents SDK](https://github.com/openai/openai-agents-python) ä½¿ä½ èƒ½å¤Ÿä»¥è½»é‡ã€æ˜“ç”¨ã€æŠ½è±¡æå°‘çš„æ–¹å¼æ„å»ºå…·å¤‡æ™ºèƒ½ä½“èƒ½åŠ›çš„ AI åº”ç”¨ã€‚å®ƒæ˜¯æˆ‘ä»¬æ­¤å‰é’ˆå¯¹æ™ºèƒ½ä½“çš„å®éªŒé¡¹ç›® [Swarm](https://github.com/openai/swarm/tree/main) çš„é¢å‘ç”Ÿäº§çš„å‡çº§ç‰ˆæœ¬ã€‚Agents SDK ä»…åŒ…å«ä¸€å°ç»„åŸºæœ¬ç»„ä»¶ï¼š

-   **æ™ºèƒ½ä½“**ï¼šé…å¤‡äº† instructions å’Œ tools çš„ LLM
-   **ä»»åŠ¡è½¬ç§»**ï¼šå…è®¸æ™ºèƒ½ä½“å°†ç‰¹å®šä»»åŠ¡å§”æ´¾ç»™å…¶ä»–æ™ºèƒ½ä½“
-   **å®‰å…¨é˜²æŠ¤æªæ–½**ï¼šæ”¯æŒå¯¹æ™ºèƒ½ä½“çš„è¾“å…¥ä¸è¾“å‡ºè¿›è¡ŒéªŒè¯
-   **ä¼šè¯**ï¼šåœ¨å¤šæ¬¡æ™ºèƒ½ä½“è¿è¡Œä¸­è‡ªåŠ¨ç»´æŠ¤å¯¹è¯å†å²

ç»“åˆ Pythonï¼Œè¿™äº›åŸºæœ¬ç»„ä»¶è¶³ä»¥è¡¨è¾¾å·¥å…·ä¸æ™ºèƒ½ä½“ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œä½¿ä½ æ— éœ€é™¡å³­å­¦ä¹ æ›²çº¿å³å¯æ„å»ºçœŸå®ä¸–ç•Œåº”ç”¨ã€‚æ­¤å¤–ï¼ŒSDK å†…ç½®äº† **è¿½è¸ª** åŠŸèƒ½ï¼Œå¸®åŠ©ä½ å¯è§†åŒ–å’Œè°ƒè¯•æ™ºèƒ½ä½“æµç¨‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼Œç”šè‡³ä¸ºä½ çš„åº”ç”¨å¾®è°ƒæ¨¡å‹ã€‚

## Why use the Agents SDK

è¯¥ SDK çš„ä¸¤ä¸ªæ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š

1. åŠŸèƒ½è¶³å¤Ÿä¸°å¯Œä»¥å€¼å¾—ä½¿ç”¨ï¼Œä½†åŸºç¡€ç»„ä»¶è¶³å¤Ÿå°‘ä»¥ä¾¿å¿«é€Ÿä¸Šæ‰‹ã€‚
2. å¼€ç®±å³ç”¨ä¸”æ•ˆæœå‡ºè‰²ï¼ŒåŒæ—¶å¯ç²¾ç¡®è‡ªå®šä¹‰è¡Œä¸ºã€‚

ä»¥ä¸‹æ˜¯ SDK çš„ä¸»è¦ç‰¹æ€§ï¼š

-   æ™ºèƒ½ä½“å¾ªç¯ï¼šå†…ç½®å¾ªç¯å¤„ç†å·¥å…·è°ƒç”¨ã€å°†ç»“æœå‘é€ç»™ LLMï¼Œå¹¶å¾ªç¯ç›´åˆ° LLM å®Œæˆã€‚
-   Python ä¼˜å…ˆï¼šä½¿ç”¨å†…ç½®è¯­è¨€ç‰¹æ€§æ¥ç¼–æ’å’Œä¸²è”æ™ºèƒ½ä½“ï¼Œè€Œæ— éœ€å­¦ä¹ æ–°çš„æŠ½è±¡ã€‚
-   ä»»åŠ¡è½¬ç§»ï¼šåœ¨å¤šä¸ªæ™ºèƒ½ä½“ä¹‹é—´è¿›è¡Œåè°ƒä¸å§”æ´¾çš„å¼ºå¤§èƒ½åŠ›ã€‚
-   å®‰å…¨é˜²æŠ¤æªæ–½ï¼šä¸æ™ºèƒ½ä½“å¹¶è¡Œæ‰§è¡Œè¾“å…¥éªŒè¯ä¸æ£€æŸ¥ï¼Œå¦‚æ£€æŸ¥å¤±è´¥åˆ™æå‰ä¸­æ–­ã€‚
-   ä¼šè¯ï¼šåœ¨å¤šæ¬¡æ™ºèƒ½ä½“è¿è¡Œä¸­è‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²ï¼Œå…å»æ‰‹åŠ¨çŠ¶æ€å¤„ç†ã€‚
-   å·¥å…·è°ƒç”¨ï¼šå°†ä»»æ„ Python å‡½æ•°å˜ä¸ºå·¥å…·ï¼Œè‡ªåŠ¨ç”Ÿæˆæ¨¡å¼å¹¶å€ŸåŠ© Pydantic è¿›è¡ŒéªŒè¯ã€‚
-   è¿½è¸ªï¼šå†…ç½®è¿½è¸ªï¼Œå¯ç”¨äºå¯è§†åŒ–ã€è°ƒè¯•ä¸ç›‘æ§å·¥ä½œæµï¼Œå¹¶ä½¿ç”¨ OpenAI çš„è¯„ä¼°ã€å¾®è°ƒä¸è’¸é¦å·¥å…·å¥—ä»¶ã€‚

## å®‰è£…

```bash
pip install openai-agents
```

## Hello World ç¤ºä¾‹

```python
from agents import Agent, Runner

agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)

# Code within the code,
# Functions calling themselves,
# Infinite loop's dance.
```

(_å¦‚æœè¿è¡Œæ­¤ç¤ºä¾‹ï¼Œè¯·ç¡®ä¿å·²è®¾ç½® `OPENAI_API_KEY` ç¯å¢ƒå˜é‡_)

```bash
export OPENAI_API_KEY=sk-...
```

================
File: docs/zh/mcp.md
================
---
search:
  exclude: true
---
# Model context protocol (MCP)

[Model context protocol](https://modelcontextprotocol.io/introduction)ï¼ˆMCPï¼‰æ ‡å‡†åŒ–äº†åº”ç”¨å‘è¯­è¨€æ¨¡å‹æš´éœ²å·¥å…·å’Œä¸Šä¸‹æ–‡çš„æ–¹å¼ã€‚æ‘˜è‡ªå®˜æ–¹æ–‡æ¡£ï¼š

> MCP æ˜¯ä¸€ä¸ªå¼€æ”¾åè®®ï¼Œç”¨äºæ ‡å‡†åŒ–åº”ç”¨å‘ LLM æä¾›ä¸Šä¸‹æ–‡çš„æ–¹å¼ã€‚å¯ä»¥å°† MCP æƒ³è±¡ä¸º AI åº”ç”¨çš„ USBâ€‘C æ¥å£ã€‚å°±åƒ USBâ€‘C ä¸ºä½ çš„è®¾å¤‡è¿æ¥å„ç§å¤–è®¾ä¸é…ä»¶æä¾›äº†æ ‡å‡†åŒ–æ–¹å¼ï¼ŒMCP ä¸ºå°† AI æ¨¡å‹è¿æ¥åˆ°ä¸åŒæ•°æ®æºä¸å·¥å…·æä¾›äº†æ ‡å‡†åŒ–æ–¹å¼ã€‚

Agents Python SDK æ”¯æŒå¤šç§ MCP ä¼ è¾“æ–¹å¼ã€‚è¿™ä½¿ä½ å¯ä»¥å¤ç”¨ç°æœ‰çš„ MCP æœåŠ¡æˆ–è‡ªè¡Œæ„å»ºï¼Œä»¥å‘æ™ºèƒ½ä½“æš´éœ²æ–‡ä»¶ç³»ç»Ÿã€HTTP æˆ–åŸºäºè¿æ¥å™¨çš„å·¥å…·ã€‚

## é€‰æ‹© MCP é›†æˆ

åœ¨å°† MCP æœåŠ¡æ¥å…¥æ™ºèƒ½ä½“ä¹‹å‰ï¼Œè¯·å…ˆå†³å®šå·¥å…·è°ƒç”¨åº”åœ¨ä½•å¤„æ‰§è¡Œï¼Œä»¥åŠå¯ä»¥è®¿é—®å“ªäº›ä¼ è¾“æ–¹å¼ã€‚ä¸‹è¡¨æ€»ç»“äº† Python SDK æ”¯æŒçš„é€‰é¡¹ã€‚

| ä½ çš„éœ€æ±‚                                                                            | æ¨èé€‰é¡¹                                              |
| ------------------------------------------------------------------------------------ | ----------------------------------------------------- |
| è®© OpenAI çš„ Responses API ä»£è¡¨æ¨¡å‹è°ƒç”¨ä¸€ä¸ªå¯å…¬å¼€è®¿é—®çš„ MCP æœåŠ¡                     | **æ‰˜ç®¡ MCP æœåŠ¡å·¥å…·**ï¼Œé€šè¿‡ [`HostedMCPTool`][agents.tool.HostedMCPTool] |
| è¿æ¥ä½ åœ¨æœ¬åœ°æˆ–è¿œç¨‹è¿è¡Œçš„å¯æµå¼ä¼ è¾“çš„ HTTP æœåŠ¡                                       | **å¯æµå¼ HTTP MCP æœåŠ¡**ï¼Œé€šè¿‡ [`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp] |
| ä¸å®ç°äº† HTTP + Server-Sent Events çš„æœåŠ¡é€šä¿¡                                        | **HTTP + SSE MCP æœåŠ¡**ï¼Œé€šè¿‡ [`MCPServerSse`][agents.mcp.server.MCPServerSse] |
| å¯åŠ¨æœ¬åœ°è¿›ç¨‹å¹¶é€šè¿‡ stdin/stdout é€šä¿¡                                                 | **stdio MCP æœåŠ¡**ï¼Œé€šè¿‡ [`MCPServerStdio`][agents.mcp.server.MCPServerStdio] |

ä¸‹é¢çš„ç« èŠ‚å°†é€ä¸€ä»‹ç»æ¯ä¸ªé€‰é¡¹ã€å¦‚ä½•é…ç½®ä»¥åŠä½•æ—¶é€‰æ‹©æŸç§ä¼ è¾“æ–¹å¼ã€‚

## 1. æ‰˜ç®¡ MCP æœåŠ¡å·¥å…·

æ‰˜ç®¡å·¥å…·å°†æ•´ä¸ªå·¥å…·å¾€è¿”æµç¨‹æ¨é€åˆ° OpenAI çš„åŸºç¡€è®¾æ–½ä¸­ã€‚ä½ çš„ä»£ç æ— éœ€åˆ—å‡ºå’Œè°ƒç”¨å·¥å…·ï¼Œ[`HostedMCPTool`][agents.tool.HostedMCPTool] ä¼šå°†æœåŠ¡æ ‡ç­¾ï¼ˆä»¥åŠå¯é€‰çš„è¿æ¥å™¨å…ƒæ•°æ®ï¼‰è½¬å‘ç»™ Responses APIã€‚æ¨¡å‹ä¼šåˆ—å‡ºè¿œç¨‹æœåŠ¡çš„å·¥å…·å¹¶ç›´æ¥è°ƒç”¨ï¼Œè€Œæ— éœ€å¯¹ä½ çš„ Python è¿›ç¨‹è¿›è¡Œé¢å¤–å›è°ƒã€‚æ‰˜ç®¡å·¥å…·ç›®å‰é€‚ç”¨äºæ”¯æŒ Responses API æ‰˜ç®¡ MCP é›†æˆçš„ OpenAI æ¨¡å‹ã€‚

### åŸºç¡€æ‰˜ç®¡ MCP å·¥å…·

é€šè¿‡åœ¨æ™ºèƒ½ä½“çš„ `tools` åˆ—è¡¨ä¸­åŠ å…¥ [`HostedMCPTool`][agents.tool.HostedMCPTool] æ¥åˆ›å»ºæ‰˜ç®¡å·¥å…·ã€‚`tool_config` å­—å…¸ä¸å‘é€åˆ° REST API çš„ JSON ç›¸åŒï¼š

```python
import asyncio

from agents import Agent, HostedMCPTool, Runner

async def main() -> None:
    agent = Agent(
        name="Assistant",
        tools=[
            HostedMCPTool(
                tool_config={
                    "type": "mcp",
                    "server_label": "gitmcp",
                    "server_url": "https://gitmcp.io/openai/codex",
                    "require_approval": "never",
                }
            )
        ],
    )

    result = await Runner.run(agent, "Which language is this repository written in?")
    print(result.final_output)

asyncio.run(main())
```

æ‰˜ç®¡æœåŠ¡ä¼šè‡ªåŠ¨æš´éœ²å…¶å·¥å…·ï¼›ä½ æ— éœ€å°†å…¶æ·»åŠ åˆ° `mcp_servers`ã€‚

### æ‰˜ç®¡ MCP ç»“æœçš„æµå¼ä¼ è¾“

æ‰˜ç®¡å·¥å…·ä»¥ä¸å·¥å…·è°ƒç”¨å®Œå…¨ç›¸åŒçš„æ–¹å¼æ”¯æŒæµå¼ä¼ è¾“ã€‚å‘ `Runner.run_streamed` ä¼ å…¥ `stream=True`ï¼Œå³å¯åœ¨æ¨¡å‹ä»åœ¨è¿è¡Œæ—¶æ¶ˆè´¹å¢é‡ MCP è¾“å‡ºï¼š

```python
result = Runner.run_streamed(agent, "Summarise this repository's top languages")
async for event in result.stream_events():
    if event.type == "run_item_stream_event":
        print(f"Received: {event.item}")
print(result.final_output)
```

### å¯é€‰çš„å®¡æ‰¹æµç¨‹

å¦‚æœæŸä¸ªæœåŠ¡å¯èƒ½æ‰§è¡Œæ•æ„Ÿæ“ä½œï¼Œä½ å¯ä»¥åœ¨æ¯æ¬¡å·¥å…·æ‰§è¡Œå‰è¦æ±‚äººå·¥æˆ–ç¨‹åºåŒ–å®¡æ‰¹ã€‚é€šè¿‡åœ¨ `tool_config` ä¸­é…ç½® `require_approval` æ¥å®ç°ï¼Œå¯ä»¥æ˜¯å•ä¸ªç­–ç•¥ï¼ˆ`"always"`ã€`"never"`ï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯å°†å·¥å…·åç§°æ˜ å°„åˆ°ç­–ç•¥çš„å­—å…¸ã€‚è‹¥è¦åœ¨ Python ä¸­åšå‡ºå†³ç­–ï¼Œè¯·æä¾› `on_approval_request` å›è°ƒã€‚

```python
from agents import MCPToolApprovalFunctionResult, MCPToolApprovalRequest

SAFE_TOOLS = {"read_project_metadata"}

def approve_tool(request: MCPToolApprovalRequest) -> MCPToolApprovalFunctionResult:
    if request.data.name in SAFE_TOOLS:
        return {"approve": True}
    return {"approve": False, "reason": "Escalate to a human reviewer"}

agent = Agent(
    name="Assistant",
    tools=[
        HostedMCPTool(
            tool_config={
                "type": "mcp",
                "server_label": "gitmcp",
                "server_url": "https://gitmcp.io/openai/codex",
                "require_approval": "always",
            },
            on_approval_request=approve_tool,
        )
    ],
)
```

å›è°ƒå¯ä»¥æ˜¯åŒæ­¥æˆ–å¼‚æ­¥çš„ï¼Œå¹¶ä¼šåœ¨æ¨¡å‹éœ€è¦å®¡æ‰¹æ•°æ®ä»¥ç»§ç»­è¿è¡Œæ—¶è¢«è°ƒç”¨ã€‚

### åŸºäºè¿æ¥å™¨çš„æ‰˜ç®¡æœåŠ¡

æ‰˜ç®¡ MCP ä¹Ÿæ”¯æŒ OpenAI è¿æ¥å™¨ã€‚ä½ å¯ä»¥ä¸æŒ‡å®š `server_url`ï¼Œè€Œæ˜¯æä¾› `connector_id` å’Œè®¿é—®ä»¤ç‰Œã€‚Responses API ä¼šå¤„ç†èº«ä»½éªŒè¯ï¼Œæ‰˜ç®¡æœåŠ¡å°†æš´éœ²è¿æ¥å™¨çš„å·¥å…·ã€‚

```python
import os

HostedMCPTool(
    tool_config={
        "type": "mcp",
        "server_label": "google_calendar",
        "connector_id": "connector_googlecalendar",
        "authorization": os.environ["GOOGLE_CALENDAR_AUTHORIZATION"],
        "require_approval": "never",
    }
)
```

å®Œæ•´å¯è¿è¡Œçš„æ‰˜ç®¡å·¥å…·ç¤ºä¾‹ï¼ˆåŒ…æ‹¬æµå¼ä¼ è¾“ã€å®¡æ‰¹å’Œè¿æ¥å™¨ï¼‰ä½äº
[`examples/hosted_mcp`](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp)ã€‚

## 2. å¯æµå¼ HTTP MCP æœåŠ¡

å½“ä½ å¸Œæœ›è‡ªè¡Œç®¡ç†ç½‘ç»œè¿æ¥æ—¶ï¼Œè¯·ä½¿ç”¨ [`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp]ã€‚å½“ä½ å¯æ§ä¼ è¾“å±‚æˆ–å¸Œæœ›åœ¨è‡ªæœ‰åŸºç¡€è®¾æ–½ä¸­è¿è¡ŒæœåŠ¡å¹¶ä¿æŒä½å»¶è¿Ÿæ—¶ï¼Œå¯æµå¼ HTTP æœåŠ¡æ˜¯ç†æƒ³é€‰æ‹©ã€‚

```python
import asyncio
import os

from agents import Agent, Runner
from agents.mcp import MCPServerStreamableHttp
from agents.model_settings import ModelSettings

async def main() -> None:
    token = os.environ["MCP_SERVER_TOKEN"]
    async with MCPServerStreamableHttp(
        name="Streamable HTTP Python Server",
        params={
            "url": "http://localhost:8000/mcp",
            "headers": {"Authorization": f"Bearer {token}"},
            "timeout": 10,
        },
        cache_tools_list=True,
        max_retry_attempts=3,
    ) as server:
        agent = Agent(
            name="Assistant",
            instructions="Use the MCP tools to answer the questions.",
            mcp_servers=[server],
            model_settings=ModelSettings(tool_choice="required"),
        )

        result = await Runner.run(agent, "Add 7 and 22.")
        print(result.final_output)

asyncio.run(main())
```

æ„é€ å‡½æ•°æ¥å—ä»¥ä¸‹é¢å¤–é€‰é¡¹ï¼š

- `client_session_timeout_seconds` æ§åˆ¶ HTTP è¯»å–è¶…æ—¶ã€‚
- `use_structured_content` åˆ‡æ¢æ˜¯å¦ä¼˜å…ˆä½¿ç”¨ `tool_result.structured_content` è€Œä¸æ˜¯æ–‡æœ¬è¾“å‡ºã€‚
- `max_retry_attempts` å’Œ `retry_backoff_seconds_base` ä¸º `list_tools()` å’Œ `call_tool()` æ·»åŠ è‡ªåŠ¨é‡è¯•ã€‚
- `tool_filter` å…è®¸ä»…æš´éœ²å·¥å…·å­é›†ï¼ˆå‚è§[å·¥å…·è¿‡æ»¤](#tool-filtering)ï¼‰ã€‚

## 3. HTTP + SSE MCP æœåŠ¡

å¦‚æœ MCP æœåŠ¡å®ç°äº† HTTP + SSE ä¼ è¾“ï¼Œè¯·å®ä¾‹åŒ– [`MCPServerSse`][agents.mcp.server.MCPServerSse]ã€‚é™¤ä¼ è¾“æ–¹å¼å¤–ï¼ŒAPI ä¸å¯æµå¼ HTTP æœåŠ¡ç›¸åŒã€‚

```python

from agents import Agent, Runner
from agents.model_settings import ModelSettings
from agents.mcp import MCPServerSse

workspace_id = "demo-workspace"

async with MCPServerSse(
    name="SSE Python Server",
    params={
        "url": "http://localhost:8000/sse",
        "headers": {"X-Workspace": workspace_id},
    },
    cache_tools_list=True,
) as server:
    agent = Agent(
        name="Assistant",
        mcp_servers=[server],
        model_settings=ModelSettings(tool_choice="required"),
    )
    result = await Runner.run(agent, "What's the weather in Tokyo?")
    print(result.final_output)
```

## 4. stdio MCP æœåŠ¡

å¯¹äºä½œä¸ºæœ¬åœ°å­è¿›ç¨‹è¿è¡Œçš„ MCP æœåŠ¡ï¼Œè¯·ä½¿ç”¨ [`MCPServerStdio`][agents.mcp.server.MCPServerStdio]ã€‚SDK ä¼šå¯åŠ¨è¯¥è¿›ç¨‹ã€ä¿æŒç®¡é“å¼€å¯ï¼Œå¹¶åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºæ—¶è‡ªåŠ¨å…³é—­ã€‚è¿™å¯¹äºå¿«é€Ÿæ¦‚å¿µéªŒè¯æˆ–æœåŠ¡ä»…æš´éœ²å‘½ä»¤è¡Œå…¥å£æ—¶éå¸¸æœ‰ç”¨ã€‚

```python
from pathlib import Path
from agents import Agent, Runner
from agents.mcp import MCPServerStdio

current_dir = Path(__file__).parent
samples_dir = current_dir / "sample_files"

async with MCPServerStdio(
    name="Filesystem Server via npx",
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", str(samples_dir)],
    },
) as server:
    agent = Agent(
        name="Assistant",
        instructions="Use the files in the sample directory to answer questions.",
        mcp_servers=[server],
    )
    result = await Runner.run(agent, "List the files available to you.")
    print(result.final_output)
```

## å·¥å…·è¿‡æ»¤

æ¯ä¸ª MCP æœåŠ¡éƒ½æ”¯æŒå·¥å…·è¿‡æ»¤ï¼Œä»¥ä¾¿ä½ ä»…æš´éœ²æ™ºèƒ½ä½“æ‰€éœ€çš„å‡½æ•°ã€‚è¿‡æ»¤å¯ä»¥åœ¨æ„é€ æ—¶è¿›è¡Œï¼Œä¹Ÿå¯ä»¥åœ¨æ¯æ¬¡è¿è¡Œæ—¶åŠ¨æ€è¿›è¡Œã€‚

### é™æ€å·¥å…·è¿‡æ»¤

ä½¿ç”¨ [`create_static_tool_filter`][agents.mcp.create_static_tool_filter] é…ç½®ç®€å•çš„å…è®¸/é˜»æ­¢åˆ—è¡¨ï¼š

```python
from pathlib import Path

from agents.mcp import MCPServerStdio, create_static_tool_filter

samples_dir = Path("/path/to/files")

filesystem_server = MCPServerStdio(
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", str(samples_dir)],
    },
    tool_filter=create_static_tool_filter(allowed_tool_names=["read_file", "write_file"]),
)
```

å½“åŒæ—¶æä¾› `allowed_tool_names` å’Œ `blocked_tool_names` æ—¶ï¼ŒSDK ä¼šå…ˆåº”ç”¨å…è®¸åˆ—è¡¨ï¼Œç„¶åä»å‰©ä½™é›†åˆä¸­ç§»é™¤ä»»ä½•è¢«é˜»æ­¢çš„å·¥å…·ã€‚

### åŠ¨æ€å·¥å…·è¿‡æ»¤

å¯¹äºæ›´å¤æ‚çš„é€»è¾‘ï¼Œä¼ å…¥ä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡ï¼Œè¯¥å¯¹è±¡æ¥æ”¶ [`ToolFilterContext`][agents.mcp.ToolFilterContext]ã€‚è¯¥å¯è°ƒç”¨å¯¹è±¡å¯ä»¥æ˜¯åŒæ­¥æˆ–å¼‚æ­¥çš„ï¼Œè¿”å› `True` è¡¨ç¤ºåº”æš´éœ²è¯¥å·¥å…·ã€‚

```python
from pathlib import Path

from agents.mcp import MCPServerStdio, ToolFilterContext

samples_dir = Path("/path/to/files")

async def context_aware_filter(context: ToolFilterContext, tool) -> bool:
    if context.agent.name == "Code Reviewer" and tool.name.startswith("danger_"):
        return False
    return True

async with MCPServerStdio(
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", str(samples_dir)],
    },
    tool_filter=context_aware_filter,
) as server:
    ...
```

è¿‡æ»¤ä¸Šä¸‹æ–‡ä¼šæš´éœ²å½“å‰çš„ `run_context`ã€è¯·æ±‚å·¥å…·çš„ `agent`ï¼Œä»¥åŠ `server_name`ã€‚

## æç¤ºè¯

MCP æœåŠ¡è¿˜å¯ä»¥æä¾›åŠ¨æ€ç”Ÿæˆæ™ºèƒ½ä½“ instructions çš„æç¤ºè¯ã€‚æ”¯æŒæç¤ºè¯çš„æœåŠ¡ä¼šæš´éœ²ä¸¤ä¸ªæ–¹æ³•ï¼š

- `list_prompts()` æšä¸¾å¯ç”¨çš„æç¤ºæ¨¡æ¿ã€‚
- `get_prompt(name, arguments)` è·å–å…·ä½“æç¤ºè¯ï¼Œå¯é€‰æ‹©å¸¦æœ‰å‚æ•°ã€‚

```python
from agents import Agent

prompt_result = await server.get_prompt(
    "generate_code_review_instructions",
    {"focus": "security vulnerabilities", "language": "python"},
)
instructions = prompt_result.messages[0].content.text

agent = Agent(
    name="Code Reviewer",
    instructions=instructions,
    mcp_servers=[server],
)
```

## ç¼“å­˜

æ¯æ¬¡æ™ºèƒ½ä½“è¿è¡Œéƒ½ä¼šå¯¹æ¯ä¸ª MCP æœåŠ¡è°ƒç”¨ `list_tools()`ã€‚è¿œç¨‹æœåŠ¡å¯èƒ½å¼•å…¥æ˜æ˜¾çš„å»¶è¿Ÿï¼Œå› æ­¤æ‰€æœ‰ MCP æœåŠ¡ç±»éƒ½æš´éœ²äº† `cache_tools_list` é€‰é¡¹ã€‚åªæœ‰åœ¨ç¡®ä¿¡å·¥å…·å®šä¹‰ä¸ä¼šé¢‘ç¹å˜åŒ–æ—¶æ‰å°†å…¶è®¾ä¸º `True`ã€‚è‹¥è¦ç¨åå¼ºåˆ¶è·å–æœ€æ–°åˆ—è¡¨ï¼Œè¯·åœ¨æœåŠ¡å®ä¾‹ä¸Šè°ƒç”¨ `invalidate_tools_cache()`ã€‚

## è¿½è¸ª

[è¿½è¸ª](./tracing.md)ä¼šè‡ªåŠ¨æ•è· MCP æ´»åŠ¨ï¼ŒåŒ…æ‹¬ï¼š

1. è°ƒç”¨ MCP æœåŠ¡åˆ—å‡ºå·¥å…·ã€‚
2. å·¥å…·è°ƒç”¨ä¸­çš„ MCP ç›¸å…³ä¿¡æ¯ã€‚

![MCP Tracing Screenshot](../assets/images/mcp-tracing.jpg)

## å»¶ä¼¸é˜…è¯»

- [Model Context Protocol](https://modelcontextprotocol.io/) â€“ è§„èŒƒä¸è®¾è®¡æŒ‡å—ã€‚
- [examples/mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp) â€“ å¯è¿è¡Œçš„ stdioã€SSE å’Œå¯æµå¼ HTTP ç¤ºä¾‹ã€‚
- [examples/hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp) â€“ å®Œæ•´çš„æ‰˜ç®¡ MCP æ¼”ç¤ºï¼ŒåŒ…æ‹¬å®¡æ‰¹ä¸è¿æ¥å™¨ã€‚

================
File: docs/zh/multi_agent.md
================
---
search:
  exclude: true
---
# ç¼–æ’å¤šä¸ªæ™ºèƒ½ä½“

ç¼–æ’æŒ‡çš„æ˜¯åº”ç”¨ä¸­æ™ºèƒ½ä½“çš„æµç¨‹ï¼šå“ªäº›æ™ºèƒ½ä½“è¿è¡Œã€ä»¥ä½•ç§é¡ºåºè¿è¡Œï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•å†³å®šæ¥ä¸‹æ¥å‘ç”Ÿä»€ä¹ˆã€‚æœ‰ä¸¤ç§ä¸»è¦çš„ç¼–æ’æ–¹å¼ï¼š

1. è®© LLM å†³ç­–ï¼šåˆ©ç”¨ LLM çš„æ™ºèƒ½è¿›è¡Œè§„åˆ’ã€æ¨ç†ï¼Œå¹¶æ®æ­¤å†³å®šé‡‡å–çš„æ­¥éª¤ã€‚
2. é€šè¿‡ä»£ç ç¼–æ’ï¼šç”¨ä½ çš„ä»£ç æ¥å†³å®šæ™ºèƒ½ä½“çš„æµç¨‹ã€‚

ä½ å¯ä»¥æ··åˆä½¿ç”¨è¿™äº›æ¨¡å¼ã€‚æ¯ç§æ¨¡å¼éƒ½æœ‰å–èˆï¼Œè¯¦è§ä¸‹æ–‡ã€‚

## é€šè¿‡ LLM ç¼–æ’

æ™ºèƒ½ä½“æ˜¯é…å¤‡äº†æŒ‡ä»¤ã€å·¥å…·å’Œä»»åŠ¡è½¬ç§»çš„ LLMã€‚è¿™æ„å‘³ç€é¢å¯¹ä¸€ä¸ªå¼€æ”¾å¼ä»»åŠ¡ï¼ŒLLM å¯ä»¥è‡ªä¸»è§„åˆ’å¦‚ä½•å®Œæˆä»»åŠ¡ï¼Œä½¿ç”¨å·¥å…·æ‰§è¡Œæ“ä½œå¹¶è·å–æ•°æ®ï¼Œå¹¶é€šè¿‡ä»»åŠ¡è½¬ç§»å°†å·¥ä½œå§”æ´¾ç»™å­æ™ºèƒ½ä½“ã€‚æ¯”å¦‚ï¼Œä¸€ä¸ªç ”ç©¶ç±»æ™ºèƒ½ä½“å¯ä»¥é…å¤‡å¦‚ä¸‹å·¥å…·ï¼š

- ç½‘ç»œæ£€ç´¢ä»¥åœ¨çº¿æŸ¥æ‰¾ä¿¡æ¯
- æ–‡ä»¶æ£€ç´¢ä¸æå–ä»¥æœç´¢ä¸“æœ‰æ•°æ®åŠè¿æ¥
- è®¡ç®—æœºæ“ä½œä»¥åœ¨è®¡ç®—æœºä¸Šæ‰§è¡Œæ“ä½œ
- ä»£ç æ‰§è¡Œä»¥è¿›è¡Œæ•°æ®åˆ†æ
- ä»»åŠ¡è½¬ç§»åˆ°æ“…é•¿è§„åˆ’ã€æŠ¥å‘Šæ’°å†™ç­‰å·¥ä½œçš„ä¸“ä¸šæ™ºèƒ½ä½“

å½“ä»»åŠ¡æ˜¯å¼€æ”¾å¼ä¸”å¸Œæœ›ä¾èµ– LLM æ™ºèƒ½æ—¶ï¼Œè¿™ç§æ¨¡å¼éå¸¸é€‚åˆã€‚å…³é”®åšæ³•åŒ…æ‹¬ï¼š

1. æ‰“ç£¨é«˜è´¨é‡æç¤ºè¯ã€‚æ˜ç¡®å¯ç”¨çš„å·¥å…·ã€å¦‚ä½•ä½¿ç”¨å®ƒä»¬ï¼Œä»¥åŠå¿…é¡»éµå®ˆçš„å‚æ•°èŒƒå›´ã€‚
2. ç›‘æ§åº”ç”¨å¹¶è¿­ä»£æ”¹è¿›ã€‚æ‰¾å‡ºé—®é¢˜æ‰€åœ¨ï¼Œå¹¶è¿­ä»£ä¼˜åŒ–ä½ çš„æç¤ºè¯ã€‚
3. å…è®¸æ™ºèƒ½ä½“è‡ªæˆ‘åæ€ä¸æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œè®©å…¶åœ¨å¾ªç¯ä¸­è¿è¡Œå¹¶è‡ªæˆ‘æ‰¹åˆ¤ï¼›æˆ–æä¾›é”™è¯¯ä¿¡æ¯å¹¶è®©å…¶æ”¹è¿›ã€‚
4. ä½¿ç”¨åœ¨å•ä¸€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²çš„ä¸“ç”¨æ™ºèƒ½ä½“ï¼Œè€ŒéæœŸæœ›ä¸€ä¸ªé€šç”¨æ™ºèƒ½ä½“åœ¨æ‰€æœ‰æ–¹é¢éƒ½å¾ˆå¼ºã€‚
5. æŠ•å…¥äº[è¯„æµ‹ï¼ˆevalsï¼‰](https://platform.openai.com/docs/guides/evals)ã€‚è¿™æœ‰åŠ©äºè®­ç»ƒä½ çš„æ™ºèƒ½ä½“ï¼Œæå‡å…¶ä»»åŠ¡èƒ½åŠ›ã€‚

## é€šè¿‡ä»£ç ç¼–æ’

å°½ç®¡é€šè¿‡ LLM ç¼–æ’å¾ˆå¼ºå¤§ï¼Œä½†é€šè¿‡ä»£ç ç¼–æ’å¯ä»¥åœ¨é€Ÿåº¦ã€æˆæœ¬å’Œæ€§èƒ½æ–¹é¢è®©ä»»åŠ¡æ›´å…·ç¡®å®šæ€§å’Œå¯é¢„æµ‹æ€§ã€‚å¸¸è§æ¨¡å¼åŒ…æ‹¬ï¼š

- ä½¿ç”¨[structured outputs](https://platform.openai.com/docs/guides/structured-outputs)ç”Ÿæˆä½ å¯ä»¥ç”¨ä»£ç æ£€æŸ¥çš„æ ¼å¼è‰¯å¥½çš„æ•°æ®ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥è®©æ™ºèƒ½ä½“å°†ä»»åŠ¡å½’ç±»ä¸ºè‹¥å¹²ç›®å½•ï¼Œç„¶ååŸºäºè¯¥ç›®å½•é€‰æ‹©ä¸‹ä¸€ä¸ªæ™ºèƒ½ä½“ã€‚
- å°†å¤šä¸ªæ™ºèƒ½ä½“ä¸²è”èµ·æ¥ï¼ŒæŠŠä¸€ä¸ªçš„è¾“å‡ºè½¬åŒ–ä¸ºä¸‹ä¸€ä¸ªçš„è¾“å…¥ã€‚ä½ å¯ä»¥å°†æ’°å†™åšå®¢æ–‡ç« çš„ä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ­¥éª¤â€”â€”åšç ”ç©¶ã€å†™å¤§çº²ã€å†™æ­£æ–‡ã€æ‰¹åˆ¤æ€§å®¡é˜…ã€å†æ”¹è¿›ã€‚
- è®©æ‰§è¡Œä»»åŠ¡çš„æ™ºèƒ½ä½“ä¸ä¸€ä¸ªè¯„ä¼°å¹¶æä¾›åé¦ˆçš„æ™ºèƒ½ä½“ä¸€èµ·åœ¨ `while` å¾ªç¯ä¸­è¿è¡Œï¼Œç›´åˆ°è¯„ä¼°è€…è®¤ä¸ºè¾“å‡ºé€šè¿‡ç‰¹å®šæ ‡å‡†ä¸ºæ­¢ã€‚
- å¹¶è¡Œè¿è¡Œå¤šä¸ªæ™ºèƒ½ä½“ï¼Œä¾‹å¦‚é€šè¿‡ Python åŸºæœ¬ç»„ä»¶å¦‚ `asyncio.gather`ã€‚å½“ä½ æœ‰å½¼æ­¤ç‹¬ç«‹çš„å¤šä¸ªä»»åŠ¡æ—¶ï¼Œè¿™æœ‰åŠ©äºæé€Ÿã€‚

æˆ‘ä»¬åœ¨[`examples/agent_patterns`](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns)ä¸­æä¾›äº†è‹¥å¹² code examplesã€‚

================
File: docs/zh/quickstart.md
================
---
search:
  exclude: true
---
# å¿«é€Ÿå¼€å§‹

## åˆ›å»ºé¡¹ç›®å’Œè™šæ‹Ÿç¯å¢ƒ

ä½ åªéœ€æ‰§è¡Œä¸€æ¬¡ã€‚

```bash
mkdir my_project
cd my_project
python -m venv .venv
```

### æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ

æ¯æ¬¡å¼€å¯æ–°çš„ç»ˆç«¯ä¼šè¯éƒ½éœ€è¦æ‰§è¡Œã€‚

```bash
source .venv/bin/activate
```

### å®‰è£… Agents SDK

```bash
pip install openai-agents # or `uv add openai-agents`, etc
```

### è®¾ç½® OpenAI API å¯†é’¥

å¦‚æœä½ è¿˜æ²¡æœ‰ï¼Œè¯·æŒ‰ç…§[è¿™äº›è¯´æ˜](https://platform.openai.com/docs/quickstart#create-and-export-an-api-key)åˆ›å»ºä¸€ä¸ª OpenAI API å¯†é’¥ã€‚

```bash
export OPENAI_API_KEY=sk-...
```

## åˆ›å»ºä½ çš„ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“

æ™ºèƒ½ä½“ç”± instructionsã€åç§°ä»¥åŠå¯é€‰é…ç½®ï¼ˆä¾‹å¦‚ `model_config`ï¼‰å®šä¹‰ã€‚

```python
from agents import Agent

agent = Agent(
    name="Math Tutor",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)
```

## æ·»åŠ æ›´å¤šæ™ºèƒ½ä½“

å¯ä»¥ç”¨ç›¸åŒæ–¹å¼å®šä¹‰å…¶ä»–æ™ºèƒ½ä½“ã€‚`handoff_descriptions` ä¸ºç¡®å®šä»»åŠ¡è½¬ç§»è·¯ç”±æä¾›é¢å¤–ä¸Šä¸‹æ–‡ã€‚

```python
from agents import Agent

history_tutor_agent = Agent(
    name="History Tutor",
    handoff_description="Specialist agent for historical questions",
    instructions="You provide assistance with historical queries. Explain important events and context clearly.",
)

math_tutor_agent = Agent(
    name="Math Tutor",
    handoff_description="Specialist agent for math questions",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)
```

## å®šä¹‰ä½ çš„ä»»åŠ¡è½¬ç§»

åœ¨æ¯ä¸ªæ™ºèƒ½ä½“ä¸Šï¼Œä½ å¯ä»¥å®šä¹‰ä¸€ä»½å¯ç”¨çš„å¤–å‘ä»»åŠ¡è½¬ç§»é€‰é¡¹æ¸…å•ï¼Œä¾›æ™ºèƒ½ä½“é€‰æ‹©ä»¥å†³å®šå¦‚ä½•æ¨è¿›å…¶ä»»åŠ¡ã€‚

```python
triage_agent = Agent(
    name="Triage Agent",
    instructions="You determine which agent to use based on the user's homework question",
    handoffs=[history_tutor_agent, math_tutor_agent]
)
```

## è¿è¡Œæ™ºèƒ½ä½“ç¼–æ’

æˆ‘ä»¬æ¥æ£€æŸ¥å·¥ä½œæµæ˜¯å¦è¿è¡Œæ­£å¸¸ï¼Œä»¥åŠåˆ†è¯Šæ™ºèƒ½ä½“æ˜¯å¦åœ¨ä¸¤ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ä¹‹é—´æ­£ç¡®è·¯ç”±ã€‚

```python
from agents import Runner

async def main():
    result = await Runner.run(triage_agent, "What is the capital of France?")
    print(result.final_output)
```

## æ·»åŠ å®‰å…¨é˜²æŠ¤æªæ–½

ä½ å¯ä»¥åœ¨è¾“å…¥æˆ–è¾“å‡ºä¸Šå®šä¹‰è‡ªå®šä¹‰çš„å®‰å…¨é˜²æŠ¤æªæ–½ã€‚

```python
from agents import GuardrailFunctionOutput, Agent, Runner
from pydantic import BaseModel


class HomeworkOutput(BaseModel):
    is_homework: bool
    reasoning: str

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the user is asking about homework.",
    output_type=HomeworkOutput,
)

async def homework_guardrail(ctx, agent, input_data):
    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
    final_output = result.final_output_as(HomeworkOutput)
    return GuardrailFunctionOutput(
        output_info=final_output,
        tripwire_triggered=not final_output.is_homework,
    )
```

## æ±‡æ€»æ•´åˆ

è®©æˆ‘ä»¬æŠŠä»¥ä¸Šç»„åˆèµ·æ¥ï¼Œè¿è¡Œå®Œæ•´å·¥ä½œæµï¼Œå¹¶ä½¿ç”¨ä»»åŠ¡è½¬ç§»å’Œè¾“å…¥å®‰å…¨é˜²æŠ¤æªæ–½ã€‚

```python
from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner
from agents.exceptions import InputGuardrailTripwireTriggered
from pydantic import BaseModel
import asyncio

class HomeworkOutput(BaseModel):
    is_homework: bool
    reasoning: str

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the user is asking about homework.",
    output_type=HomeworkOutput,
)

math_tutor_agent = Agent(
    name="Math Tutor",
    handoff_description="Specialist agent for math questions",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)

history_tutor_agent = Agent(
    name="History Tutor",
    handoff_description="Specialist agent for historical questions",
    instructions="You provide assistance with historical queries. Explain important events and context clearly.",
)


async def homework_guardrail(ctx, agent, input_data):
    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
    final_output = result.final_output_as(HomeworkOutput)
    return GuardrailFunctionOutput(
        output_info=final_output,
        tripwire_triggered=not final_output.is_homework,
    )

triage_agent = Agent(
    name="Triage Agent",
    instructions="You determine which agent to use based on the user's homework question",
    handoffs=[history_tutor_agent, math_tutor_agent],
    input_guardrails=[
        InputGuardrail(guardrail_function=homework_guardrail),
    ],
)

async def main():
    # Example 1: History question
    try:
        result = await Runner.run(triage_agent, "who was the first president of the united states?")
        print(result.final_output)
    except InputGuardrailTripwireTriggered as e:
        print("Guardrail blocked this input:", e)

    # Example 2: General/philosophical question
    try:
        result = await Runner.run(triage_agent, "What is the meaning of life?")
        print(result.final_output)
    except InputGuardrailTripwireTriggered as e:
        print("Guardrail blocked this input:", e)

if __name__ == "__main__":
    asyncio.run(main())
```

## æŸ¥çœ‹ä½ çš„è¿½è¸ª

è¦å›é¡¾æ™ºèƒ½ä½“è¿è¡ŒæœŸé—´å‘ç”Ÿçš„äº‹æƒ…ï¼Œè¯·å‰å¾€ [OpenAI æ§åˆ¶å°ä¸­çš„ Trace æŸ¥çœ‹å™¨](https://platform.openai.com/traces)æŸ¥çœ‹ä½ çš„æ™ºèƒ½ä½“è¿è¡Œè¿½è¸ªã€‚

## åç»­æ­¥éª¤

äº†è§£å¦‚ä½•æ„å»ºæ›´å¤æ‚çš„æ™ºèƒ½ä½“æµç¨‹ï¼š

- å­¦ä¹ å¦‚ä½•é…ç½®[æ™ºèƒ½ä½“](agents.md)ã€‚
- äº†è§£[è¿è¡Œæ™ºèƒ½ä½“](running_agents.md)ã€‚
- äº†è§£[å·¥å…·](tools.md)ã€[å®‰å…¨é˜²æŠ¤æªæ–½](guardrails.md)å’Œ[æ¨¡å‹](models/index.md)ã€‚

================
File: docs/zh/release.md
================
---
search:
  exclude: true
---
# å‘è¡Œæµç¨‹/å˜æ›´æ—¥å¿—

æœ¬é¡¹ç›®é‡‡ç”¨ç»è½»å¾®ä¿®æ”¹çš„è¯­ä¹‰åŒ–ç‰ˆæœ¬æ§åˆ¶æ–¹æ¡ˆï¼Œç‰ˆæœ¬å·å½¢å¼ä¸º `0.Y.Z`ã€‚å‰å¯¼çš„ `0` è¡¨ç¤ºè¯¥ SDK ä»åœ¨å¿«é€Ÿæ¼”è¿›ä¸­ã€‚å„éƒ¨åˆ†çš„é€’å¢è§„åˆ™å¦‚ä¸‹ï¼š

## æ¬¡è¦ï¼ˆ`Y`ï¼‰ç‰ˆæœ¬

å¯¹äºæœªæ ‡æ³¨ä¸º beta çš„ä»»ä½•å…¬å…±æ¥å£å‡ºç°çš„**ç ´åæ€§å˜æ›´**ï¼Œæˆ‘ä»¬å°†æå‡æ¬¡è¦ç‰ˆæœ¬å· `Y`ã€‚ä¾‹å¦‚ï¼Œä» `0.0.x` å‡çº§åˆ° `0.1.x` å¯èƒ½åŒ…å«ç ´åæ€§å˜æ›´ã€‚

å¦‚æœä½ ä¸å¸Œæœ›å¼•å…¥ç ´åæ€§å˜æ›´ï¼Œå»ºè®®åœ¨é¡¹ç›®ä¸­å›ºå®šä¾èµ–åˆ° `0.0.x` ç‰ˆæœ¬ã€‚

## è¡¥ä¸ï¼ˆ`Z`ï¼‰ç‰ˆæœ¬

å¯¹äºéç ´åæ€§å˜æ›´ï¼Œæˆ‘ä»¬å°†é€’å¢ `Z`ï¼š

- ç¼ºé™·ä¿®å¤
- æ–°åŠŸèƒ½
- ç§æœ‰æ¥å£çš„å˜æ›´
- beta åŠŸèƒ½çš„æ›´æ–°

## ç ´åæ€§å˜æ›´æ—¥å¿—

### 0.2.0

åœ¨æ­¤ç‰ˆæœ¬ä¸­ï¼Œè‹¥å¹²åŸæœ¬æ¥æ”¶ `Agent` ä½œä¸ºå‚æ•°çš„ä½ç½®ï¼Œç°åœ¨æ”¹ä¸ºæ¥æ”¶ `AgentBase` ä½œä¸ºå‚æ•°ã€‚ä¾‹å¦‚ï¼ŒMCP æœåŠ¡ä¸­çš„ `list_tools()` è°ƒç”¨ã€‚è¿™çº¯å±ç±»å‹å±‚é¢çš„å˜æ›´ï¼Œä½ ä»å°†æ”¶åˆ° `Agent` å¯¹è±¡ã€‚è¦æ›´æ–°çš„è¯ï¼Œåªéœ€å°†ç±»å‹é”™è¯¯ä¸­çš„ `Agent` æ›¿æ¢ä¸º `AgentBase` å³å¯ã€‚

### 0.1.0

åœ¨æ­¤ç‰ˆæœ¬ä¸­ï¼Œ[`MCPServer.list_tools()`][agents.mcp.server.MCPServer] æ–°å¢äº†ä¸¤ä¸ªå‚æ•°ï¼š`run_context` å’Œ `agent`ã€‚ä½ éœ€è¦åœ¨ä»»ä½•ç»§æ‰¿è‡ª `MCPServer` çš„ç±»ä¸­æ·»åŠ è¿™äº›å‚æ•°ã€‚

================
File: docs/zh/repl.md
================
---
search:
  exclude: true
---
# REPL å®ç”¨å·¥å…·

SDK æä¾›äº† `run_demo_loop`ï¼Œå¯åœ¨ç»ˆç«¯ä¸­å¿«é€Ÿã€äº¤äº’å¼åœ°æµ‹è¯•æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚

```python
import asyncio
from agents import Agent, run_demo_loop

async def main() -> None:
    agent = Agent(name="Assistant", instructions="You are a helpful assistant.")
    await run_demo_loop(agent)

if __name__ == "__main__":
    asyncio.run(main())
```

`run_demo_loop` ä¼šåœ¨å¾ªç¯ä¸­æç¤ºè¾“å…¥ç”¨æˆ·è¾“å…¥ï¼Œå¹¶åœ¨è½®æ¬¡ä¹‹é—´ä¿ç•™å¯¹è¯å†å²ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒä¼šåœ¨ç”Ÿæˆæ—¶ä»¥æµå¼ä¼ è¾“çš„æ–¹å¼è¾“å‡ºæ¨¡å‹ç»“æœã€‚è¿è¡Œä¸Šè¿°ç¤ºä¾‹åï¼Œrun_demo_loop ä¼šå¯åŠ¨ä¸€ä¸ªäº¤äº’å¼èŠå¤©ä¼šè¯ã€‚å®ƒä¼šæŒç»­è¯·æ±‚ä½ çš„è¾“å…¥ï¼Œè®°ä½è½®æ¬¡ä¹‹é—´çš„æ•´ä¸ªå¯¹è¯å†å²ï¼ˆå› æ­¤ä½ çš„æ™ºèƒ½ä½“çŸ¥é“å·²è®¨è®ºçš„å†…å®¹ï¼‰ï¼Œå¹¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®æ—¶è‡ªåŠ¨å‘ä½ æµå¼ä¼ è¾“æ™ºèƒ½ä½“çš„å›å¤ã€‚

è¦ç»“æŸæ­¤èŠå¤©ä¼šè¯ï¼Œåªéœ€è¾“å…¥ `quit` æˆ– `exit`ï¼ˆå¹¶æŒ‰å›è½¦ï¼‰ï¼Œæˆ–ä½¿ç”¨ `Ctrl-D` é”®ç›˜å¿«æ·é”®ã€‚

================
File: docs/zh/results.md
================
---
search:
  exclude: true
---
# ç»“æœ

å½“ä½ è°ƒç”¨ `Runner.run` æ–¹æ³•æ—¶ï¼Œä½ ä¼šå¾—åˆ°ï¼š

-   å¦‚æœè°ƒç”¨ `run` æˆ– `run_sync`ï¼Œåˆ™è¿”å› [`RunResult`][agents.result.RunResult]
-   å¦‚æœè°ƒç”¨ `run_streamed`ï¼Œåˆ™è¿”å› [`RunResultStreaming`][agents.result.RunResultStreaming]

ä¸¤è€…éƒ½ç»§æ‰¿è‡ª [`RunResultBase`][agents.result.RunResultBase]ï¼Œå¤§å¤šæ•°æœ‰ç”¨ä¿¡æ¯éƒ½åœ¨è¿™é‡Œã€‚

## æœ€ç»ˆè¾“å‡º

[`final_output`][agents.result.RunResultBase.final_output] å±æ€§åŒ…å«æœ€åä¸€ä¸ªè¿è¡Œçš„æ™ºèƒ½ä½“çš„æœ€ç»ˆè¾“å‡ºã€‚å®ƒå¯èƒ½æ˜¯ï¼š

-   `str`ï¼Œå¦‚æœæœ€åä¸€ä¸ªæ™ºèƒ½ä½“æœªå®šä¹‰ `output_type`
-   ç±»å‹ä¸º `last_agent.output_type` çš„å¯¹è±¡ï¼Œå¦‚æœè¯¥æ™ºèƒ½ä½“å®šä¹‰äº†è¾“å‡ºç±»å‹

!!! note

    `final_output` çš„ç±»å‹æ˜¯ `Any`ã€‚ç”±äºå­˜åœ¨ ä»»åŠ¡è½¬ç§»ï¼ˆhandoffsï¼‰ï¼Œæˆ‘ä»¬æ— æ³•è¿›è¡Œé™æ€ç±»å‹æ ‡æ³¨ã€‚å¦‚æœå‘ç”Ÿä»»åŠ¡è½¬ç§»ï¼Œä»»ä½•æ™ºèƒ½ä½“éƒ½å¯èƒ½æˆä¸ºæœ€åä¸€ä¸ªè¿è¡Œçš„æ™ºèƒ½ä½“ï¼Œå› æ­¤æˆ‘ä»¬åœ¨é™æ€ä¸Šæ— æ³•çŸ¥é“å¯èƒ½çš„è¾“å‡ºç±»å‹é›†åˆã€‚

## ä¸‹ä¸€è½®è¾“å…¥

ä½ å¯ä»¥ä½¿ç”¨ [`result.to_input_list()`][agents.result.RunResultBase.to_input_list] å°†ç»“æœè½¬æ¢ä¸ºä¸€ä¸ªè¾“å…¥åˆ—è¡¨ï¼Œè¯¥åˆ—è¡¨æŠŠä½ æä¾›çš„åŸå§‹è¾“å…¥ä¸æ™ºèƒ½ä½“è¿è¡ŒæœŸé—´ç”Ÿæˆçš„æ¡ç›®è¿æ¥èµ·æ¥ã€‚è¿™æ ·å¯ä»¥æ–¹ä¾¿åœ°å°†ä¸€æ¬¡æ™ºèƒ½ä½“è¿è¡Œçš„è¾“å‡ºä¼ é€’ç»™å¦ä¸€æ¬¡è¿è¡Œï¼Œæˆ–åœ¨å¾ªç¯ä¸­è¿è¡Œå¹¶åœ¨æ¯æ¬¡è¿½åŠ æ–°çš„ç”¨æˆ·è¾“å…¥ã€‚

## æœ€åä¸€ä¸ªæ™ºèƒ½ä½“

[`last_agent`][agents.result.RunResultBase.last_agent] å±æ€§åŒ…å«æœ€åä¸€ä¸ªè¿è¡Œçš„æ™ºèƒ½ä½“ã€‚æ ¹æ®ä½ çš„åº”ç”¨ï¼Œè¿™é€šå¸¸å¯¹ä¸‹ä¸€æ¬¡ç”¨æˆ·è¾“å…¥å¾ˆæœ‰ç”¨ã€‚æ¯”å¦‚ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªå‰çº¿åˆ†è¯Šæ™ºèƒ½ä½“ä¼šå°†ä»»åŠ¡è½¬ç§»ç»™ç‰¹å®šè¯­è¨€çš„æ™ºèƒ½ä½“ï¼Œä½ å¯ä»¥å­˜å‚¨è¿™ä¸ªæœ€åçš„æ™ºèƒ½ä½“ï¼Œå¹¶åœ¨ç”¨æˆ·ä¸‹æ¬¡å‘æ™ºèƒ½ä½“å‘é€æ¶ˆæ¯æ—¶å¤ç”¨å®ƒã€‚

## æ–°æ¡ç›®

[`new_items`][agents.result.RunResultBase.new_items] å±æ€§åŒ…å«è¿è¡ŒæœŸé—´ç”Ÿæˆçš„æ–°æ¡ç›®ã€‚æ¡ç›®ä¸º [`RunItem`][agents.items.RunItem]ã€‚è¿è¡Œæ¡ç›®å°è£…äº†ç”± LLM ç”Ÿæˆçš„åŸå§‹æ¡ç›®ã€‚

-   [`MessageOutputItem`][agents.items.MessageOutputItem] è¡¨ç¤ºæ¥è‡ª LLM çš„æ¶ˆæ¯ã€‚åŸå§‹æ¡ç›®æ˜¯ç”Ÿæˆçš„æ¶ˆæ¯ã€‚
-   [`HandoffCallItem`][agents.items.HandoffCallItem] è¡¨ç¤º LLM è°ƒç”¨äº†ä»»åŠ¡è½¬ç§»å·¥å…·ã€‚åŸå§‹æ¡ç›®æ˜¯æ¥è‡ª LLM çš„å·¥å…·è°ƒç”¨æ¡ç›®ã€‚
-   [`HandoffOutputItem`][agents.items.HandoffOutputItem] è¡¨ç¤ºå‘ç”Ÿäº†ä»»åŠ¡è½¬ç§»ã€‚åŸå§‹æ¡ç›®æ˜¯å¯¹ä»»åŠ¡è½¬ç§»å·¥å…·è°ƒç”¨çš„å·¥å…·å“åº”ã€‚ä½ ä¹Ÿå¯ä»¥ä»æ¡ç›®ä¸­è®¿é—®æº/ç›®æ ‡æ™ºèƒ½ä½“ã€‚
-   [`ToolCallItem`][agents.items.ToolCallItem] è¡¨ç¤º LLM è°ƒç”¨äº†æŸä¸ªå·¥å…·ã€‚
-   [`ToolCallOutputItem`][agents.items.ToolCallOutputItem] è¡¨ç¤ºæŸä¸ªå·¥å…·è¢«è°ƒç”¨ã€‚åŸå§‹æ¡ç›®æ˜¯å·¥å…·å“åº”ã€‚ä½ ä¹Ÿå¯ä»¥ä»æ¡ç›®ä¸­è®¿é—®å·¥å…·è¾“å‡ºã€‚
-   [`ReasoningItem`][agents.items.ReasoningItem] è¡¨ç¤ºæ¥è‡ª LLM çš„æ¨ç†æ¡ç›®ã€‚åŸå§‹æ¡ç›®æ˜¯ç”Ÿæˆçš„æ¨ç†ã€‚

## å…¶ä»–ä¿¡æ¯

### å®‰å…¨é˜²æŠ¤æªæ–½ç»“æœ

[`input_guardrail_results`][agents.result.RunResultBase.input_guardrail_results] å’Œ [`output_guardrail_results`][agents.result.RunResultBase.output_guardrail_results] å±æ€§åŒ…å«ï¼ˆè‹¥æœ‰ï¼‰å®‰å…¨é˜²æŠ¤æªæ–½çš„ç»“æœã€‚å®‰å…¨é˜²æŠ¤æªæ–½çš„ç»“æœæœ‰æ—¶åŒ…å«ä½ å¯èƒ½å¸Œæœ›è®°å½•æˆ–å­˜å‚¨çš„æœ‰ç”¨ä¿¡æ¯ï¼Œå› æ­¤æˆ‘ä»¬å°†å…¶æä¾›ç»™ä½ ã€‚

### åŸå§‹å“åº”

[`raw_responses`][agents.result.RunResultBase.raw_responses] å±æ€§åŒ…å«ç”± LLM ç”Ÿæˆçš„ [`ModelResponse`][agents.items.ModelResponse]ã€‚

### åŸå§‹è¾“å…¥

[`input`][agents.result.RunResultBase.input] å±æ€§åŒ…å«ä½ ä¼ é€’ç»™ `run` æ–¹æ³•çš„åŸå§‹è¾“å…¥ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä½ ä¸éœ€è¦å®ƒï¼Œä½†åœ¨éœ€è¦æ—¶å¯ä»¥ä½¿ç”¨ã€‚

================
File: docs/zh/running_agents.md
================
---
search:
  exclude: true
---
# è¿è¡Œæ™ºèƒ½ä½“

ä½ å¯ä»¥é€šè¿‡ [`Runner`][agents.run.Runner] ç±»æ¥è¿è¡Œæ™ºèƒ½ä½“ã€‚ä½ æœ‰ 3 ç§é€‰æ‹©ï¼š

1. [`Runner.run()`][agents.run.Runner.run]ï¼šå¼‚æ­¥è¿è¡Œå¹¶è¿”å› [`RunResult`][agents.result.RunResult]ã€‚
2. [`Runner.run_sync()`][agents.run.Runner.run_sync]ï¼šåŒæ­¥æ–¹æ³•ï¼Œå†…éƒ¨å®é™…è°ƒç”¨ `.run()`ã€‚
3. [`Runner.run_streamed()`][agents.run.Runner.run_streamed]ï¼šå¼‚æ­¥è¿è¡Œå¹¶è¿”å› [`RunResultStreaming`][agents.result.RunResultStreaming]ã€‚å®ƒä»¥æµå¼æ¨¡å¼è°ƒç”¨ LLMï¼Œå¹¶åœ¨æ¥æ”¶åˆ°äº‹ä»¶æ—¶å°†å…¶æµå¼ä¼ è¾“ç»™ä½ ã€‚

```python
from agents import Agent, Runner

async def main():
    agent = Agent(name="Assistant", instructions="You are a helpful assistant")

    result = await Runner.run(agent, "Write a haiku about recursion in programming.")
    print(result.final_output)
    # Code within the code,
    # Functions calling themselves,
    # Infinite loop's dance
```

åœ¨[ç»“æœæŒ‡å—](results.md)ä¸­äº†è§£æ›´å¤šã€‚

## æ™ºèƒ½ä½“å¾ªç¯

å½“ä½ åœ¨ `Runner` ä¸­ä½¿ç”¨ run æ–¹æ³•æ—¶ï¼Œä½ ä¼šä¼ å…¥ä¸€ä¸ªèµ·å§‹æ™ºèƒ½ä½“å’Œè¾“å…¥ã€‚è¾“å…¥å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼ˆè§†ä¸ºç”¨æˆ·æ¶ˆæ¯ï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯è¾“å…¥é¡¹åˆ—è¡¨ï¼Œè¿™äº›è¾“å…¥é¡¹ç¬¦åˆ OpenAI Responses API çš„æ ¼å¼ã€‚

runner éšåè¿è¡Œä¸€ä¸ªå¾ªç¯ï¼š

1. æˆ‘ä»¬é’ˆå¯¹å½“å‰æ™ºèƒ½ä½“ä¸å½“å‰è¾“å…¥è°ƒç”¨ LLMã€‚
2. LLM äº§ç”Ÿè¾“å‡ºã€‚
    1. å¦‚æœ LLM è¿”å› `final_output`ï¼Œå¾ªç¯ç»“æŸå¹¶è¿”å›ç»“æœã€‚
    2. å¦‚æœ LLM è¿›è¡Œä»»åŠ¡è½¬ç§»ï¼Œæˆ‘ä»¬ä¼šæ›´æ–°å½“å‰æ™ºèƒ½ä½“ä¸è¾“å…¥ï¼Œå¹¶é‡æ–°è¿è¡Œå¾ªç¯ã€‚
    3. å¦‚æœ LLM äº§ç”Ÿå·¥å…·è°ƒç”¨ï¼Œæˆ‘ä»¬ä¼šè¿è¡Œè¿™äº›å·¥å…·è°ƒç”¨ã€é™„åŠ ç»“æœï¼Œå¹¶é‡æ–°è¿è¡Œå¾ªç¯ã€‚
3. å¦‚æœè¶…è¿‡ä¼ å…¥çš„ `max_turns`ï¼Œæˆ‘ä»¬ä¼šæŠ›å‡º [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded] å¼‚å¸¸ã€‚

!!! note

    æ˜¯å¦è§†ä¸ºâ€œæœ€ç»ˆè¾“å‡ºâ€çš„è§„åˆ™æ˜¯ï¼šLLM ç”Ÿæˆäº†å…·æœ‰æœŸæœ›ç±»å‹çš„æ–‡æœ¬è¾“å‡ºï¼Œä¸”æ²¡æœ‰å·¥å…·è°ƒç”¨ã€‚

## æµå¼ä¼ è¾“

æµå¼ä¼ è¾“å…è®¸ä½ åœ¨ LLM è¿è¡Œæ—¶é¢å¤–æ¥æ”¶æµå¼äº‹ä»¶ã€‚æµç»“æŸåï¼Œ[`RunResultStreaming`][agents.result.RunResultStreaming] å°†åŒ…å«å…³äºæœ¬æ¬¡è¿è¡Œçš„å®Œæ•´ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ–°äº§ç”Ÿçš„è¾“å‡ºã€‚ä½ å¯ä»¥è°ƒç”¨ `.stream_events()` è·å–æµå¼äº‹ä»¶ã€‚å‚è§[æµå¼ä¼ è¾“æŒ‡å—](streaming.md)ã€‚

## è¿è¡Œé…ç½®

`run_config` å‚æ•°å¯ç”¨äºä¸ºæ™ºèƒ½ä½“è¿è¡Œé…ç½®ä¸€äº›å…¨å±€è®¾ç½®ï¼š

- [`model`][agents.run.RunConfig.model]ï¼šå…è®¸è®¾ç½®ä¸€ä¸ªå…¨å±€ä½¿ç”¨çš„ LLM æ¨¡å‹ï¼Œè€Œä¸è€ƒè™‘æ¯ä¸ª Agent è‡ªèº«çš„ `model`ã€‚
- [`model_provider`][agents.run.RunConfig.model_provider]ï¼šç”¨äºæŸ¥æ‰¾æ¨¡å‹åç§°çš„æ¨¡å‹æä¾›æ–¹ï¼Œé»˜è®¤ä¸º OpenAIã€‚
- [`model_settings`][agents.run.RunConfig.model_settings]ï¼šè¦†ç›–æ™ºèƒ½ä½“ç‰¹å®šè®¾ç½®ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥å…¨å±€è®¾ç½® `temperature` æˆ– `top_p`ã€‚
- [`input_guardrails`][agents.run.RunConfig.input_guardrails], [`output_guardrails`][agents.run.RunConfig.output_guardrails]ï¼šåœ¨æ‰€æœ‰è¿è¡Œä¸­åŒ…å«çš„è¾“å…¥æˆ–è¾“å‡ºå®‰å…¨é˜²æŠ¤æªæ–½åˆ—è¡¨ã€‚
- [`handoff_input_filter`][agents.run.RunConfig.handoff_input_filter]ï¼šé€‚ç”¨äºæ‰€æœ‰ä»»åŠ¡è½¬ç§»çš„å…¨å±€è¾“å…¥è¿‡æ»¤å™¨ï¼ˆå¦‚æœè¯¥ä»»åŠ¡è½¬ç§»å°šæœªè®¾ç½®ï¼‰ã€‚è¾“å…¥è¿‡æ»¤å™¨å…è®¸ä½ ç¼–è¾‘å‘é€ç»™æ–°æ™ºèƒ½ä½“çš„è¾“å…¥ã€‚è¯¦è§ [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] çš„æ–‡æ¡£ã€‚
- [`tracing_disabled`][agents.run.RunConfig.tracing_disabled]ï¼šå…è®¸ä¸ºæ•´ä¸ªè¿è¡Œç¦ç”¨[è¿½è¸ª](tracing.md)ã€‚
- [`trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data]ï¼šé…ç½®è¿½è¸ªä¸­æ˜¯å¦åŒ…å«æ½œåœ¨æ•æ„Ÿæ•°æ®ï¼Œä¾‹å¦‚ LLM ä¸å·¥å…·è°ƒç”¨çš„è¾“å…¥/è¾“å‡ºã€‚
- [`workflow_name`][agents.run.RunConfig.workflow_name], [`trace_id`][agents.run.RunConfig.trace_id], [`group_id`][agents.run.RunConfig.group_id]ï¼šä¸ºæœ¬æ¬¡è¿è¡Œè®¾ç½®è¿½è¸ªçš„å·¥ä½œæµåç§°ã€è¿½è¸ª ID å’Œè¿½è¸ªåˆ†ç»„ IDã€‚æˆ‘ä»¬å»ºè®®è‡³å°‘è®¾ç½® `workflow_name`ã€‚åˆ†ç»„ ID ä¸ºå¯é€‰å­—æ®µï¼Œå…è®¸ä½ åœ¨å¤šæ¬¡è¿è¡Œä¹‹é—´å…³è”è¿½è¸ªã€‚
- [`trace_metadata`][agents.run.RunConfig.trace_metadata]ï¼šè¦åŒ…å«åœ¨æ‰€æœ‰è¿½è¸ªä¸­çš„å…ƒæ•°æ®ã€‚

## ä¼šè¯/èŠå¤©çº¿ç¨‹

è°ƒç”¨ä»»æ„è¿è¡Œæ–¹æ³•éƒ½å¯èƒ½å¯¼è‡´ä¸€ä¸ªæˆ–å¤šä¸ªæ™ºèƒ½ä½“è¿è¡Œï¼ˆå› æ­¤ä¹Ÿå¯èƒ½æœ‰ä¸€ä¸ªæˆ–å¤šä¸ª LLM è°ƒç”¨ï¼‰ï¼Œä½†å®ƒä»£è¡¨èŠå¤©ä¼šè¯ä¸­çš„å•ä¸ªé€»è¾‘è½®æ¬¡ã€‚ä¾‹å¦‚ï¼š

1. ç”¨æˆ·è½®æ¬¡ï¼šç”¨æˆ·è¾“å…¥æ–‡æœ¬
2. Runner è¿è¡Œï¼šç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“è°ƒç”¨ LLMã€è¿è¡Œå·¥å…·ã€è¿›è¡Œä¸€æ¬¡ä»»åŠ¡è½¬ç§»åˆ°ç¬¬äºŒä¸ªæ™ºèƒ½ä½“ï¼Œç¬¬äºŒä¸ªæ™ºèƒ½ä½“è¿è¡Œæ›´å¤šå·¥å…·ï¼Œç„¶åäº§ç”Ÿè¾“å‡ºã€‚

åœ¨æ™ºèƒ½ä½“è¿è¡Œç»“æŸæ—¶ï¼Œä½ å¯ä»¥é€‰æ‹©å‘ç”¨æˆ·å±•ç¤ºçš„å†…å®¹ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥å°†æ™ºèƒ½ä½“ç”Ÿæˆçš„æ¯ä¸ªæ–°é¡¹éƒ½å±•ç¤ºç»™ç”¨æˆ·ï¼Œæˆ–åªå±•ç¤ºæœ€ç»ˆè¾“å‡ºã€‚æ— è®ºå“ªç§æ–¹å¼ï¼Œç”¨æˆ·éƒ½å¯èƒ½æå‡ºåç»­é—®é¢˜ï¼Œæ­¤æ—¶ä½ å¯ä»¥å†æ¬¡è°ƒç”¨ run æ–¹æ³•ã€‚

### æ‰‹åŠ¨ä¼šè¯ç®¡ç†

ä½ å¯ä»¥ä½¿ç”¨ [`RunResultBase.to_input_list()`][agents.result.RunResultBase.to_input_list] æ–¹æ³•æ‰‹åŠ¨ç®¡ç†ä¼šè¯å†å²ï¼Œä»è€Œè·å–ä¸‹ä¸€è½®çš„è¾“å…¥ï¼š

```python
async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    thread_id = "thread_123"  # Example thread ID
    with trace(workflow_name="Conversation", group_id=thread_id):
        # First turn
        result = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
        print(result.final_output)
        # San Francisco

        # Second turn
        new_input = result.to_input_list() + [{"role": "user", "content": "What state is it in?"}]
        result = await Runner.run(agent, new_input)
        print(result.final_output)
        # California
```

### ä½¿ç”¨ Sessions çš„è‡ªåŠ¨ä¼šè¯ç®¡ç†

æ›´ç®€å•çš„åšæ³•æ˜¯ä½¿ç”¨ [Sessions](sessions.md) è‡ªåŠ¨å¤„ç†ä¼šè¯å†å²ï¼Œè€Œæ— éœ€æ‰‹åŠ¨è°ƒç”¨ `.to_input_list()`ï¼š

```python
from agents import Agent, Runner, SQLiteSession

async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    # Create session instance
    session = SQLiteSession("conversation_123")

    thread_id = "thread_123"  # Example thread ID
    with trace(workflow_name="Conversation", group_id=thread_id):
        # First turn
        result = await Runner.run(agent, "What city is the Golden Gate Bridge in?", session=session)
        print(result.final_output)
        # San Francisco

        # Second turn - agent automatically remembers previous context
        result = await Runner.run(agent, "What state is it in?", session=session)
        print(result.final_output)
        # California
```

Sessions ä¼šè‡ªåŠ¨ï¼š

- åœ¨æ¯æ¬¡è¿è¡Œå‰è·å–ä¼šè¯å†å²
- åœ¨æ¯æ¬¡è¿è¡Œåå­˜å‚¨æ–°æ¶ˆæ¯
- ä¸ºä¸åŒçš„ä¼šè¯ ID ç»´æŠ¤ç‹¬ç«‹çš„ä¼šè¯

æ›´å¤šè¯¦æƒ…å‚è§ [Sessions æ–‡æ¡£](sessions.md)ã€‚

### æœåŠ¡ç«¯æ‰˜ç®¡çš„ä¼šè¯

ä½ ä¹Ÿå¯ä»¥è®© OpenAI çš„ä¼šè¯çŠ¶æ€åŠŸèƒ½åœ¨æœåŠ¡ç«¯ç®¡ç†ä¼šè¯çŠ¶æ€ï¼Œè€Œæ— éœ€åœ¨æœ¬åœ°é€šè¿‡ `to_input_list()` æˆ– `Sessions` å¤„ç†ã€‚è¿™æ ·å¯ä»¥åœ¨ä¸æ‰‹åŠ¨é‡å‘æ‰€æœ‰å†å²æ¶ˆæ¯çš„æƒ…å†µä¸‹ä¿ç•™ä¼šè¯å†å²ã€‚è¯¦æƒ…å‚è§ [OpenAI Conversation state æŒ‡å—](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses)ã€‚

OpenAI æä¾›ä¸¤ç§åœ¨å¤šè½®ä¹‹é—´è·Ÿè¸ªçŠ¶æ€çš„æ–¹å¼ï¼š

#### 1. ä½¿ç”¨ `conversation_id`

ä½ é¦–å…ˆä½¿ç”¨ OpenAI Conversations API åˆ›å»ºä¸€ä¸ªä¼šè¯ï¼Œç„¶ååœ¨åç»­æ¯æ¬¡è°ƒç”¨ä¸­å¤ç”¨å…¶ IDï¼š

```python
from agents import Agent, Runner
from openai import AsyncOpenAI

client = AsyncOpenAI()

async def main():
    # Create a server-managed conversation
    conversation = await client.conversations.create()
    conv_id = conversation.id    

    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    # First turn
    result1 = await Runner.run(agent, "What city is the Golden Gate Bridge in?", conversation_id=conv_id)
    print(result1.final_output)
    # San Francisco

    # Second turn reuses the same conversation_id
    result2 = await Runner.run(
        agent,
        "What state is it in?",
        conversation_id=conv_id,
    )
    print(result2.final_output)
    # California
```

#### 2. ä½¿ç”¨ `previous_response_id`

å¦ä¸€ç§é€‰æ‹©æ˜¯**å“åº”ä¸²è”**ï¼ˆresponse chainingï¼‰ï¼Œå³æ¯ä¸€è½®éƒ½æ˜¾å¼é“¾æ¥åˆ°ä¸Šä¸€è½®çš„ response IDã€‚

```python
from agents import Agent, Runner

async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    # First turn
    result1 = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
    print(result1.final_output)
    # San Francisco

    # Second turn, chained to the previous response
    result2 = await Runner.run(
        agent,
        "What state is it in?",
        previous_response_id=result1.last_response_id,
    )
    print(result2.final_output)
    # California
```

## é•¿æ—¶é—´è¿è¡Œçš„æ™ºèƒ½ä½“ä¸äººå·¥åœ¨ç¯

ä½ å¯ä»¥ä½¿ç”¨ Agents SDK çš„ [Temporal](https://temporal.io/) é›†æˆæ¥è¿è¡ŒæŒä¹…çš„ã€é•¿æ—¶é—´è¿è¡Œçš„å·¥ä½œæµï¼ŒåŒ…æ‹¬äººå·¥åœ¨ç¯ä»»åŠ¡ã€‚åœ¨[æ­¤è§†é¢‘](https://www.youtube.com/watch?v=fFBZqzT4DD8)ä¸­æŸ¥çœ‹ Temporal ä¸ Agents SDK ååŒå®Œæˆé•¿æ—¶ä»»åŠ¡çš„æ¼”ç¤ºï¼Œå¹¶[åœ¨æ­¤æŸ¥çœ‹æ–‡æ¡£](https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents)ã€‚

## å¼‚å¸¸

SDK åœ¨æŸäº›æƒ…å†µä¸‹ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚å®Œæ•´åˆ—è¡¨è§ [`agents.exceptions`][]ã€‚æ¦‚è§ˆå¦‚ä¸‹ï¼š

- [`AgentsException`][agents.exceptions.AgentsException]ï¼šSDK å†…æŠ›å‡ºçš„æ‰€æœ‰å¼‚å¸¸çš„åŸºç±»ã€‚å®ƒä½œä¸ºé€šç”¨ç±»å‹ï¼Œå…¶ä»–ç‰¹å®šå¼‚å¸¸å‡ä»å…¶æ´¾ç”Ÿã€‚
- [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded]ï¼šå½“æ™ºèƒ½ä½“è¿è¡Œè¶…è¿‡ä¼ é€’ç»™ `Runner.run`ã€`Runner.run_sync` æˆ– `Runner.run_streamed` æ–¹æ³•çš„ `max_turns` é™åˆ¶æ—¶æŠ›å‡ºã€‚å®ƒè¡¨ç¤ºæ™ºèƒ½ä½“æ— æ³•åœ¨æŒ‡å®šçš„äº¤äº’è½®æ•°å†…å®Œæˆä»»åŠ¡ã€‚
- [`ModelBehaviorError`][agents.exceptions.ModelBehaviorError]ï¼šå½“åº•å±‚æ¨¡å‹ï¼ˆLLMï¼‰äº§ç”Ÿæ„å¤–æˆ–æ— æ•ˆè¾“å‡ºæ—¶å‘ç”Ÿã€‚è¿™å¯èƒ½åŒ…æ‹¬ï¼š
    - æ ¼å¼é”™è¯¯çš„ JSONï¼šå½“æ¨¡å‹ä¸ºå·¥å…·è°ƒç”¨æˆ–å…¶ç›´æ¥è¾“å‡ºæä¾›äº†æ ¼å¼é”™è¯¯çš„ JSON ç»“æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨å®šä¹‰äº†ç‰¹å®š `output_type` æ—¶ã€‚
    - æ„å¤–çš„å·¥å…·ç›¸å…³å¤±è´¥ï¼šå½“æ¨¡å‹æœªæŒ‰é¢„æœŸæ–¹å¼ä½¿ç”¨å·¥å…·æ—¶
- [`UserError`][agents.exceptions.UserError]ï¼šå½“ä½ ï¼ˆä½¿ç”¨è¯¥ SDK ç¼–å†™ä»£ç çš„äººï¼‰åœ¨ä½¿ç”¨ SDK æ—¶å‘ç”Ÿé”™è¯¯æ—¶æŠ›å‡ºã€‚é€šå¸¸ç”±é”™è¯¯çš„ä»£ç å®ç°ã€æ— æ•ˆé…ç½®æˆ–å¯¹ SDK API çš„è¯¯ç”¨å¯¼è‡´ã€‚
- [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered], [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered]ï¼šå½“åˆ†åˆ«æ»¡è¶³è¾“å…¥å®‰å…¨é˜²æŠ¤æªæ–½æˆ–è¾“å‡ºå®‰å…¨é˜²æŠ¤æªæ–½çš„æ¡ä»¶æ—¶æŠ›å‡ºã€‚è¾“å…¥å®‰å…¨é˜²æŠ¤æªæ–½åœ¨å¤„ç†å‰æ£€æŸ¥ä¼ å…¥æ¶ˆæ¯ï¼Œè€Œè¾“å‡ºå®‰å…¨é˜²æŠ¤æªæ–½åœ¨äº¤ä»˜å‰æ£€æŸ¥æ™ºèƒ½ä½“çš„æœ€ç»ˆå“åº”ã€‚

================
File: docs/zh/streaming.md
================
---
search:
  exclude: true
---
# æµå¼ä¼ è¾“

æµå¼ä¼ è¾“å…è®¸ä½ åœ¨æ™ºèƒ½ä½“è¿è¡Œè¿‡ç¨‹ä¸­è®¢é˜…æ›´æ–°ã€‚è¿™å¯¹äºå‘ç»ˆç«¯ç”¨æˆ·å±•ç¤ºè¿›åº¦æ›´æ–°å’Œéƒ¨åˆ†å“åº”éå¸¸æœ‰ç”¨ã€‚

è¦è¿›è¡Œæµå¼ä¼ è¾“ï¼Œä½ å¯ä»¥è°ƒç”¨[`Runner.run_streamed()`][agents.run.Runner.run_streamed]ï¼Œå®ƒä¼šè¿”å›ä¸€ä¸ª[`RunResultStreaming`][agents.result.RunResultStreaming]ã€‚è°ƒç”¨`result.stream_events()`ä¼šç»™ä½ ä¸€ä¸ªå¼‚æ­¥çš„[`StreamEvent`][agents.stream_events.StreamEvent]å¯¹è±¡æµï¼Œè¯¦è§ä¸‹æ–‡ã€‚

## åŸå§‹å“åº”äº‹ä»¶

[`RawResponsesStreamEvent`][agents.stream_events.RawResponsesStreamEvent]æ˜¯ç›´æ¥ä» LLM ä¼ é€’çš„åŸå§‹äº‹ä»¶ã€‚å®ƒä»¬é‡‡ç”¨ OpenAI Responses API æ ¼å¼ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªäº‹ä»¶éƒ½æœ‰ä¸€ä¸ªç±»å‹ï¼ˆä¾‹å¦‚ `response.created`ã€`response.output_text.delta` ç­‰ï¼‰å’Œæ•°æ®ã€‚å¦‚æœä½ å¸Œæœ›åœ¨æ¶ˆæ¯ç”Ÿæˆæ—¶ç«‹åˆ»å°†å…¶æµå¼ä¼ è¾“ç»™ç”¨æˆ·ï¼Œè¿™äº›äº‹ä»¶ä¼šéå¸¸æœ‰ç”¨ã€‚

ä¾‹å¦‚ï¼Œä»¥ä¸‹ä»£ç å°†é€ token è¾“å‡ºç”± LLM ç”Ÿæˆçš„æ–‡æœ¬ã€‚

```python
import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner

async def main():
    agent = Agent(
        name="Joker",
        instructions="You are a helpful assistant.",
    )

    result = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            print(event.data.delta, end="", flush=True)


if __name__ == "__main__":
    asyncio.run(main())
```

## è¿è¡Œé¡¹äº‹ä»¶ä¸æ™ºèƒ½ä½“äº‹ä»¶

[`RunItemStreamEvent`][agents.stream_events.RunItemStreamEvent] æ˜¯æ›´é«˜å±‚æ¬¡çš„äº‹ä»¶ã€‚å®ƒä¼šåœ¨æŸä¸ªæ¡ç›®å®Œå…¨ç”Ÿæˆæ—¶é€šçŸ¥ä½ ã€‚ç›¸æ¯”äºé€ä¸ª tokenï¼Œè¿™ä½¿ä½ èƒ½å¤Ÿåœ¨â€œæ¶ˆæ¯å·²ç”Ÿæˆâ€â€œå·¥å…·å·²è¿è¡Œâ€ç­‰å±‚é¢æ¨é€è¿›åº¦æ›´æ–°ã€‚ç±»ä¼¼åœ°ï¼Œ[`AgentUpdatedStreamEvent`][agents.stream_events.AgentUpdatedStreamEvent] ä¼šåœ¨å½“å‰æ™ºèƒ½ä½“å‘ç”Ÿå˜åŒ–æ—¶ï¼ˆä¾‹å¦‚ç”±äºä¸€æ¬¡ä»»åŠ¡è½¬ç§»ï¼‰å‘ä½ æä¾›æ›´æ–°ã€‚

ä¾‹å¦‚ï¼Œä»¥ä¸‹ä»£ç ä¼šå¿½ç•¥åŸå§‹äº‹ä»¶ï¼Œä»…å°†æ›´æ–°æµå¼ä¼ è¾“ç»™ç”¨æˆ·ã€‚

```python
import asyncio
import random
from agents import Agent, ItemHelpers, Runner, function_tool

@function_tool
def how_many_jokes() -> int:
    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="Joker",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],
    )

    result = Runner.run_streamed(
        agent,
        input="Hello",
    )
    print("=== Run starting ===")

    async for event in result.stream_events():
        # We'll ignore the raw responses event deltas
        if event.type == "raw_response_event":
            continue
        # When the agent updates, print that
        elif event.type == "agent_updated_stream_event":
            print(f"Agent updated: {event.new_agent.name}")
            continue
        # When items are generated, print them
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}")
            else:
                pass  # Ignore other event types

    print("=== Run complete ===")


if __name__ == "__main__":
    asyncio.run(main())
```

================
File: docs/zh/tools.md
================
---
search:
  exclude: true
---
# å·¥å…·

å·¥å…·è®©æ™ºèƒ½ä½“èƒ½å¤Ÿé‡‡å–è¡ŒåŠ¨ï¼šä¾‹å¦‚è·å–æ•°æ®ã€è¿è¡Œä»£ç ã€è°ƒç”¨å¤–éƒ¨ APIï¼Œç”šè‡³è¿›è¡Œè®¡ç®—æœºæ“ä½œã€‚Agents SDK ä¸­æœ‰ä¸‰ç±»å·¥å…·ï¼š

- æ‰˜ç®¡å·¥å…·ï¼šè¿™äº›å·¥å…·åœ¨ LLM æœåŠ¡ç«¯ä¸ AI æ¨¡å‹å¹¶è¡Œè¿è¡Œã€‚OpenAI æä¾›æ£€ç´¢ã€ç½‘ç»œæ£€ç´¢å’Œè®¡ç®—æœºæ“ä½œç­‰æ‰˜ç®¡å·¥å…·ã€‚
- Function callingï¼šå®ƒå…è®¸ä½ å°†ä»»æ„ Python å‡½æ•°ç”¨ä½œå·¥å…·ã€‚
- æ™ºèƒ½ä½“ä½œä¸ºå·¥å…·ï¼šè¿™å…è®¸ä½ å°†ä¸€ä¸ªæ™ºèƒ½ä½“ä½œä¸ºå·¥å…·ä½¿ç”¨ï¼Œä½¿æ™ºèƒ½ä½“æ— éœ€è¿›è¡Œä»»åŠ¡è½¬ç§»å³å¯è°ƒç”¨å…¶ä»–æ™ºèƒ½ä½“ã€‚

## æ‰˜ç®¡å·¥å…·

åœ¨ä½¿ç”¨ [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] æ—¶ï¼ŒOpenAI æä¾›äº†ä¸€äº›å†…ç½®å·¥å…·ï¼š

- [`WebSearchTool`][agents.tool.WebSearchTool] è®©æ™ºèƒ½ä½“å¯ä»¥è¿›è¡Œç½‘ç»œæ£€ç´¢ã€‚
- [`FileSearchTool`][agents.tool.FileSearchTool] å…è®¸ä»ä½ çš„ OpenAI å‘é‡å­˜å‚¨ä¸­æ£€ç´¢ä¿¡æ¯ã€‚
- [`ComputerTool`][agents.tool.ComputerTool] å…è®¸è‡ªåŠ¨åŒ–è®¡ç®—æœºæ“ä½œä»»åŠ¡ã€‚
- [`CodeInterpreterTool`][agents.tool.CodeInterpreterTool] è®© LLM åœ¨æ²™ç®±ç¯å¢ƒä¸­æ‰§è¡Œä»£ç ã€‚
- [`HostedMCPTool`][agents.tool.HostedMCPTool] å°†è¿œç¨‹ MCP æœåŠ¡çš„å·¥å…·æš´éœ²ç»™æ¨¡å‹ã€‚
- [`ImageGenerationTool`][agents.tool.ImageGenerationTool] åŸºäºæç¤ºç”Ÿæˆå›¾åƒã€‚
- [`LocalShellTool`][agents.tool.LocalShellTool] åœ¨ä½ çš„æœºå™¨ä¸Šè¿è¡Œ shell å‘½ä»¤ã€‚

```python
from agents import Agent, FileSearchTool, Runner, WebSearchTool

agent = Agent(
    name="Assistant",
    tools=[
        WebSearchTool(),
        FileSearchTool(
            max_num_results=3,
            vector_store_ids=["VECTOR_STORE_ID"],
        ),
    ],
)

async def main():
    result = await Runner.run(agent, "Which coffee shop should I go to, taking into account my preferences and the weather today in SF?")
    print(result.final_output)
```

## å·¥å…·è°ƒç”¨

ä½ å¯ä»¥å°†ä»»æ„ Python å‡½æ•°ç”¨ä½œå·¥å…·ã€‚Agents SDK ä¼šè‡ªåŠ¨å®Œæˆå·¥å…·è®¾ç½®ï¼š

- å·¥å…·åç§°å°†æ˜¯è¯¥ Python å‡½æ•°çš„åç§°ï¼ˆæˆ–ä½ å¯è‡ªå®šä¹‰åç§°ï¼‰
- å·¥å…·æè¿°å°†å–è‡ªå‡½æ•°çš„ docstringï¼ˆæˆ–ä½ å¯è‡ªå®šä¹‰æè¿°ï¼‰
- å‡½æ•°è¾“å…¥çš„ schema ä¼šæ ¹æ®å‡½æ•°å‚æ•°è‡ªåŠ¨åˆ›å»º
- å„è¾“å…¥çš„æè¿°å°†å–è‡ªå‡½æ•°çš„ docstringï¼Œé™¤éç¦ç”¨

æˆ‘ä»¬ä½¿ç”¨ Python çš„ `inspect` æ¨¡å—æå–å‡½æ•°ç­¾åï¼Œä½¿ç”¨ [`griffe`](https://mkdocstrings.github.io/griffe/) è§£æ docstringï¼Œå¹¶ç”¨ `pydantic` åˆ›å»º schemaã€‚

```python
import json

from typing_extensions import TypedDict, Any

from agents import Agent, FunctionTool, RunContextWrapper, function_tool


class Location(TypedDict):
    lat: float
    long: float

@function_tool  # (1)!
async def fetch_weather(location: Location) -> str:
    # (2)!
    """Fetch the weather for a given location.

    Args:
        location: The location to fetch the weather for.
    """
    # In real life, we'd fetch the weather from a weather API
    return "sunny"


@function_tool(name_override="fetch_data")  # (3)!
def read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -> str:
    """Read the contents of a file.

    Args:
        path: The path to the file to read.
        directory: The directory to read the file from.
    """
    # In real life, we'd read the file from the file system
    return "<file contents>"


agent = Agent(
    name="Assistant",
    tools=[fetch_weather, read_file],  # (4)!
)

for tool in agent.tools:
    if isinstance(tool, FunctionTool):
        print(tool.name)
        print(tool.description)
        print(json.dumps(tool.params_json_schema, indent=2))
        print()

```

1. ä½ å¯ä»¥åœ¨å‡½æ•°å‚æ•°ä¸­ä½¿ç”¨ä»»æ„ Python ç±»å‹ï¼Œå‡½æ•°å¯ä»¥æ˜¯åŒæ­¥æˆ–å¼‚æ­¥ã€‚
2. å¦‚æœå­˜åœ¨ docstringï¼Œåˆ™ç”¨äºæå–å·¥å…·æè¿°å’Œå‚æ•°æè¿°ã€‚
3. å‡½æ•°å¯é€‰åœ°æ¥æ”¶ `context`ï¼ˆå¿…é¡»æ˜¯ç¬¬ä¸€ä¸ªå‚æ•°ï¼‰ã€‚ä½ ä¹Ÿå¯ä»¥è®¾ç½®è¦†ç›–é¡¹ï¼Œå¦‚å·¥å…·åç§°ã€æè¿°ã€ä½¿ç”¨çš„ docstring é£æ ¼ç­‰ã€‚
4. ä½ å¯ä»¥å°†è£…é¥°åçš„å‡½æ•°ä¼ å…¥å·¥å…·åˆ—è¡¨ä¸­ã€‚

??? note "å±•å¼€æŸ¥çœ‹è¾“å‡º"

    ```
    fetch_weather
    Fetch the weather for a given location.
    {
    "$defs": {
      "Location": {
        "properties": {
          "lat": {
            "title": "Lat",
            "type": "number"
          },
          "long": {
            "title": "Long",
            "type": "number"
          }
        },
        "required": [
          "lat",
          "long"
        ],
        "title": "Location",
        "type": "object"
      }
    },
    "properties": {
      "location": {
        "$ref": "#/$defs/Location",
        "description": "The location to fetch the weather for."
      }
    },
    "required": [
      "location"
    ],
    "title": "fetch_weather_args",
    "type": "object"
    }

    fetch_data
    Read the contents of a file.
    {
    "properties": {
      "path": {
        "description": "The path to the file to read.",
        "title": "Path",
        "type": "string"
      },
      "directory": {
        "anyOf": [
          {
            "type": "string"
          },
          {
            "type": "null"
          }
        ],
        "default": null,
        "description": "The directory to read the file from.",
        "title": "Directory"
      }
    },
    "required": [
      "path"
    ],
    "title": "fetch_data_args",
    "type": "object"
    }
    ```

### ä»å·¥å…·è°ƒç”¨è¿”å›å›¾åƒæˆ–æ–‡ä»¶

é™¤äº†è¿”å›æ–‡æœ¬è¾“å‡ºå¤–ï¼Œä½ è¿˜å¯ä»¥å°†ä¸€ä¸ªæˆ–å¤šä¸ªå›¾åƒæˆ–æ–‡ä»¶ä½œä¸ºå‡½æ•°å·¥å…·çš„è¾“å‡ºã€‚ä¸ºæ­¤ï¼Œä½ å¯ä»¥è¿”å›ä»¥ä¸‹ä»»æ„ä¸€ç§ï¼š

- å›¾åƒï¼š[`ToolOutputImage`][agents.tool.ToolOutputImage]ï¼ˆæˆ– TypedDict ç‰ˆæœ¬ [`ToolOutputImageDict`][agents.tool.ToolOutputImageDict]ï¼‰
- æ–‡ä»¶ï¼š[`ToolOutputFileContent`][agents.tool.ToolOutputFileContent]ï¼ˆæˆ– TypedDict ç‰ˆæœ¬ [`ToolOutputFileContentDict`][agents.tool.ToolOutputFileContentDict]ï¼‰
- æ–‡æœ¬ï¼šå­—ç¬¦ä¸²æˆ–å¯è½¬ä¸ºå­—ç¬¦ä¸²çš„å¯¹è±¡ï¼Œæˆ– [`ToolOutputText`][agents.tool.ToolOutputText]ï¼ˆæˆ– TypedDict ç‰ˆæœ¬ [`ToolOutputTextDict`][agents.tool.ToolOutputTextDict]ï¼‰

### è‡ªå®šä¹‰å‡½æ•°å·¥å…·

æœ‰æ—¶ä½ å¹¶ä¸æƒ³ç”¨ Python å‡½æ•°ä½œä¸ºå·¥å…·ã€‚å¦‚æœéœ€è¦ï¼Œä½ å¯ä»¥ç›´æ¥åˆ›å»ºä¸€ä¸ª [`FunctionTool`][agents.tool.FunctionTool]ã€‚ä½ éœ€è¦æä¾›ï¼š

- `name`
- `description`
- `params_json_schema`ï¼Œå³å‚æ•°çš„ JSON schema
- `on_invoke_tool`ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¼‚æ­¥å‡½æ•°ï¼Œæ¥æ”¶ [`ToolContext`][agents.tool_context.ToolContext] å’Œä»¥ JSON å­—ç¬¦ä¸²è¡¨ç¤ºçš„å‚æ•°ï¼Œå¹¶ä¸”å¿…é¡»è¿”å›å­—ç¬¦ä¸²å½¢å¼çš„å·¥å…·è¾“å‡ºã€‚

```python
from typing import Any

from pydantic import BaseModel

from agents import RunContextWrapper, FunctionTool



def do_some_work(data: str) -> str:
    return "done"


class FunctionArgs(BaseModel):
    username: str
    age: int


async def run_function(ctx: RunContextWrapper[Any], args: str) -> str:
    parsed = FunctionArgs.model_validate_json(args)
    return do_some_work(data=f"{parsed.username} is {parsed.age} years old")


tool = FunctionTool(
    name="process_user",
    description="Processes extracted user data",
    params_json_schema=FunctionArgs.model_json_schema(),
    on_invoke_tool=run_function,
)
```

### å‚æ•°ä¸ docstring çš„è‡ªåŠ¨è§£æ

å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬ä¼šè‡ªåŠ¨è§£æå‡½æ•°ç­¾åä»¥æå–å·¥å…·çš„ schemaï¼Œå¹¶è§£æ docstring æ¥æå–å·¥å…·åŠå„å‚æ•°çš„æè¿°ã€‚æ³¨æ„äº‹é¡¹ï¼š

1. ä½¿ç”¨ `inspect` æ¨¡å—è¿›è¡Œç­¾åè§£æã€‚æˆ‘ä»¬åˆ©ç”¨ç±»å‹æ³¨è§£äº†è§£å‚æ•°ç±»å‹ï¼Œå¹¶åŠ¨æ€æ„å»ºä¸€ä¸ª Pydantic æ¨¡å‹æ¥è¡¨ç¤ºæ•´ä½“ schemaã€‚å®ƒæ”¯æŒå¤§å¤šæ•°ç±»å‹ï¼ŒåŒ…æ‹¬ Python åŸºæœ¬ç»„ä»¶ã€Pydantic æ¨¡å‹ã€TypedDict ç­‰ã€‚
2. æˆ‘ä»¬ä½¿ç”¨ `griffe` è§£æ docstringã€‚æ”¯æŒçš„ docstring æ ¼å¼åŒ…æ‹¬ `google`ã€`sphinx` å’Œ `numpy`ã€‚æˆ‘ä»¬ä¼šå°è¯•è‡ªåŠ¨æ£€æµ‹ docstring æ ¼å¼ï¼Œä½†è¿™åªæ˜¯å°½åŠ›è€Œä¸ºï¼Œä½ ä¹Ÿå¯ä»¥åœ¨è°ƒç”¨ `function_tool` æ—¶æ˜¾å¼è®¾ç½®ã€‚ä½ ä¹Ÿå¯ä»¥é€šè¿‡å°† `use_docstring_info` è®¾ä¸º `False` æ¥ç¦ç”¨ docstring è§£æã€‚

ç”¨äºæå– schema çš„ä»£ç ä½äº [`agents.function_schema`][]ã€‚

## æ™ºèƒ½ä½“ä½œä¸ºå·¥å…·

åœ¨æŸäº›å·¥ä½œæµä¸­ï¼Œä½ å¯èƒ½å¸Œæœ›ç”±ä¸€ä¸ªä¸­å¿ƒæ™ºèƒ½ä½“æ¥ç¼–æ’ä¸€ç»„ä¸“ä¸šåŒ–æ™ºèƒ½ä½“çš„ç½‘ç»œï¼Œè€Œä¸æ˜¯è¿›è¡Œä»»åŠ¡è½¬ç§»ã€‚ä½ å¯ä»¥é€šè¿‡å°†æ™ºèƒ½ä½“å»ºæ¨¡ä¸ºå·¥å…·æ¥å®ç°è¿™ä¸€ç‚¹ã€‚

```python
from agents import Agent, Runner
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You translate the user's message to Spanish",
)

french_agent = Agent(
    name="French agent",
    instructions="You translate the user's message to French",
)

orchestrator_agent = Agent(
    name="orchestrator_agent",
    instructions=(
        "You are a translation agent. You use the tools given to you to translate."
        "If asked for multiple translations, you call the relevant tools."
    ),
    tools=[
        spanish_agent.as_tool(
            tool_name="translate_to_spanish",
            tool_description="Translate the user's message to Spanish",
        ),
        french_agent.as_tool(
            tool_name="translate_to_french",
            tool_description="Translate the user's message to French",
        ),
    ],
)

async def main():
    result = await Runner.run(orchestrator_agent, input="Say 'Hello, how are you?' in Spanish.")
    print(result.final_output)
```

### è‡ªå®šä¹‰å·¥å…·åŒ–æ™ºèƒ½ä½“

`agent.as_tool` å‡½æ•°æ˜¯ä¸€ä¸ªæ–¹ä¾¿æ–¹æ³•ï¼Œä¾¿äºå°†æ™ºèƒ½ä½“è½¬æ¢ä¸ºå·¥å…·ã€‚ä¸è¿‡å®ƒå¹¶ä¸æ”¯æŒæ‰€æœ‰é…ç½®ï¼›ä¾‹å¦‚ï¼Œä½ æ— æ³•è®¾ç½® `max_turns`ã€‚å¯¹äºé«˜çº§ç”¨ä¾‹ï¼Œè¯·åœ¨ä½ çš„å·¥å…·å®ç°ä¸­ç›´æ¥ä½¿ç”¨ `Runner.run`ï¼š

```python
@function_tool
async def run_my_agent() -> str:
    """A tool that runs the agent with custom configs"""

    agent = Agent(name="My agent", instructions="...")

    result = await Runner.run(
        agent,
        input="...",
        max_turns=5,
        run_config=...
    )

    return str(result.final_output)
```

### è‡ªå®šä¹‰è¾“å‡ºæå–

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½å¸Œæœ›åœ¨å°†è¾“å‡ºè¿”å›ç»™ä¸­å¿ƒæ™ºèƒ½ä½“ä¹‹å‰ä¿®æ”¹å·¥å…·åŒ–æ™ºèƒ½ä½“çš„è¾“å‡ºã€‚å¦‚æœä½ å¸Œæœ›ï¼š

- ä»å­æ™ºèƒ½ä½“çš„å¯¹è¯å†å²ä¸­æå–ç‰¹å®šä¿¡æ¯ï¼ˆä¾‹å¦‚ JSON è´Ÿè½½ï¼‰ã€‚
- è½¬æ¢æˆ–é‡æ–°æ ¼å¼åŒ–æ™ºèƒ½ä½“çš„æœ€ç»ˆç­”æ¡ˆï¼ˆä¾‹å¦‚å°† Markdown è½¬æ¢ä¸ºçº¯æ–‡æœ¬æˆ– CSVï¼‰ã€‚
- éªŒè¯è¾“å‡ºï¼Œæˆ–åœ¨æ™ºèƒ½ä½“å“åº”ç¼ºå¤±æˆ–æ ¼å¼é”™è¯¯æ—¶æä¾›å›é€€å€¼ã€‚

ä½ å¯ä»¥é€šè¿‡å‘ `as_tool` æ–¹æ³•æä¾› `custom_output_extractor` å‚æ•°æ¥å®ç°ï¼š

```python
async def extract_json_payload(run_result: RunResult) -> str:
    # Scan the agentâ€™s outputs in reverse order until we find a JSON-like message from a tool call.
    for item in reversed(run_result.new_items):
        if isinstance(item, ToolCallOutputItem) and item.output.strip().startswith("{"):
            return item.output.strip()
    # Fallback to an empty JSON object if nothing was found
    return "{}"


json_tool = data_agent.as_tool(
    tool_name="get_data_json",
    tool_description="Run the data agent and return only its JSON payload",
    custom_output_extractor=extract_json_payload,
)
```

### æ¡ä»¶å¯ç”¨å·¥å…·

ä½ å¯ä»¥åœ¨è¿è¡Œæ—¶ä½¿ç”¨ `is_enabled` å‚æ•°æœ‰æ¡ä»¶åœ°å¯ç”¨æˆ–ç¦ç”¨æ™ºèƒ½ä½“å·¥å…·ã€‚è¿™å…è®¸ä½ åŸºäºä¸Šä¸‹æ–‡ã€ç”¨æˆ·åå¥½æˆ–è¿è¡Œæ—¶æ¡ä»¶åŠ¨æ€ç­›é€‰ LLM å¯ç”¨çš„å·¥å…·ã€‚

```python
import asyncio
from agents import Agent, AgentBase, Runner, RunContextWrapper
from pydantic import BaseModel

class LanguageContext(BaseModel):
    language_preference: str = "french_spanish"

def french_enabled(ctx: RunContextWrapper[LanguageContext], agent: AgentBase) -> bool:
    """Enable French for French+Spanish preference."""
    return ctx.context.language_preference == "french_spanish"

# Create specialized agents
spanish_agent = Agent(
    name="spanish_agent",
    instructions="You respond in Spanish. Always reply to the user's question in Spanish.",
)

french_agent = Agent(
    name="french_agent",
    instructions="You respond in French. Always reply to the user's question in French.",
)

# Create orchestrator with conditional tools
orchestrator = Agent(
    name="orchestrator",
    instructions=(
        "You are a multilingual assistant. You use the tools given to you to respond to users. "
        "You must call ALL available tools to provide responses in different languages. "
        "You never respond in languages yourself, you always use the provided tools."
    ),
    tools=[
        spanish_agent.as_tool(
            tool_name="respond_spanish",
            tool_description="Respond to the user's question in Spanish",
            is_enabled=True,  # Always enabled
        ),
        french_agent.as_tool(
            tool_name="respond_french",
            tool_description="Respond to the user's question in French",
            is_enabled=french_enabled,
        ),
    ],
)

async def main():
    context = RunContextWrapper(LanguageContext(language_preference="french_spanish"))
    result = await Runner.run(orchestrator, "How are you?", context=context.context)
    print(result.final_output)

asyncio.run(main())
```

`is_enabled` å‚æ•°å¯æ¥å—ï¼š

- **å¸ƒå°”å€¼**ï¼š`True`ï¼ˆå§‹ç»ˆå¯ç”¨ï¼‰æˆ– `False`ï¼ˆå§‹ç»ˆç¦ç”¨ï¼‰
- **å¯è°ƒç”¨å‡½æ•°**ï¼šæ¥æ”¶ `(context, agent)` å¹¶è¿”å›å¸ƒå°”å€¼çš„å‡½æ•°
- **å¼‚æ­¥å‡½æ•°**ï¼šç”¨äºå¤æ‚æ¡ä»¶é€»è¾‘çš„å¼‚æ­¥å‡½æ•°

è¢«ç¦ç”¨çš„å·¥å…·åœ¨è¿è¡Œæ—¶ä¼šå®Œå…¨å¯¹ LLM éšè—ï¼Œè¿™å¯¹äºä»¥ä¸‹åœºæ™¯å¾ˆæœ‰ç”¨ï¼š

- åŸºäºç”¨æˆ·æƒé™è¿›è¡ŒåŠŸèƒ½å¼€å…³
- åŸºäºç¯å¢ƒçš„å·¥å…·å¯ç”¨æ€§ï¼ˆå¼€å‘ vs ç”Ÿäº§ï¼‰
- ä¸åŒå·¥å…·é…ç½®çš„ A/B æµ‹è¯•
- åŸºäºè¿è¡Œæ—¶çŠ¶æ€çš„åŠ¨æ€å·¥å…·è¿‡æ»¤

## åœ¨å·¥å…·è°ƒç”¨ä¸­å¤„ç†é”™è¯¯

å½“ä½ é€šè¿‡ `@function_tool` åˆ›å»ºå‡½æ•°å·¥å…·æ—¶ï¼Œå¯ä»¥ä¼ å…¥ä¸€ä¸ª `failure_error_function`ã€‚è¿™æ˜¯ä¸€ä¸ªåœ¨å·¥å…·è°ƒç”¨å´©æºƒæ—¶ä¸º LLM æä¾›é”™è¯¯å“åº”çš„å‡½æ•°ã€‚

- é»˜è®¤æƒ…å†µä¸‹ï¼ˆå³æœªä¼ å…¥ä»»ä½•å€¼ï¼‰ï¼Œä¼šè¿è¡Œ `default_tool_error_function`ï¼Œå‘ŠçŸ¥ LLM å‘ç”Ÿäº†é”™è¯¯ã€‚
- å¦‚æœä½ ä¼ å…¥äº†è‡ªå®šä¹‰é”™è¯¯å‡½æ•°ï¼Œåˆ™ä¼šè¿è¡Œè¯¥å‡½æ•°ï¼Œå¹¶å°†å“åº”å‘é€ç»™ LLMã€‚
- å¦‚æœä½ æ˜¾å¼ä¼ å…¥ `None`ï¼Œåˆ™ä»»ä½•å·¥å…·è°ƒç”¨é”™è¯¯éƒ½ä¼šè¢«é‡æ–°æŠ›å‡ºä¾›ä½ å¤„ç†ã€‚å¦‚æœæ¨¡å‹ç”Ÿæˆäº†æ— æ•ˆ JSONï¼Œè¿™å¯èƒ½æ˜¯ `ModelBehaviorError`ï¼›å¦‚æœä½ çš„ä»£ç å´©æºƒäº†ï¼Œè¿™å¯èƒ½æ˜¯ `UserError`ï¼Œç­‰ç­‰ã€‚

```python
from agents import function_tool, RunContextWrapper
from typing import Any

def my_custom_error_function(context: RunContextWrapper[Any], error: Exception) -> str:
    """A custom function to provide a user-friendly error message."""
    print(f"A tool call failed with the following error: {error}")
    return "An internal server error occurred. Please try again later."

@function_tool(failure_error_function=my_custom_error_function)
def get_user_profile(user_id: str) -> str:
    """Fetches a user profile from a mock API.
     This function demonstrates a 'flaky' or failing API call.
    """
    if user_id == "user_123":
        return "User profile for user_123 successfully retrieved."
    else:
        raise ValueError(f"Could not retrieve profile for user_id: {user_id}. API returned an error.")

```

å¦‚æœä½ æ˜¯æ‰‹åŠ¨åˆ›å»º `FunctionTool` å¯¹è±¡ï¼Œåˆ™å¿…é¡»åœ¨ `on_invoke_tool` å‡½æ•°å†…éƒ¨å¤„ç†é”™è¯¯ã€‚

================
File: docs/zh/tracing.md
================
---
search:
  exclude: true
---
# è¿½è¸ª

Agents SDK å†…ç½®äº†è¿½è¸ªåŠŸèƒ½ï¼Œä¼šåœ¨æ™ºèƒ½ä½“è¿è¡ŒæœŸé—´æ”¶é›†å®Œæ•´çš„äº‹ä»¶è®°å½•ï¼šLLM ç”Ÿæˆã€å·¥å…·è°ƒç”¨ã€ä»»åŠ¡è½¬ç§»ã€å®‰å…¨é˜²æŠ¤æªæ–½ï¼Œç”šè‡³è‡ªå®šä¹‰äº‹ä»¶ã€‚ä½¿ç”¨ [Traces ä»ªè¡¨æ¿](https://platform.openai.com/traces)ï¼Œä½ å¯ä»¥åœ¨å¼€å‘å’Œç”Ÿäº§ç¯å¢ƒä¸­å¯¹å·¥ä½œæµè¿›è¡Œè°ƒè¯•ã€å¯è§†åŒ–å’Œç›‘æ§ã€‚

!!!note

    è¿½è¸ªé»˜è®¤å¯ç”¨ã€‚å¯é€šè¿‡ä¸¤ç§æ–¹å¼ç¦ç”¨è¿½è¸ªï¼š

    1. é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡ `OPENAI_AGENTS_DISABLE_TRACING=1` å…¨å±€ç¦ç”¨è¿½è¸ª
    2. é€šè¿‡å°† [`agents.run.RunConfig.tracing_disabled`][] è®¾ä¸º `True`ï¼Œå¯¹å•æ¬¡è¿è¡Œç¦ç”¨è¿½è¸ª

***å¯¹äºä½¿ç”¨ OpenAI API å¹¶æ‰§è¡Œé›¶æ•°æ®ä¿ç•™ï¼ˆZDRï¼‰ç­–ç•¥çš„ç»„ç»‡ï¼Œè¿½è¸ªä¸å¯ç”¨ã€‚***

## è¿½è¸ªä¸ Span

-   **è¿½è¸ªï¼ˆTracesï¼‰** è¡¨ç¤ºä¸€æ¬¡â€œå·¥ä½œæµâ€çš„ç«¯åˆ°ç«¯æ“ä½œã€‚å®ƒä»¬ç”± Span ç»„æˆã€‚è¿½è¸ªå…·æœ‰ä»¥ä¸‹å±æ€§ï¼š
    -   `workflow_name`ï¼šé€»è¾‘ä¸Šçš„å·¥ä½œæµæˆ–åº”ç”¨ã€‚ä¾‹å¦‚ â€œCode generationâ€ æˆ– â€œCustomer serviceâ€ã€‚
    -   `trace_id`ï¼šè¿½è¸ªçš„å”¯ä¸€ IDã€‚å¦‚æœæœªä¼ å…¥ï¼Œå°†è‡ªåŠ¨ç”Ÿæˆã€‚å¿…é¡»ç¬¦åˆæ ¼å¼ `trace_<32_alphanumeric>`ã€‚
    -   `group_id`ï¼šå¯é€‰çš„åˆ†ç»„ IDï¼Œç”¨äºå…³è”åŒä¸€ä¼šè¯ä¸­çš„å¤šä¸ªè¿½è¸ªã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥ä½¿ç”¨èŠå¤©çº¿ç¨‹ IDã€‚
    -   `disabled`ï¼šè‹¥ä¸º Trueï¼Œåˆ™ä¸ä¼šè®°å½•è¯¥è¿½è¸ªã€‚
    -   `metadata`ï¼šè¿½è¸ªçš„å¯é€‰å…ƒæ•°æ®ã€‚
-   **Span** è¡¨ç¤ºæœ‰å¼€å§‹å’Œç»“æŸæ—¶é—´çš„æ“ä½œã€‚Span å…·æœ‰ï¼š
    -   `started_at` å’Œ `ended_at` æ—¶é—´æˆ³ã€‚
    -   `trace_id`ï¼Œè¡¨ç¤ºå®ƒæ‰€å±çš„è¿½è¸ª
    -   `parent_id`ï¼ŒæŒ‡å‘è¯¥ Span çš„çˆ¶ Spanï¼ˆå¦‚æœæœ‰ï¼‰
    -   `span_data`ï¼Œå…³äº Span çš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œ`AgentSpanData` åŒ…å«å…³äºæ™ºèƒ½ä½“çš„ä¿¡æ¯ï¼Œ`GenerationSpanData` åŒ…å«å…³äº LLM ç”Ÿæˆçš„ä¿¡æ¯ï¼Œç­‰ç­‰ã€‚

## é»˜è®¤è¿½è¸ª

é»˜è®¤æƒ…å†µä¸‹ï¼ŒSDK ä¼šè¿½è¸ªä»¥ä¸‹å†…å®¹ï¼š

-   æ•´ä¸ª `Runner.{run, run_sync, run_streamed}()` ä¼šè¢« `trace()` åŒ…è£¹ã€‚
-   æ¯æ¬¡æ™ºèƒ½ä½“è¿è¡Œéƒ½ä¼šè¢« `agent_span()` åŒ…è£¹
-   LLM ç”Ÿæˆä¼šè¢« `generation_span()` åŒ…è£¹
-   å·¥å…·è°ƒç”¨ä¼šåˆ†åˆ«è¢« `function_span()` åŒ…è£¹
-   å®‰å…¨é˜²æŠ¤æªæ–½ä¼šè¢« `guardrail_span()` åŒ…è£¹
-   ä»»åŠ¡è½¬ç§»ä¼šè¢« `handoff_span()` åŒ…è£¹
-   éŸ³é¢‘è¾“å…¥ï¼ˆè¯­éŸ³è½¬æ–‡æœ¬ï¼‰ä¼šè¢« `transcription_span()` åŒ…è£¹
-   éŸ³é¢‘è¾“å‡ºï¼ˆæ–‡æœ¬è½¬è¯­éŸ³ï¼‰ä¼šè¢« `speech_span()` åŒ…è£¹
-   ç›¸å…³çš„éŸ³é¢‘ Span å¯èƒ½ä¼šå½’åœ¨ `speech_group_span()` ä¹‹ä¸‹

é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿½è¸ªåç§°ä¸º â€œAgent workflowâ€ã€‚å¦‚æœä½ ä½¿ç”¨ `trace`ï¼Œå¯ä»¥è®¾ç½®è¯¥åç§°ï¼›æˆ–è€…ä½ å¯ä»¥é€šè¿‡ [`RunConfig`][agents.run.RunConfig] é…ç½®åç§°å’Œå…¶ä»–å±æ€§ã€‚

æ­¤å¤–ï¼Œä½ å¯ä»¥è®¾ç½®[è‡ªå®šä¹‰è¿½è¸ªè¿›ç¨‹](#custom-tracing-processors)ï¼Œå°†è¿½è¸ªæ¨é€åˆ°å…¶ä»–ç›®çš„åœ°ï¼ˆä½œä¸ºæ›¿ä»£æˆ–æ¬¡è¦ç›®çš„åœ°ï¼‰ã€‚

## æ›´é«˜å±‚çº§çš„è¿½è¸ª

æœ‰æ—¶ï¼Œä½ å¯èƒ½å¸Œæœ›å¤šæ¬¡è°ƒç”¨ `run()` å±äºåŒä¸€ä¸ªè¿½è¸ªã€‚ä½ å¯ä»¥é€šè¿‡å°†æ•´ä¸ªä»£ç åŒ…è£¹åœ¨ `trace()` ä¸­æ¥å®ç°ã€‚

```python
from agents import Agent, Runner, trace

async def main():
    agent = Agent(name="Joke generator", instructions="Tell funny jokes.")

    with trace("Joke workflow"): # (1)!
        first_result = await Runner.run(agent, "Tell me a joke")
        second_result = await Runner.run(agent, f"Rate this joke: {first_result.final_output}")
        print(f"Joke: {first_result.final_output}")
        print(f"Rating: {second_result.final_output}")
```

1. å› ä¸ºä¸¤æ¬¡å¯¹ `Runner.run` çš„è°ƒç”¨éƒ½åŒ…è£¹åœ¨ `with trace()` ä¸­ï¼Œå•æ¬¡è¿è¡Œå°†å½’å…¥åŒä¸€ä¸ªæ•´ä½“è¿½è¸ªï¼Œè€Œä¸ä¼šåˆ›å»ºä¸¤ä¸ªè¿½è¸ªã€‚

## åˆ›å»ºè¿½è¸ª

ä½ å¯ä»¥ä½¿ç”¨ [`trace()`][agents.tracing.trace] å‡½æ•°æ¥åˆ›å»ºä¸€ä¸ªè¿½è¸ªã€‚è¿½è¸ªéœ€è¦æ˜¾å¼å¼€å§‹å’Œç»“æŸã€‚ä½ æœ‰ä¸¤ç§æ–¹å¼ï¼š

1. æ¨èï¼šå°† trace ä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ä½¿ç”¨ï¼Œå³ `with trace(...) as my_trace`ã€‚è¿™ä¼šåœ¨åˆé€‚çš„æ—¶é—´è‡ªåŠ¨å¼€å§‹å’Œç»“æŸè¿½è¸ªã€‚
2. ä¹Ÿå¯ä»¥æ‰‹åŠ¨è°ƒç”¨ [`trace.start()`][agents.tracing.Trace.start] å’Œ [`trace.finish()`][agents.tracing.Trace.finish]ã€‚

å½“å‰è¿½è¸ªé€šè¿‡ Python çš„ [`contextvar`](https://docs.python.org/3/library/contextvars.html) è¿›è¡Œè·Ÿè¸ªã€‚è¿™æ„å‘³ç€å®ƒèƒ½è‡ªåŠ¨é€‚é…å¹¶å‘åœºæ™¯ã€‚å¦‚æœä½ æ‰‹åŠ¨å¼€å§‹/ç»“æŸè¿½è¸ªï¼Œéœ€è¦åœ¨ `start()`/`finish()` ä¸­ä¼ å…¥ `mark_as_current` å’Œ `reset_current` æ¥æ›´æ–°å½“å‰è¿½è¸ªã€‚

## åˆ›å»º Span

ä½ å¯ä»¥ä½¿ç”¨å„ç§ [`*_span()`][agents.tracing.create] æ–¹æ³•æ¥åˆ›å»ºä¸€ä¸ª Spanã€‚ä¸€èˆ¬æ¥è¯´ï¼Œä½ ä¸éœ€è¦æ‰‹åŠ¨åˆ›å»º Spanã€‚æä¾›äº†ä¸€ä¸ª [`custom_span()`][agents.tracing.custom_span] å‡½æ•°ç”¨äºè¿½è¸ªè‡ªå®šä¹‰ Span ä¿¡æ¯ã€‚

Span ä¼šè‡ªåŠ¨å½’å…¥å½“å‰è¿½è¸ªï¼Œå¹¶åµŒå¥—åœ¨æœ€è¿‘çš„å½“å‰ Span ä¹‹ä¸‹ï¼Œè¯¥çŠ¶æ€é€šè¿‡ Python çš„ [`contextvar`](https://docs.python.org/3/library/contextvars.html) è¿›è¡Œè·Ÿè¸ªã€‚

## æ•æ„Ÿæ•°æ®

æŸäº› Span å¯èƒ½ä¼šæ•è·æ½œåœ¨æ•æ„Ÿæ•°æ®ã€‚

`generation_span()` ä¼šå­˜å‚¨ LLM ç”Ÿæˆçš„è¾“å…¥/è¾“å‡ºï¼Œè€Œ `function_span()` ä¼šå­˜å‚¨å‡½æ•°è°ƒç”¨çš„è¾“å…¥/è¾“å‡ºã€‚è¿™äº›å¯èƒ½åŒ…å«æ•æ„Ÿæ•°æ®ï¼Œå› æ­¤ä½ å¯ä»¥é€šè¿‡ [`RunConfig.trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data] ç¦ç”¨å¯¹æ­¤ç±»æ•°æ®çš„é‡‡é›†ã€‚

åŒæ ·ï¼ŒéŸ³é¢‘ç±» Span é»˜è®¤ä¼šåŒ…å«è¾“å…¥ä¸è¾“å‡ºéŸ³é¢‘çš„ base64 ç¼–ç  PCM æ•°æ®ã€‚ä½ å¯ä»¥é€šè¿‡é…ç½® [`VoicePipelineConfig.trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data] æ¥ç¦ç”¨è¿™äº›éŸ³é¢‘æ•°æ®çš„é‡‡é›†ã€‚

## è‡ªå®šä¹‰è¿½è¸ªè¿›ç¨‹

è¿½è¸ªçš„é«˜å±‚æ¶æ„å¦‚ä¸‹ï¼š

-   åœ¨åˆå§‹åŒ–æ—¶ï¼Œæˆ‘ä»¬ä¼šåˆ›å»ºå…¨å±€çš„ [`TraceProvider`][agents.tracing.setup.TraceProvider]ï¼Œç”¨äºåˆ›å»ºè¿½è¸ªã€‚
-   æˆ‘ä»¬å°† `TraceProvider` é…ç½®ä¸ºä½¿ç”¨ [`BatchTraceProcessor`][agents.tracing.processors.BatchTraceProcessor] å°†è¿½è¸ª/Span æ‰¹é‡å‘é€åˆ° [`BackendSpanExporter`][agents.tracing.processors.BackendSpanExporter]ï¼Œåè€…ä¼šå°† Span å’Œè¿½è¸ªæ‰¹é‡å¯¼å‡ºåˆ° OpenAI çš„åç«¯ã€‚

è‹¥è¦è‡ªå®šä¹‰æ­¤é»˜è®¤è®¾ç½®ã€å°†è¿½è¸ªå‘é€è‡³æ›¿ä»£æˆ–é™„åŠ çš„åç«¯ï¼Œæˆ–ä¿®æ”¹å¯¼å‡ºå™¨è¡Œä¸ºï¼Œä½ æœ‰ä¸¤ç§é€‰æ‹©ï¼š

1. [`add_trace_processor()`][agents.tracing.add_trace_processor] å…è®¸ä½ æ·»åŠ ä¸€ä¸ªâ€œé¢å¤–çš„â€è¿½è¸ªè¿›ç¨‹ï¼Œå®ƒä¼šåœ¨è¿½è¸ªå’Œ Span å°±ç»ªæ—¶æ¥æ”¶å®ƒä»¬ã€‚è¿™æ ·ä½ å¯ä»¥åœ¨å°†è¿½è¸ªå‘é€åˆ° OpenAI åç«¯ä¹‹å¤–æ‰§è¡Œè‡ªå·±çš„å¤„ç†ã€‚
2. [`set_trace_processors()`][agents.tracing.set_trace_processors] å…è®¸ä½ â€œæ›¿æ¢â€é»˜è®¤è¿›ç¨‹ä¸ºä½ è‡ªå·±çš„è¿½è¸ªè¿›ç¨‹ã€‚è¿™æ„å‘³ç€é™¤éä½ åŒ…å«ä¼šæ‰§è¡Œè¯¥æ“ä½œçš„ `TracingProcessor`ï¼Œå¦åˆ™è¿½è¸ªå°†ä¸ä¼šå‘é€åˆ° OpenAI åç«¯ã€‚

## ä¸é OpenAI æ¨¡å‹çš„è¿½è¸ª

ä½ å¯ä»¥ä½¿ç”¨ OpenAI API key æ­é…é OpenAI æ¨¡å‹ï¼Œåœ¨æ— éœ€ç¦ç”¨è¿½è¸ªçš„æƒ…å†µä¸‹ï¼Œåœ¨ OpenAI Traces ä»ªè¡¨æ¿ä¸Šå¯ç”¨å…è´¹çš„è¿½è¸ªã€‚

```python
import os
from agents import set_tracing_export_api_key, Agent, Runner
from agents.extensions.models.litellm_model import LitellmModel

tracing_api_key = os.environ["OPENAI_API_KEY"]
set_tracing_export_api_key(tracing_api_key)

model = LitellmModel(
    model="your-model-name",
    api_key="your-api-key",
)

agent = Agent(
    name="Assistant",
    model=model,
)
```

## å¤‡æ³¨
- åœ¨ OpenAI Traces ä»ªè¡¨æ¿æŸ¥çœ‹å…è´¹è¿½è¸ªã€‚

## å¤–éƒ¨è¿½è¸ªè¿›ç¨‹åˆ—è¡¨

-   [Weights & Biases](https://weave-docs.wandb.ai/guides/integrations/openai_agents)
-   [Arize-Phoenix](https://docs.arize.com/phoenix/tracing/integrations-tracing/openai-agents-sdk)
-   [Future AGI](https://docs.futureagi.com/future-agi/products/observability/auto-instrumentation/openai_agents)
-   [MLflow (self-hosted/OSS)](https://mlflow.org/docs/latest/tracing/integrations/openai-agent)
-   [MLflow (Databricks hosted)](https://docs.databricks.com/aws/en/mlflow/mlflow-tracing#-automatic-tracing)
-   [Braintrust](https://braintrust.dev/docs/guides/traces/integrations#openai-agents-sdk)
-   [Pydantic Logfire](https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents)
-   [AgentOps](https://docs.agentops.ai/v1/integrations/agentssdk)
-   [Scorecard](https://docs.scorecard.io/docs/documentation/features/tracing#openai-agents-sdk-integration)
-   [Keywords AI](https://docs.keywordsai.co/integration/development-frameworks/openai-agent)
-   [LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
-   [Maxim AI](https://www.getmaxim.ai/docs/observe/integrations/openai-agents-sdk)
-   [Comet Opik](https://www.comet.com/docs/opik/tracing/integrations/openai_agents)
-   [Langfuse](https://langfuse.com/docs/integrations/openaiagentssdk/openai-agents)
-   [Langtrace](https://docs.langtrace.ai/supported-integrations/llm-frameworks/openai-agents-sdk)
-   [Okahu-Monocle](https://github.com/monocle2ai/monocle)
-   [Galileo](https://v2docs.galileo.ai/integrations/openai-agent-integration#openai-agent-integration)
-   [Portkey AI](https://portkey.ai/docs/integrations/agents/openai-agents)
-   [LangDB AI](https://docs.langdb.ai/getting-started/working-with-agent-frameworks/working-with-openai-agents-sdk)
-   [Agenta](https://docs.agenta.ai/observability/integrations/openai-agents)

================
File: docs/zh/usage.md
================
---
search:
  exclude: true
---
# ä½¿ç”¨é‡

Agents SDK ä¼šè‡ªåŠ¨ä¸ºæ¯æ¬¡è¿è¡Œè·Ÿè¸ªä»¤ç‰Œä½¿ç”¨é‡ã€‚ä½ å¯ä»¥ä»è¿è¡Œä¸Šä¸‹æ–‡ä¸­è·å–å®ƒï¼Œç”¨äºç›‘æ§æˆæœ¬ã€å®æ–½é™åˆ¶æˆ–è®°å½•åˆ†ææ•°æ®ã€‚

## è·Ÿè¸ªå†…å®¹

- **requests**: å‘èµ·çš„ LLM API è°ƒç”¨æ¬¡æ•°
- **input_tokens**: å‘é€çš„è¾“å…¥ä»¤ç‰Œæ€»æ•°
- **output_tokens**: æ¥æ”¶çš„è¾“å‡ºä»¤ç‰Œæ€»æ•°
- **total_tokens**: è¾“å…¥ + è¾“å‡º
- **details**:
  - `input_tokens_details.cached_tokens`
  - `output_tokens_details.reasoning_tokens`

## ä»ä¸€æ¬¡è¿è¡Œä¸­è·å–ä½¿ç”¨é‡

åœ¨æ‰§è¡Œ `Runner.run(...)` ä¹‹åï¼Œé€šè¿‡ `result.context_wrapper.usage` è®¿é—®ä½¿ç”¨é‡ã€‚

```python
result = await Runner.run(agent, "What's the weather in Tokyo?")
usage = result.context_wrapper.usage

print("Requests:", usage.requests)
print("Input tokens:", usage.input_tokens)
print("Output tokens:", usage.output_tokens)
print("Total tokens:", usage.total_tokens)
```

ä½¿ç”¨é‡ä¼šåœ¨è¯¥æ¬¡è¿è¡Œä¸­çš„æ‰€æœ‰æ¨¡å‹è°ƒç”¨ä¹‹é—´èšåˆï¼ˆåŒ…æ‹¬å·¥å…·è°ƒç”¨å’Œä»»åŠ¡è½¬ç§»ï¼‰ã€‚

### åœ¨ LiteLLM æ¨¡å‹ä¸­å¯ç”¨ä½¿ç”¨é‡

LiteLLM æä¾›æ–¹é»˜è®¤ä¸æŠ¥å‘Šä½¿ç”¨æŒ‡æ ‡ã€‚å½“ä½ ä½¿ç”¨ [`LitellmModel`](models/litellm.md) æ—¶ï¼Œå°† `ModelSettings(include_usage=True)` ä¼ ç»™ä½ çš„æ™ºèƒ½ä½“ï¼Œä»¥ä¾¿ LiteLLM çš„å“åº”å¡«å…… `result.context_wrapper.usage`ã€‚

```python
from agents import Agent, ModelSettings, Runner
from agents.extensions.models.litellm_model import LitellmModel

agent = Agent(
    name="Assistant",
    model=LitellmModel(model="your/model", api_key="..."),
    model_settings=ModelSettings(include_usage=True),
)

result = await Runner.run(agent, "What's the weather in Tokyo?")
print(result.context_wrapper.usage.total_tokens)
```

## ä½¿ç”¨ä¼šè¯è·å–ä½¿ç”¨é‡

å½“ä½ ä½¿ç”¨ `Session`ï¼ˆä¾‹å¦‚ `SQLiteSession`ï¼‰æ—¶ï¼Œæ¯æ¬¡è°ƒç”¨ `Runner.run(...)` éƒ½ä¼šè¿”å›è¯¥æ¬¡è¿è¡Œçš„ä½¿ç”¨é‡ã€‚ä¼šè¯ä¼šç»´æŠ¤å¯¹è¯å†å²ç”¨äºä¸Šä¸‹æ–‡ï¼Œä½†æ¯æ¬¡è¿è¡Œçš„ä½¿ç”¨é‡å½¼æ­¤ç‹¬ç«‹ã€‚

```python
session = SQLiteSession("my_conversation")

first = await Runner.run(agent, "Hi!", session=session)
print(first.context_wrapper.usage.total_tokens)  # Usage for first run

second = await Runner.run(agent, "Can you elaborate?", session=session)
print(second.context_wrapper.usage.total_tokens)  # Usage for second run
```

è¯·æ³¨æ„ï¼Œå°½ç®¡ä¼šè¯ä¼šåœ¨è¿è¡Œä¹‹é—´ä¿ç•™å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œä½†æ¯æ¬¡ `Runner.run()` è¿”å›çš„ä½¿ç”¨é‡ä»…ä»£è¡¨è¯¥æ¬¡æ‰§è¡Œã€‚åœ¨ä¼šè¯ä¸­ï¼Œå…ˆå‰æ¶ˆæ¯å¯èƒ½ä¼šä½œä¸ºè¾“å…¥é‡æ–°æä¾›ç»™æ¯æ¬¡è¿è¡Œï¼Œä»è€Œå½±å“åç»­è½®æ¬¡çš„è¾“å…¥ä»¤ç‰Œè®¡æ•°ã€‚

## åœ¨é’©å­ä¸­ä½¿ç”¨ä½¿ç”¨é‡

å¦‚æœä½ åœ¨ä½¿ç”¨ `RunHooks`ï¼Œä¼ é€’ç»™æ¯ä¸ªé’©å­çš„ `context` å¯¹è±¡åŒ…å« `usage`ã€‚è¿™ä½¿ä½ å¯ä»¥åœ¨å…³é”®ç”Ÿå‘½å‘¨æœŸæ—¶åˆ»è®°å½•ä½¿ç”¨é‡ã€‚

```python
class MyHooks(RunHooks):
    async def on_agent_end(self, context: RunContextWrapper, agent: Agent, output: Any) -> None:
        u = context.usage
        print(f"{agent.name} â†’ {u.requests} requests, {u.total_tokens} total tokens")
```

## API å‚è€ƒ

æ¬²äº†è§£è¯¦ç»†çš„ API æ–‡æ¡£ï¼Œè¯·å‚è§ï¼š

- [`Usage`][agents.usage.Usage] - ä½¿ç”¨é‡è¿½è¸ªæ•°æ®ç»“æ„
- [`RunContextWrapper`][agents.run.RunContextWrapper] - ä»è¿è¡Œä¸Šä¸‹æ–‡è®¿é—®ä½¿ç”¨é‡
- [`RunHooks`][agents.run.RunHooks] - é’©å…¥ä½¿ç”¨é‡è¿½è¸ªç”Ÿå‘½å‘¨æœŸ

================
File: docs/zh/visualization.md
================
---
search:
  exclude: true
---
# æ™ºèƒ½ä½“å¯è§†åŒ–

æ™ºèƒ½ä½“å¯è§†åŒ–å…è®¸ä½ ä½¿ç”¨ **Graphviz** ç”Ÿæˆæ™ºèƒ½ä½“åŠå…¶å…³ç³»çš„ç»“æ„åŒ–å›¾å½¢è¡¨ç¤ºã€‚è¿™æœ‰åŠ©äºç†è§£æ™ºèƒ½ä½“ã€å·¥å…·å’Œä»»åŠ¡è½¬ç§»åœ¨åº”ç”¨ä¸­çš„äº¤äº’æ–¹å¼ã€‚

## å®‰è£…

å®‰è£…å¯é€‰çš„ `viz` ä¾èµ–ç»„ï¼š

```bash
pip install "openai-agents[viz]"
```

## ç”Ÿæˆå›¾

ä½ å¯ä»¥ä½¿ç”¨ `draw_graph` å‡½æ•°ç”Ÿæˆæ™ºèƒ½ä½“å¯è§†åŒ–ã€‚è¯¥å‡½æ•°ä¼šåˆ›å»ºä¸€ä¸ªæœ‰å‘å›¾ï¼Œå…¶ä¸­ï¼š

- **æ™ºèƒ½ä½“** ç”¨é»„è‰²æ–¹æ¡†è¡¨ç¤ºã€‚
- **MCP æœåŠ¡** ç”¨ç°è‰²æ–¹æ¡†è¡¨ç¤ºã€‚
- **å·¥å…·** ç”¨ç»¿è‰²æ¤­åœ†è¡¨ç¤ºã€‚
- **ä»»åŠ¡è½¬ç§»** ç”¨ä»ä¸€ä¸ªæ™ºèƒ½ä½“æŒ‡å‘å¦ä¸€ä¸ªæ™ºèƒ½ä½“çš„æœ‰å‘è¾¹è¡¨ç¤ºã€‚

### ç¤ºä¾‹ç”¨æ³•

```python
import os

from agents import Agent, function_tool
from agents.mcp.server import MCPServerStdio
from agents.extensions.visualization import draw_graph

@function_tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
)

current_dir = os.path.dirname(os.path.abspath(__file__))
samples_dir = os.path.join(current_dir, "sample_files")
mcp_server = MCPServerStdio(
    name="Filesystem Server, via npx",
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", samples_dir],
    },
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    tools=[get_weather],
    mcp_servers=[mcp_server],
)

draw_graph(triage_agent)
```

![Agent Graph](../assets/images/graph.png)

è¿™å°†ç”Ÿæˆä¸€ä¸ªå›¾ï¼Œç›´è§‚å±•ç¤º **åˆ†è¯Šæ™ºèƒ½ä½“** çš„ç»“æ„åŠå…¶ä¸å­æ™ºèƒ½ä½“å’Œå·¥å…·çš„è¿æ¥å…³ç³»ã€‚


## å¯è§†åŒ–è¯´æ˜

ç”Ÿæˆçš„å›¾åŒ…å«ï¼š

- ä¸€ä¸ªè¡¨ç¤ºå…¥å£ç‚¹çš„ **èµ·å§‹èŠ‚ç‚¹**ï¼ˆ`__start__`ï¼‰ã€‚
- ç”¨å¸¦é»„è‰²å¡«å……çš„ **çŸ©å½¢** è¡¨ç¤ºçš„æ™ºèƒ½ä½“ã€‚
- ç”¨å¸¦ç»¿è‰²å¡«å……çš„ **æ¤­åœ†** è¡¨ç¤ºçš„å·¥å…·ã€‚
- ç”¨å¸¦ç°è‰²å¡«å……çš„ **çŸ©å½¢** è¡¨ç¤ºçš„ MCP æœåŠ¡ã€‚
- è¡¨ç¤ºäº¤äº’çš„æœ‰å‘è¾¹ï¼š
  - **å®çº¿ç®­å¤´** è¡¨ç¤ºæ™ºèƒ½ä½“åˆ°æ™ºèƒ½ä½“çš„ä»»åŠ¡è½¬ç§»ã€‚
  - **ç‚¹çº¿ç®­å¤´** è¡¨ç¤ºå·¥å…·è°ƒç”¨ã€‚
  - **è™šçº¿ç®­å¤´** è¡¨ç¤º MCP æœåŠ¡è°ƒç”¨ã€‚
- ä¸€ä¸ªè¡¨ç¤ºæ‰§è¡Œç»ˆæ­¢ä½ç½®çš„ **ç»“æŸèŠ‚ç‚¹**ï¼ˆ`__end__`ï¼‰ã€‚

**æ³¨æ„ï¼š** åœ¨è¾ƒæ–°çš„ `agents` åŒ…ç‰ˆæœ¬ï¼ˆå·²åœ¨ **v0.2.8** éªŒè¯ï¼‰ä¸­ä¼šæ¸²æŸ“ MCP æœåŠ¡ã€‚å¦‚æœåœ¨ä½ çš„å¯è§†åŒ–ä¸­æœªçœ‹åˆ° MCP æ–¹æ¡†ï¼Œè¯·å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬ã€‚

## è‡ªå®šä¹‰å›¾

### æ˜¾ç¤ºå›¾å½¢
é»˜è®¤æƒ…å†µä¸‹ï¼Œ`draw_graph` ä¼šè¡Œå†…æ˜¾ç¤ºå›¾å½¢ã€‚è¦åœ¨å•ç‹¬çª—å£ä¸­æ˜¾ç¤ºï¼Œè¯·ç¼–å†™å¦‚ä¸‹ä»£ç ï¼š

```python
draw_graph(triage_agent).view()
```

### ä¿å­˜å›¾å½¢
é»˜è®¤æƒ…å†µä¸‹ï¼Œ`draw_graph` ä¼šè¡Œå†…æ˜¾ç¤ºå›¾å½¢ã€‚è¦å°†å…¶ä¿å­˜ä¸ºæ–‡ä»¶ï¼Œè¯·æŒ‡å®šæ–‡ä»¶åï¼š

```python
draw_graph(triage_agent, filename="agent_graph")
```

è¿™å°†åœ¨å·¥ä½œç›®å½•ä¸­ç”Ÿæˆ `agent_graph.png`ã€‚

================
File: docs/agents.md
================
# Agents

Agents are the core building block in your apps. An agent is a large language model (LLM), configured with instructions and tools.

## Basic configuration

The most common properties of an agent you'll configure are:

-   `name`: A required string that identifies your agent.
-   `instructions`: also known as a developer message or system prompt.
-   `model`: which LLM to use, and optional `model_settings` to configure model tuning parameters like temperature, top_p, etc.
-   `tools`: Tools that the agent can use to achieve its tasks.

```python
from agents import Agent, ModelSettings, function_tool

@function_tool
def get_weather(city: str) -> str:
    """returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Haiku agent",
    instructions="Always respond in haiku form",
    model="gpt-5-nano",
    tools=[get_weather],
)
```

## Context

Agents are generic on their `context` type. Context is a dependency-injection tool: it's an object you create and pass to `Runner.run()`, that is passed to every agent, tool, handoff etc, and it serves as a grab bag of dependencies and state for the agent run. You can provide any Python object as the context.

```python
@dataclass
class UserContext:
    name: str
    uid: str
    is_pro_user: bool

    async def fetch_purchases() -> list[Purchase]:
        return ...

agent = Agent[UserContext](
    ...,
)
```

## Output types

By default, agents produce plain text (i.e. `str`) outputs. If you want the agent to produce a particular type of output, you can use the `output_type` parameter. A common choice is to use [Pydantic](https://docs.pydantic.dev/) objects, but we support any type that can be wrapped in a Pydantic [TypeAdapter](https://docs.pydantic.dev/latest/api/type_adapter/) - dataclasses, lists, TypedDict, etc.

```python
from pydantic import BaseModel
from agents import Agent


class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

agent = Agent(
    name="Calendar extractor",
    instructions="Extract calendar events from text",
    output_type=CalendarEvent,
)
```

!!! note

    When you pass an `output_type`, that tells the model to use [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) instead of regular plain text responses.

## Multi-agent system design patterns

There are many ways to design multiâ€‘agent systems, but we commonly see two broadly applicable patterns:

1. Manager (agents as tools): A central manager/orchestrator invokes specialized subâ€‘agents as tools and retains control of the conversation.
2. Handoffs: Peer agents hand off control to a specialized agent that takes over the conversation. This is decentralized.

See [our practical guide to building agents](https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf) for more details.

### Manager (agents as tools)

The `customer_facing_agent` handles all user interaction and invokes specialized subâ€‘agents exposed as tools. Read more in the [tools](tools.md#agents-as-tools) documentation.

```python
from agents import Agent

booking_agent = Agent(...)
refund_agent = Agent(...)

customer_facing_agent = Agent(
    name="Customer-facing agent",
    instructions=(
        "Handle all direct user communication. "
        "Call the relevant tools when specialized expertise is needed."
    ),
    tools=[
        booking_agent.as_tool(
            tool_name="booking_expert",
            tool_description="Handles booking questions and requests.",
        ),
        refund_agent.as_tool(
            tool_name="refund_expert",
            tool_description="Handles refund questions and requests.",
        )
    ],
)
```

### Handoffs

Handoffs are subâ€‘agents the agent can delegate to. When a handoff occurs, the delegated agent receives the conversation history and takes over the conversation. This pattern enables modular, specialized agents that excel at a single task. Read more in the [handoffs](handoffs.md) documentation.

```python
from agents import Agent

booking_agent = Agent(...)
refund_agent = Agent(...)

triage_agent = Agent(
    name="Triage agent",
    instructions=(
        "Help the user with their questions. "
        "If they ask about booking, hand off to the booking agent. "
        "If they ask about refunds, hand off to the refund agent."
    ),
    handoffs=[booking_agent, refund_agent],
)
```

## Dynamic instructions

In most cases, you can provide instructions when you create the agent. However, you can also provide dynamic instructions via a function. The function will receive the agent and context, and must return the prompt. Both regular and `async` functions are accepted.

```python
def dynamic_instructions(
    context: RunContextWrapper[UserContext], agent: Agent[UserContext]
) -> str:
    return f"The user's name is {context.context.name}. Help them with their questions."


agent = Agent[UserContext](
    name="Triage agent",
    instructions=dynamic_instructions,
)
```

## Lifecycle events (hooks)

Sometimes, you want to observe the lifecycle of an agent. For example, you may want to log events, or pre-fetch data when certain events occur. You can hook into the agent lifecycle with the `hooks` property. Subclass the [`AgentHooks`][agents.lifecycle.AgentHooks] class, and override the methods you're interested in.

## Guardrails

Guardrails allow you to run checks/validations on user input in parallel to the agent running, and on the agent's output once it is produced. For example, you could screen the user's input and agent's output for relevance. Read more in the [guardrails](guardrails.md) documentation.

## Cloning/copying agents

By using the `clone()` method on an agent, you can duplicate an Agent, and optionally change any properties you like.

```python
pirate_agent = Agent(
    name="Pirate",
    instructions="Write like a pirate",
    model="gpt-4.1",
)

robot_agent = pirate_agent.clone(
    name="Robot",
    instructions="Write like a robot",
)
```

## Forcing tool use

Supplying a list of tools doesn't always mean the LLM will use a tool. You can force tool use by setting [`ModelSettings.tool_choice`][agents.model_settings.ModelSettings.tool_choice]. Valid values are:

1. `auto`, which allows the LLM to decide whether or not to use a tool.
2. `required`, which requires the LLM to use a tool (but it can intelligently decide which tool).
3. `none`, which requires the LLM to _not_ use a tool.
4. Setting a specific string e.g. `my_tool`, which requires the LLM to use that specific tool.

```python
from agents import Agent, Runner, function_tool, ModelSettings

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Weather Agent",
    instructions="Retrieve weather details.",
    tools=[get_weather],
    model_settings=ModelSettings(tool_choice="get_weather")
)
```

## Tool Use Behavior

The `tool_use_behavior` parameter in the `Agent` configuration controls how tool outputs are handled:

- `"run_llm_again"`: The default. Tools are run, and the LLM processes the results to produce a final response.
- `"stop_on_first_tool"`: The output of the first tool call is used as the final response, without further LLM processing.

```python
from agents import Agent, Runner, function_tool, ModelSettings

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Weather Agent",
    instructions="Retrieve weather details.",
    tools=[get_weather],
    tool_use_behavior="stop_on_first_tool"
)
```

- `StopAtTools(stop_at_tool_names=[...])`: Stops if any specified tool is called, using its output as the final response.

```python
from agents import Agent, Runner, function_tool
from agents.agent import StopAtTools

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

@function_tool
def sum_numbers(a: int, b: int) -> int:
    """Adds two numbers."""
    return a + b

agent = Agent(
    name="Stop At Stock Agent",
    instructions="Get weather or sum numbers.",
    tools=[get_weather, sum_numbers],
    tool_use_behavior=StopAtTools(stop_at_tool_names=["get_weather"])
)
```

- `ToolsToFinalOutputFunction`: A custom function that processes tool results and decides whether to stop or continue with the LLM.

```python
from agents import Agent, Runner, function_tool, FunctionToolResult, RunContextWrapper
from agents.agent import ToolsToFinalOutputResult
from typing import List, Any

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

def custom_tool_handler(
    context: RunContextWrapper[Any],
    tool_results: List[FunctionToolResult]
) -> ToolsToFinalOutputResult:
    """Processes tool results to decide final output."""
    for result in tool_results:
        if result.output and "sunny" in result.output:
            return ToolsToFinalOutputResult(
                is_final_output=True,
                final_output=f"Final weather: {result.output}"
            )
    return ToolsToFinalOutputResult(
        is_final_output=False,
        final_output=None
    )

agent = Agent(
    name="Weather Agent",
    instructions="Retrieve weather details.",
    tools=[get_weather],
    tool_use_behavior=custom_tool_handler
)
```

!!! note

    To prevent infinite loops, the framework automatically resets `tool_choice` to "auto" after a tool call. This behavior is configurable via [`agent.reset_tool_choice`][agents.agent.Agent.reset_tool_choice]. The infinite loop is because tool results are sent to the LLM, which then generates another tool call because of `tool_choice`, ad infinitum.

================
File: docs/config.md
================
# Configuring the SDK

## API keys and clients

By default, the SDK looks for the `OPENAI_API_KEY` environment variable for LLM requests and tracing, as soon as it is imported. If you are unable to set that environment variable before your app starts, you can use the [set_default_openai_key()][agents.set_default_openai_key] function to set the key.

```python
from agents import set_default_openai_key

set_default_openai_key("sk-...")
```

Alternatively, you can also configure an OpenAI client to be used. By default, the SDK creates an `AsyncOpenAI` instance, using the API key from the environment variable or the default key set above. You can change this by using the [set_default_openai_client()][agents.set_default_openai_client] function.

```python
from openai import AsyncOpenAI
from agents import set_default_openai_client

custom_client = AsyncOpenAI(base_url="...", api_key="...")
set_default_openai_client(custom_client)
```

Finally, you can also customize the OpenAI API that is used. By default, we use the OpenAI Responses API. You can override this to use the Chat Completions API by using the [set_default_openai_api()][agents.set_default_openai_api] function.

```python
from agents import set_default_openai_api

set_default_openai_api("chat_completions")
```

## Tracing

Tracing is enabled by default. It uses the OpenAI API keys from the section above by default (i.e. the environment variable or the default key you set). You can specifically set the API key used for tracing by using the [`set_tracing_export_api_key`][agents.set_tracing_export_api_key] function.

```python
from agents import set_tracing_export_api_key

set_tracing_export_api_key("sk-...")
```

You can also disable tracing entirely by using the [`set_tracing_disabled()`][agents.set_tracing_disabled] function.

```python
from agents import set_tracing_disabled

set_tracing_disabled(True)
```

## Debug logging

The SDK has two Python loggers without any handlers set. By default, this means that warnings and errors are sent to `stdout`, but other logs are suppressed.

To enable verbose logging, use the [`enable_verbose_stdout_logging()`][agents.enable_verbose_stdout_logging] function.

```python
from agents import enable_verbose_stdout_logging

enable_verbose_stdout_logging()
```

Alternatively, you can customize the logs by adding handlers, filters, formatters, etc. You can read more in the [Python logging guide](https://docs.python.org/3/howto/logging.html).

```python
import logging

logger = logging.getLogger("openai.agents") # or openai.agents.tracing for the Tracing logger

# To make all logs show up
logger.setLevel(logging.DEBUG)
# To make info and above show up
logger.setLevel(logging.INFO)
# To make warning and above show up
logger.setLevel(logging.WARNING)
# etc

# You can customize this as needed, but this will output to `stderr` by default
logger.addHandler(logging.StreamHandler())
```

### Sensitive data in logs

Certain logs may contain sensitive data (for example, user data). If you want to disable this data from being logged, set the following environment variables.

To disable logging LLM inputs and outputs:

```bash
export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1
```

To disable logging tool inputs and outputs:

```bash
export OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1
```

================
File: docs/context.md
================
# Context management

Context is an overloaded term. There are two main classes of context you might care about:

1. Context available locally to your code: this is data and dependencies you might need when tool functions run, during callbacks like `on_handoff`, in lifecycle hooks, etc.
2. Context available to LLMs: this is data the LLM sees when generating a response.

## Local context

This is represented via the [`RunContextWrapper`][agents.run_context.RunContextWrapper] class and the [`context`][agents.run_context.RunContextWrapper.context] property within it. The way this works is:

1. You create any Python object you want. A common pattern is to use a dataclass or a Pydantic object.
2. You pass that object to the various run methods (e.g. `Runner.run(..., **context=whatever**))`.
3. All your tool calls, lifecycle hooks etc will be passed a wrapper object, `RunContextWrapper[T]`, where `T` represents your context object type which you can access via `wrapper.context`.

The **most important** thing to be aware of: every agent, tool function, lifecycle etc for a given agent run must use the same _type_ of context.

You can use the context for things like:

-   Contextual data for your run (e.g. things like a username/uid or other information about the user)
-   Dependencies (e.g. logger objects, data fetchers, etc)
-   Helper functions

!!! danger "Note"

    The context object is **not** sent to the LLM. It is purely a local object that you can read from, write to and call methods on it.

```python
import asyncio
from dataclasses import dataclass

from agents import Agent, RunContextWrapper, Runner, function_tool

@dataclass
class UserInfo:  # (1)!
    name: str
    uid: int

@function_tool
async def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str:  # (2)!
    """Fetch the age of the user. Call this function to get user's age information."""
    return f"The user {wrapper.context.name} is 47 years old"

async def main():
    user_info = UserInfo(name="John", uid=123)

    agent = Agent[UserInfo](  # (3)!
        name="Assistant",
        tools=[fetch_user_age],
    )

    result = await Runner.run(  # (4)!
        starting_agent=agent,
        input="What is the age of the user?",
        context=user_info,
    )

    print(result.final_output)  # (5)!
    # The user John is 47 years old.

if __name__ == "__main__":
    asyncio.run(main())
```

1. This is the context object. We've used a dataclass here, but you can use any type.
2. This is a tool. You can see it takes a `RunContextWrapper[UserInfo]`. The tool implementation reads from the context.
3. We mark the agent with the generic `UserInfo`, so that the typechecker can catch errors (for example, if we tried to pass a tool that took a different context type).
4. The context is passed to the `run` function.
5. The agent correctly calls the tool and gets the age.

---

### Advanced: `ToolContext`

In some cases, you might want to access extra metadata about the tool being executed â€” such as its name, call ID, or raw argument string.  
For this, you can use the [`ToolContext`][agents.tool_context.ToolContext] class, which extends `RunContextWrapper`.

```python
from typing import Annotated
from pydantic import BaseModel, Field
from agents import Agent, Runner, function_tool
from agents.tool_context import ToolContext

class WeatherContext(BaseModel):
    user_id: str

class Weather(BaseModel):
    city: str = Field(description="The city name")
    temperature_range: str = Field(description="The temperature range in Celsius")
    conditions: str = Field(description="The weather conditions")

@function_tool
def get_weather(ctx: ToolContext[WeatherContext], city: Annotated[str, "The city to get the weather for"]) -> Weather:
    print(f"[debug] Tool context: (name: {ctx.tool_name}, call_id: {ctx.tool_call_id}, args: {ctx.tool_arguments})")
    return Weather(city=city, temperature_range="14-20C", conditions="Sunny with wind.")

agent = Agent(
    name="Weather Agent",
    instructions="You are a helpful agent that can tell the weather of a given city.",
    tools=[get_weather],
)
```

`ToolContext` provides the same `.context` property as `RunContextWrapper`,  
plus additional fields specific to the current tool call:

- `tool_name` â€“ the name of the tool being invoked  
- `tool_call_id` â€“ a unique identifier for this tool call  
- `tool_arguments` â€“ the raw argument string passed to the tool  

Use `ToolContext` when you need tool-level metadata during execution.  
For general context sharing between agents and tools, `RunContextWrapper` remains sufficient.

---

## Agent/LLM context

When an LLM is called, the **only** data it can see is from the conversation history. This means that if you want to make some new data available to the LLM, you must do it in a way that makes it available in that history. There are a few ways to do this:

1. You can add it to the Agent `instructions`. This is also known as a "system prompt" or "developer message". System prompts can be static strings, or they can be dynamic functions that receive the context and output a string. This is a common tactic for information that is always useful (for example, the user's name or the current date).
2. Add it to the `input` when calling the `Runner.run` functions. This is similar to the `instructions` tactic, but allows you to have messages that are lower in the [chain of command](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command).
3. Expose it via function tools. This is useful for _on-demand_ context - the LLM decides when it needs some data, and can call the tool to fetch that data.
4. Use retrieval or web search. These are special tools that are able to fetch relevant data from files or databases (retrieval), or from the web (web search). This is useful for "grounding" the response in relevant contextual data.

================
File: docs/examples.md
================
# Examples

Check out a variety of sample implementations of the SDK in the examples section of the [repo](https://github.com/openai/openai-agents-python/tree/main/examples). The examples are organized into several categories that demonstrate different patterns and capabilities.

## Categories

-   **[agent_patterns](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns):**
    Examples in this category illustrate common agent design patterns, such as

    -   Deterministic workflows
    -   Agents as tools
    -   Parallel agent execution
    -   Conditional tool usage
    -   Input/output guardrails
    -   LLM as a judge
    -   Routing
    -   Streaming guardrails

-   **[basic](https://github.com/openai/openai-agents-python/tree/main/examples/basic):**
    These examples showcase foundational capabilities of the SDK, such as

    -   Hello world examples (Default model, GPT-5, open-weight model)
    -   Agent lifecycle management
    -   Dynamic system prompts
    -   Streaming outputs (text, items, function call args)
    -   Prompt templates
    -   File handling (local and remote, images and PDFs)
    -   Usage tracking
    -   Non-strict output types
    -   Previous response ID usage

-   **[customer_service](https://github.com/openai/openai-agents-python/tree/main/examples/customer_service):**
    Example customer service system for an airline.

-   **[financial_research_agent](https://github.com/openai/openai-agents-python/tree/main/examples/financial_research_agent):**
    A financial research agent that demonstrates structured research workflows with agents and tools for financial data analysis.

-   **[handoffs](https://github.com/openai/openai-agents-python/tree/main/examples/handoffs):**
    See practical examples of agent handoffs with message filtering.

-   **[hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp):**
    Examples demonstrating how to use hosted MCP (Model Context Protocol) connectors and approvals.

-   **[mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp):**
    Learn how to build agents with MCP (Model Context Protocol), including:

    -   Filesystem examples
    -   Git examples
    -   MCP prompt server examples
    -   SSE (Server-Sent Events) examples
    -   Streamable HTTP examples

-   **[memory](https://github.com/openai/openai-agents-python/tree/main/examples/memory):**
    Examples of different memory implementations for agents, including:

    -   SQLite session storage
    -   Advanced SQLite session storage
    -   Redis session storage
    -   SQLAlchemy session storage
    -   Encrypted session storage
    -   OpenAI session storage

-   **[model_providers](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers):**
    Explore how to use non-OpenAI models with the SDK, including custom providers and LiteLLM integration.

-   **[realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime):**
    Examples showing how to build real-time experiences using the SDK, including:

    -   Web applications
    -   Command-line interfaces
    -   Twilio integration

-   **[reasoning_content](https://github.com/openai/openai-agents-python/tree/main/examples/reasoning_content):**
    Examples demonstrating how to work with reasoning content and structured outputs.

-   **[research_bot](https://github.com/openai/openai-agents-python/tree/main/examples/research_bot):**
    Simple deep research clone that demonstrates complex multi-agent research workflows.

-   **[tools](https://github.com/openai/openai-agents-python/tree/main/examples/tools):**
    Learn how to implement OAI hosted tools such as:

    -   Web search and web search with filters
    -   File search
    -   Code interpreter
    -   Computer use
    -   Image generation

-   **[voice](https://github.com/openai/openai-agents-python/tree/main/examples/voice):**
    See examples of voice agents, using our TTS and STT models, including streamed voice examples.

================
File: docs/guardrails.md
================
# Guardrails

Guardrails run _in parallel_ to your agents, enabling you to do checks and validations of user input. For example, imagine you have an agent that uses a very smart (and hence slow/expensive) model to help with customer requests. You wouldn't want malicious users to ask the model to help them with their math homework. So, you can run a guardrail with a fast/cheap model. If the guardrail detects malicious usage, it can immediately raise an error, which stops the expensive model from running and saves you time/money.

There are two kinds of guardrails:

1. Input guardrails run on the initial user input
2. Output guardrails run on the final agent output

## Input guardrails

Input guardrails run in 3 steps:

1. First, the guardrail receives the same input passed to the agent.
2. Next, the guardrail function runs to produce a [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput], which is then wrapped in an [`InputGuardrailResult`][agents.guardrail.InputGuardrailResult]
3. Finally, we check if [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] is true. If true, an [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered] exception is raised, so you can appropriately respond to the user or handle the exception.

!!! Note

    Input guardrails are intended to run on user input, so an agent's guardrails only run if the agent is the *first* agent. You might wonder, why is the `guardrails` property on the agent instead of passed to `Runner.run`? It's because guardrails tend to be related to the actual Agent - you'd run different guardrails for different agents, so colocating the code is useful for readability.

## Output guardrails

Output guardrails run in 3 steps:

1. First, the guardrail receives the output produced by the agent.
2. Next, the guardrail function runs to produce a [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput], which is then wrapped in an [`OutputGuardrailResult`][agents.guardrail.OutputGuardrailResult]
3. Finally, we check if [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] is true. If true, an [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered] exception is raised, so you can appropriately respond to the user or handle the exception.

!!! Note

    Output guardrails are intended to run on the final agent output, so an agent's guardrails only run if the agent is the *last* agent. Similar to the input guardrails, we do this because guardrails tend to be related to the actual Agent - you'd run different guardrails for different agents, so colocating the code is useful for readability.

## Tripwires

If the input or output fails the guardrail, the Guardrail can signal this with a tripwire. As soon as we see a guardrail that has triggered the tripwires, we immediately raise a `{Input,Output}GuardrailTripwireTriggered` exception and halt the Agent execution.

## Implementing a guardrail

You need to provide a function that receives input, and returns a [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]. In this example, we'll do this by running an Agent under the hood.

```python
from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    InputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    TResponseInputItem,
    input_guardrail,
)

class MathHomeworkOutput(BaseModel):
    is_math_homework: bool
    reasoning: str

guardrail_agent = Agent( # (1)!
    name="Guardrail check",
    instructions="Check if the user is asking you to do their math homework.",
    output_type=MathHomeworkOutput,
)


@input_guardrail
async def math_guardrail( # (2)!
    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, input, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output, # (3)!
        tripwire_triggered=result.final_output.is_math_homework,
    )


agent = Agent(  # (4)!
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    input_guardrails=[math_guardrail],
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except InputGuardrailTripwireTriggered:
        print("Math homework guardrail tripped")
```

1. We'll use this agent in our guardrail function.
2. This is the guardrail function that receives the agent's input/context, and returns the result.
3. We can include extra information in the guardrail result.
4. This is the actual agent that defines the workflow.

Output guardrails are similar.

```python
from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    OutputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    output_guardrail,
)
class MessageOutput(BaseModel): # (1)!
    response: str

class MathOutput(BaseModel): # (2)!
    reasoning: str
    is_math: bool

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the output includes any math.",
    output_type=MathOutput,
)

@output_guardrail
async def math_guardrail(  # (3)!
    ctx: RunContextWrapper, agent: Agent, output: MessageOutput
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, output.response, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output,
        tripwire_triggered=result.final_output.is_math,
    )

agent = Agent( # (4)!
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    output_guardrails=[math_guardrail],
    output_type=MessageOutput,
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except OutputGuardrailTripwireTriggered:
        print("Math output guardrail tripped")
```

1. This is the actual agent's output type.
2. This is the guardrail's output type.
3. This is the guardrail function that receives the agent's output, and returns the result.
4. This is the actual agent that defines the workflow.

================
File: docs/handoffs.md
================
# Handoffs

Handoffs allow an agent to delegate tasks to another agent. This is particularly useful in scenarios where different agents specialize in distinct areas. For example, a customer support app might have agents that each specifically handle tasks like order status, refunds, FAQs, etc.

Handoffs are represented as tools to the LLM. So if there's a handoff to an agent named `Refund Agent`, the tool would be called `transfer_to_refund_agent`.

## Creating a handoff

All agents have a [`handoffs`][agents.agent.Agent.handoffs] param, which can either take an `Agent` directly, or a `Handoff` object that customizes the Handoff.

You can create a handoff using the [`handoff()`][agents.handoffs.handoff] function provided by the Agents SDK. This function allows you to specify the agent to hand off to, along with optional overrides and input filters.

### Basic Usage

Here's how you can create a simple handoff:

```python
from agents import Agent, handoff

billing_agent = Agent(name="Billing agent")
refund_agent = Agent(name="Refund agent")

# (1)!
triage_agent = Agent(name="Triage agent", handoffs=[billing_agent, handoff(refund_agent)])
```

1. You can use the agent directly (as in `billing_agent`), or you can use the `handoff()` function.

### Customizing handoffs via the `handoff()` function

The [`handoff()`][agents.handoffs.handoff] function lets you customize things.

-   `agent`: This is the agent to which things will be handed off.
-   `tool_name_override`: By default, the `Handoff.default_tool_name()` function is used, which resolves to `transfer_to_<agent_name>`. You can override this.
-   `tool_description_override`: Override the default tool description from `Handoff.default_tool_description()`
-   `on_handoff`: A callback function executed when the handoff is invoked. This is useful for things like kicking off some data fetching as soon as you know a handoff is being invoked. This function receives the agent context, and can optionally also receive LLM generated input. The input data is controlled by the `input_type` param.
-   `input_type`: The type of input expected by the handoff (optional).
-   `input_filter`: This lets you filter the input received by the next agent. See below for more.
-   `is_enabled`: Whether the handoff is enabled. This can be a boolean or a function that returns a boolean, allowing you to dynamically enable or disable the handoff at runtime.

```python
from agents import Agent, handoff, RunContextWrapper

def on_handoff(ctx: RunContextWrapper[None]):
    print("Handoff called")

agent = Agent(name="My agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    tool_name_override="custom_handoff_tool",
    tool_description_override="Custom description",
)
```

## Handoff inputs

In certain situations, you want the LLM to provide some data when it calls a handoff. For example, imagine a handoff to an "Escalation agent". You might want a reason to be provided, so you can log it.

```python
from pydantic import BaseModel

from agents import Agent, handoff, RunContextWrapper

class EscalationData(BaseModel):
    reason: str

async def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):
    print(f"Escalation agent called with reason: {input_data.reason}")

agent = Agent(name="Escalation agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    input_type=EscalationData,
)
```

## Input filters

When a handoff occurs, it's as though the new agent takes over the conversation, and gets to see the entire previous conversation history. If you want to change this, you can set an [`input_filter`][agents.handoffs.Handoff.input_filter]. An input filter is a function that receives the existing input via a [`HandoffInputData`][agents.handoffs.HandoffInputData], and must return a new `HandoffInputData`.

There are some common patterns (for example removing all tool calls from the history), which are implemented for you in [`agents.extensions.handoff_filters`][]

```python
from agents import Agent, handoff
from agents.extensions import handoff_filters

agent = Agent(name="FAQ agent")

handoff_obj = handoff(
    agent=agent,
    input_filter=handoff_filters.remove_all_tools, # (1)!
)
```

1. This will automatically remove all tools from the history when `FAQ agent` is called.

## Recommended prompts

To make sure that LLMs understand handoffs properly, we recommend including information about handoffs in your agents. We have a suggested prefix in [`agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX`][], or you can call [`agents.extensions.handoff_prompt.prompt_with_handoff_instructions`][] to automatically add recommended data to your prompts.

```python
from agents import Agent
from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX

billing_agent = Agent(
    name="Billing agent",
    instructions=f"""{RECOMMENDED_PROMPT_PREFIX}
    <Fill in the rest of your prompt here>.""",
)
```

================
File: docs/index.md
================
# OpenAI Agents SDK

The [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It's a production-ready upgrade of our previous experimentation for agents, [Swarm](https://github.com/openai/swarm/tree/main). The Agents SDK has a very small set of primitives:

-   **Agents**, which are LLMs equipped with instructions and tools
-   **Handoffs**, which allow agents to delegate to other agents for specific tasks
-   **Guardrails**, which enable validation of agent inputs and outputs
-   **Sessions**, which automatically maintains conversation history across agent runs

In combination with Python, these primitives are powerful enough to express complex relationships between tools and agents, and allow you to build real-world applications without a steep learning curve. In addition, the SDK comes with built-in **tracing** that lets you visualize and debug your agentic flows, as well as evaluate them and even fine-tune models for your application.

## Why use the Agents SDK

The SDK has two driving design principles:

1. Enough features to be worth using, but few enough primitives to make it quick to learn.
2. Works great out of the box, but you can customize exactly what happens.

Here are the main features of the SDK:

-   Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.
-   Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.
-   Handoffs: A powerful feature to coordinate and delegate between multiple agents.
-   Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.
-   Sessions: Automatic conversation history management across agent runs, eliminating manual state handling.
-   Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.
-   Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.

## Installation

```bash
pip install openai-agents
```

## Hello world example

```python
from agents import Agent, Runner

agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)

# Code within the code,
# Functions calling themselves,
# Infinite loop's dance.
```

(_If running this, ensure you set the `OPENAI_API_KEY` environment variable_)

```bash
export OPENAI_API_KEY=sk-...
```

================
File: docs/mcp.md
================
# Model context protocol (MCP)

The [Model context protocol](https://modelcontextprotocol.io/introduction) (MCP) standardises how applications expose tools and
context to language models. From the official documentation:

> MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI
> applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP
> provides a standardized way to connect AI models to different data sources and tools.

The Agents Python SDK understands multiple MCP transports. This lets you reuse existing MCP servers or build your own to expose
filesystem, HTTP, or connector backed tools to an agent.

## Choosing an MCP integration

Before wiring an MCP server into an agent decide where the tool calls should execute and which transports you can reach. The
matrix below summarises the options that the Python SDK supports.

| What you need                                                                        | Recommended option                                    |
| ------------------------------------------------------------------------------------ | ----------------------------------------------------- |
| Let OpenAI's Responses API call a publicly reachable MCP server on the model's behalf| **Hosted MCP server tools** via [`HostedMCPTool`][agents.tool.HostedMCPTool] |
| Connect to Streamable HTTP servers that you run locally or remotely                  | **Streamable HTTP MCP servers** via [`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp] |
| Talk to servers that implement HTTP with Server-Sent Events                          | **HTTP with SSE MCP servers** via [`MCPServerSse`][agents.mcp.server.MCPServerSse] |
| Launch a local process and communicate over stdin/stdout                             | **stdio MCP servers** via [`MCPServerStdio`][agents.mcp.server.MCPServerStdio] |

The sections below walk through each option, how to configure it, and when to prefer one transport over another.

## 1. Hosted MCP server tools

Hosted tools push the entire tool round-trip into OpenAI's infrastructure. Instead of your code listing and calling tools, the
[`HostedMCPTool`][agents.tool.HostedMCPTool] forwards a server label (and optional connector metadata) to the Responses API. The
model lists the remote server's tools and invokes them without an extra callback to your Python process. Hosted tools currently
work with OpenAI models that support the Responses API's hosted MCP integration.

### Basic hosted MCP tool

Create a hosted tool by adding a [`HostedMCPTool`][agents.tool.HostedMCPTool] to the agent's `tools` list. The `tool_config`
dict mirrors the JSON you would send to the REST API:

```python
import asyncio

from agents import Agent, HostedMCPTool, Runner

async def main() -> None:
    agent = Agent(
        name="Assistant",
        tools=[
            HostedMCPTool(
                tool_config={
                    "type": "mcp",
                    "server_label": "gitmcp",
                    "server_url": "https://gitmcp.io/openai/codex",
                    "require_approval": "never",
                }
            )
        ],
    )

    result = await Runner.run(agent, "Which language is this repository written in?")
    print(result.final_output)

asyncio.run(main())
```

The hosted server exposes its tools automatically; you do not add it to `mcp_servers`.

### Streaming hosted MCP results

Hosted tools support streaming results in exactly the same way as function tools. Pass `stream=True` to `Runner.run_streamed` to
consume incremental MCP output while the model is still working:

```python
result = Runner.run_streamed(agent, "Summarise this repository's top languages")
async for event in result.stream_events():
    if event.type == "run_item_stream_event":
        print(f"Received: {event.item}")
print(result.final_output)
```

### Optional approval flows

If a server can perform sensitive operations you can require human or programmatic approval before each tool execution. Configure
`require_approval` in the `tool_config` with either a single policy (`"always"`, `"never"`) or a dict mapping tool names to
policies. To make the decision inside Python, provide an `on_approval_request` callback.

```python
from agents import MCPToolApprovalFunctionResult, MCPToolApprovalRequest

SAFE_TOOLS = {"read_project_metadata"}

def approve_tool(request: MCPToolApprovalRequest) -> MCPToolApprovalFunctionResult:
    if request.data.name in SAFE_TOOLS:
        return {"approve": True}
    return {"approve": False, "reason": "Escalate to a human reviewer"}

agent = Agent(
    name="Assistant",
    tools=[
        HostedMCPTool(
            tool_config={
                "type": "mcp",
                "server_label": "gitmcp",
                "server_url": "https://gitmcp.io/openai/codex",
                "require_approval": "always",
            },
            on_approval_request=approve_tool,
        )
    ],
)
```

The callback can be synchronous or asynchronous and is invoked whenever the model needs approval data to keep running.

### Connector-backed hosted servers

Hosted MCP also supports OpenAI connectors. Instead of specifying a `server_url`, supply a `connector_id` and an access token. The
Responses API handles authentication and the hosted server exposes the connector's tools.

```python
import os

HostedMCPTool(
    tool_config={
        "type": "mcp",
        "server_label": "google_calendar",
        "connector_id": "connector_googlecalendar",
        "authorization": os.environ["GOOGLE_CALENDAR_AUTHORIZATION"],
        "require_approval": "never",
    }
)
```

Fully working hosted tool samplesâ€”including streaming, approvals, and connectorsâ€”live in
[`examples/hosted_mcp`](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp).

## 2. Streamable HTTP MCP servers

When you want to manage the network connection yourself, use
[`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp]. Streamable HTTP servers are ideal when you control the
transport or want to run the server inside your own infrastructure while keeping latency low.

```python
import asyncio
import os

from agents import Agent, Runner
from agents.mcp import MCPServerStreamableHttp
from agents.model_settings import ModelSettings

async def main() -> None:
    token = os.environ["MCP_SERVER_TOKEN"]
    async with MCPServerStreamableHttp(
        name="Streamable HTTP Python Server",
        params={
            "url": "http://localhost:8000/mcp",
            "headers": {"Authorization": f"Bearer {token}"},
            "timeout": 10,
        },
        cache_tools_list=True,
        max_retry_attempts=3,
    ) as server:
        agent = Agent(
            name="Assistant",
            instructions="Use the MCP tools to answer the questions.",
            mcp_servers=[server],
            model_settings=ModelSettings(tool_choice="required"),
        )

        result = await Runner.run(agent, "Add 7 and 22.")
        print(result.final_output)

asyncio.run(main())
```

The constructor accepts additional options:

- `client_session_timeout_seconds` controls HTTP read timeouts.
- `use_structured_content` toggles whether `tool_result.structured_content` is preferred over textual output.
- `max_retry_attempts` and `retry_backoff_seconds_base` add automatic retries for `list_tools()` and `call_tool()`.
- `tool_filter` lets you expose only a subset of tools (see [Tool filtering](#tool-filtering)).

## 3. HTTP with SSE MCP servers

If the MCP server implements the HTTP with SSE transport, instantiate
[`MCPServerSse`][agents.mcp.server.MCPServerSse]. Apart from the transport, the API is identical to the Streamable HTTP server.

```python

from agents import Agent, Runner
from agents.model_settings import ModelSettings
from agents.mcp import MCPServerSse

workspace_id = "demo-workspace"

async with MCPServerSse(
    name="SSE Python Server",
    params={
        "url": "http://localhost:8000/sse",
        "headers": {"X-Workspace": workspace_id},
    },
    cache_tools_list=True,
) as server:
    agent = Agent(
        name="Assistant",
        mcp_servers=[server],
        model_settings=ModelSettings(tool_choice="required"),
    )
    result = await Runner.run(agent, "What's the weather in Tokyo?")
    print(result.final_output)
```

## 4. stdio MCP servers

For MCP servers that run as local subprocesses, use [`MCPServerStdio`][agents.mcp.server.MCPServerStdio]. The SDK spawns the
process, keeps the pipes open, and closes them automatically when the context manager exits. This option is helpful for quick
proofs of concept or when the server only exposes a command line entry point.

```python
from pathlib import Path
from agents import Agent, Runner
from agents.mcp import MCPServerStdio

current_dir = Path(__file__).parent
samples_dir = current_dir / "sample_files"

async with MCPServerStdio(
    name="Filesystem Server via npx",
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", str(samples_dir)],
    },
) as server:
    agent = Agent(
        name="Assistant",
        instructions="Use the files in the sample directory to answer questions.",
        mcp_servers=[server],
    )
    result = await Runner.run(agent, "List the files available to you.")
    print(result.final_output)
```

## Tool filtering

Each MCP server supports tool filters so that you can expose only the functions that your agent needs. Filtering can happen at
construction time or dynamically per run.

### Static tool filtering

Use [`create_static_tool_filter`][agents.mcp.create_static_tool_filter] to configure simple allow/block lists:

```python
from pathlib import Path

from agents.mcp import MCPServerStdio, create_static_tool_filter

samples_dir = Path("/path/to/files")

filesystem_server = MCPServerStdio(
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", str(samples_dir)],
    },
    tool_filter=create_static_tool_filter(allowed_tool_names=["read_file", "write_file"]),
)
```

When both `allowed_tool_names` and `blocked_tool_names` are supplied the SDK applies the allow-list first and then removes any
blocked tools from the remaining set.

### Dynamic tool filtering

For more elaborate logic pass a callable that receives a [`ToolFilterContext`][agents.mcp.ToolFilterContext]. The callable can be
synchronous or asynchronous and returns `True` when the tool should be exposed.

```python
from pathlib import Path

from agents.mcp import MCPServerStdio, ToolFilterContext

samples_dir = Path("/path/to/files")

async def context_aware_filter(context: ToolFilterContext, tool) -> bool:
    if context.agent.name == "Code Reviewer" and tool.name.startswith("danger_"):
        return False
    return True

async with MCPServerStdio(
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", str(samples_dir)],
    },
    tool_filter=context_aware_filter,
) as server:
    ...
```

The filter context exposes the active `run_context`, the `agent` requesting the tools, and the `server_name`.

## Prompts

MCP servers can also provide prompts that dynamically generate agent instructions. Servers that support prompts expose two
methods:

- `list_prompts()` enumerates the available prompt templates.
- `get_prompt(name, arguments)` fetches a concrete prompt, optionally with parameters.

```python
from agents import Agent

prompt_result = await server.get_prompt(
    "generate_code_review_instructions",
    {"focus": "security vulnerabilities", "language": "python"},
)
instructions = prompt_result.messages[0].content.text

agent = Agent(
    name="Code Reviewer",
    instructions=instructions,
    mcp_servers=[server],
)
```

## Caching

Every agent run calls `list_tools()` on each MCP server. Remote servers can introduce noticeable latency, so all of the MCP
server classes expose a `cache_tools_list` option. Set it to `True` only if you are confident that the tool definitions do not
change frequently. To force a fresh list later, call `invalidate_tools_cache()` on the server instance.

## Tracing

[Tracing](./tracing.md) automatically captures MCP activity, including:

1. Calls to the MCP server to list tools.
2. MCP-related information on tool calls.

![MCP Tracing Screenshot](./assets/images/mcp-tracing.jpg)

## Further reading

- [Model Context Protocol](https://modelcontextprotocol.io/) â€“ the specification and design guides.
- [examples/mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp) â€“ runnable stdio, SSE, and Streamable HTTP samples.
- [examples/hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp) â€“ complete hosted MCP demonstrations including approvals and connectors.

================
File: docs/multi_agent.md
================
# Orchestrating multiple agents

Orchestration refers to the flow of agents in your app. Which agents run, in what order, and how do they decide what happens next? There are two main ways to orchestrate agents:

1. Allowing the LLM to make decisions: this uses the intelligence of an LLM to plan, reason, and decide on what steps to take based on that.
2. Orchestrating via code: determining the flow of agents via your code.

You can mix and match these patterns. Each has their own tradeoffs, described below.

## Orchestrating via LLM

An agent is an LLM equipped with instructions, tools and handoffs. This means that given an open-ended task, the LLM can autonomously plan how it will tackle the task, using tools to take actions and acquire data, and using handoffs to delegate tasks to sub-agents. For example, a research agent could be equipped with tools like:

-   Web search to find information online
-   File search and retrieval to search through proprietary data and connections
-   Computer use to take actions on a computer
-   Code execution to do data analysis
-   Handoffs to specialized agents that are great at planning, report writing and more.

This pattern is great when the task is open-ended and you want to rely on the intelligence of an LLM. The most important tactics here are:

1. Invest in good prompts. Make it clear what tools are available, how to use them, and what parameters it must operate within.
2. Monitor your app and iterate on it. See where things go wrong, and iterate on your prompts.
3. Allow the agent to introspect and improve. For example, run it in a loop, and let it critique itself; or, provide error messages and let it improve.
4. Have specialized agents that excel in one task, rather than having a general purpose agent that is expected to be good at anything.
5. Invest in [evals](https://platform.openai.com/docs/guides/evals). This lets you train your agents to improve and get better at tasks.

## Orchestrating via code

While orchestrating via LLM is powerful, orchestrating via code makes tasks more deterministic and predictable, in terms of speed, cost and performance. Common patterns here are:

-   Using [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) to generate well formed data that you can inspect with your code. For example, you might ask an agent to classify the task into a few categories, and then pick the next agent based on the category.
-   Chaining multiple agents by transforming the output of one into the input of the next. You can decompose a task like writing a blog post into a series of steps - do research, write an outline, write the blog post, critique it, and then improve it.
-   Running the agent that performs the task in a `while` loop with an agent that evaluates and provides feedback, until the evaluator says the output passes certain criteria.
-   Running multiple agents in parallel, e.g. via Python primitives like `asyncio.gather`. This is useful for speed when you have multiple tasks that don't depend on each other.

We have a number of examples in [`examples/agent_patterns`](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns).

================
File: docs/quickstart.md
================
# Quickstart

## Create a project and virtual environment

You'll only need to do this once.

```bash
mkdir my_project
cd my_project
python -m venv .venv
```

### Activate the virtual environment

Do this every time you start a new terminal session.

```bash
source .venv/bin/activate
```

### Install the Agents SDK

```bash
pip install openai-agents # or `uv add openai-agents`, etc
```

### Set an OpenAI API key

If you don't have one, follow [these instructions](https://platform.openai.com/docs/quickstart#create-and-export-an-api-key) to create an OpenAI API key.

```bash
export OPENAI_API_KEY=sk-...
```

## Create your first agent

Agents are defined with instructions, a name, and optional config (such as `model_config`)

```python
from agents import Agent

agent = Agent(
    name="Math Tutor",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)
```

## Add a few more agents

Additional agents can be defined in the same way. `handoff_descriptions` provide additional context for determining handoff routing

```python
from agents import Agent

history_tutor_agent = Agent(
    name="History Tutor",
    handoff_description="Specialist agent for historical questions",
    instructions="You provide assistance with historical queries. Explain important events and context clearly.",
)

math_tutor_agent = Agent(
    name="Math Tutor",
    handoff_description="Specialist agent for math questions",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)
```

## Define your handoffs

On each agent, you can define an inventory of outgoing handoff options that the agent can choose from to decide how to make progress on their task.

```python
triage_agent = Agent(
    name="Triage Agent",
    instructions="You determine which agent to use based on the user's homework question",
    handoffs=[history_tutor_agent, math_tutor_agent]
)
```

## Run the agent orchestration

Let's check that the workflow runs and the triage agent correctly routes between the two specialist agents.

```python
from agents import Runner

async def main():
    result = await Runner.run(triage_agent, "What is the capital of France?")
    print(result.final_output)
```

## Add a guardrail

You can define custom guardrails to run on the input or output.

```python
from agents import GuardrailFunctionOutput, Agent, Runner
from pydantic import BaseModel


class HomeworkOutput(BaseModel):
    is_homework: bool
    reasoning: str

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the user is asking about homework.",
    output_type=HomeworkOutput,
)

async def homework_guardrail(ctx, agent, input_data):
    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
    final_output = result.final_output_as(HomeworkOutput)
    return GuardrailFunctionOutput(
        output_info=final_output,
        tripwire_triggered=not final_output.is_homework,
    )
```

## Put it all together

Let's put it all together and run the entire workflow, using handoffs and the input guardrail.

```python
from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner
from agents.exceptions import InputGuardrailTripwireTriggered
from pydantic import BaseModel
import asyncio

class HomeworkOutput(BaseModel):
    is_homework: bool
    reasoning: str

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the user is asking about homework.",
    output_type=HomeworkOutput,
)

math_tutor_agent = Agent(
    name="Math Tutor",
    handoff_description="Specialist agent for math questions",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)

history_tutor_agent = Agent(
    name="History Tutor",
    handoff_description="Specialist agent for historical questions",
    instructions="You provide assistance with historical queries. Explain important events and context clearly.",
)


async def homework_guardrail(ctx, agent, input_data):
    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
    final_output = result.final_output_as(HomeworkOutput)
    return GuardrailFunctionOutput(
        output_info=final_output,
        tripwire_triggered=not final_output.is_homework,
    )

triage_agent = Agent(
    name="Triage Agent",
    instructions="You determine which agent to use based on the user's homework question",
    handoffs=[history_tutor_agent, math_tutor_agent],
    input_guardrails=[
        InputGuardrail(guardrail_function=homework_guardrail),
    ],
)

async def main():
    # Example 1: History question
    try:
        result = await Runner.run(triage_agent, "who was the first president of the united states?")
        print(result.final_output)
    except InputGuardrailTripwireTriggered as e:
        print("Guardrail blocked this input:", e)

    # Example 2: General/philosophical question
    try:
        result = await Runner.run(triage_agent, "What is the meaning of life?")
        print(result.final_output)
    except InputGuardrailTripwireTriggered as e:
        print("Guardrail blocked this input:", e)

if __name__ == "__main__":
    asyncio.run(main())
```

## View your traces

To review what happened during your agent run, navigate to the [Trace viewer in the OpenAI Dashboard](https://platform.openai.com/traces) to view traces of your agent runs.

## Next steps

Learn how to build more complex agentic flows:

-   Learn about how to configure [Agents](agents.md).
-   Learn about [running agents](running_agents.md).
-   Learn about [tools](tools.md), [guardrails](guardrails.md) and [models](models/index.md).

================
File: docs/release.md
================
# Release process/changelog

The project follows a slightly modified version of semantic versioning using the form `0.Y.Z`. The leading `0` indicates the SDK is still evolving rapidly. Increment the components as follows:

## Minor (`Y`) versions

We will increase minor versions `Y` for **breaking changes** to any public interfaces that are not marked as beta. For example, going from `0.0.x` to `0.1.x` might include breaking changes.

If you don't want breaking changes, we recommend pinning to `0.0.x` versions in your project.

## Patch (`Z`) versions

We will increment `Z` for non-breaking changes:

-   Bug fixes
-   New features
-   Changes to private interfaces
-   Updates to beta features

## Breaking change changelog

### 0.2.0

In this version, a few places that used to take `Agent` as an arg, now take `AgentBase` as an arg instead. For example, the `list_tools()` call in MCP servers. This is a purely typing change, you will still receive `Agent` objects. To update, just fix type errors by replacing `Agent` with `AgentBase`.

### 0.1.0

In this version, [`MCPServer.list_tools()`][agents.mcp.server.MCPServer] has two new params: `run_context` and `agent`. You'll need to add these params to any classes that subclass `MCPServer`.

================
File: docs/repl.md
================
# REPL utility

The SDK provides `run_demo_loop` for quick, interactive testing of an agent's behavior directly in your terminal.


```python
import asyncio
from agents import Agent, run_demo_loop

async def main() -> None:
    agent = Agent(name="Assistant", instructions="You are a helpful assistant.")
    await run_demo_loop(agent)

if __name__ == "__main__":
    asyncio.run(main())
```

`run_demo_loop` prompts for user input in a loop, keeping the conversation history between turns. By default, it streams model output as it is produced. When you run the example above, run_demo_loop starts an interactive chat session. It continuously asks for your input, remembers the entire conversation history between turns (so your agent knows what's been discussed) and automatically streams the agent's responses to you in real-time as they are generated.

To end this chat session, simply type `quit` or `exit` (and press Enter) or use the `Ctrl-D` keyboardÂ shortcut.

================
File: docs/results.md
================
# Results

When you call the `Runner.run` methods, you either get a:

-   [`RunResult`][agents.result.RunResult] if you call `run` or `run_sync`
-   [`RunResultStreaming`][agents.result.RunResultStreaming] if you call `run_streamed`

Both of these inherit from [`RunResultBase`][agents.result.RunResultBase], which is where most useful information is present.

## Final output

The [`final_output`][agents.result.RunResultBase.final_output] property contains the final output of the last agent that ran. This is either:

-   a `str`, if the last agent didn't have an `output_type` defined
-   an object of type `last_agent.output_type`, if the agent had an output type defined.

!!! note

    `final_output` is of type `Any`. We can't statically type this, because of handoffs. If handoffs occur, that means any Agent might be the last agent, so we don't statically know the set of possible output types.

## Inputs for the next turn

You can use [`result.to_input_list()`][agents.result.RunResultBase.to_input_list] to turn the result into an input list that concatenates the original input you provided, to the items generated during the agent run. This makes it convenient to take the outputs of one agent run and pass them into another run, or to run it in a loop and append new user inputs each time.

## Last agent

The [`last_agent`][agents.result.RunResultBase.last_agent] property contains the last agent that ran. Depending on your application, this is often useful for the next time the user inputs something. For example, if you have a frontline triage agent that hands off to a language-specific agent, you can store the last agent, and re-use it the next time the user messages the agent.

## New items

The [`new_items`][agents.result.RunResultBase.new_items] property contains the new items generated during the run. The items are [`RunItem`][agents.items.RunItem]s. A run item wraps the raw item generated by the LLM.

-   [`MessageOutputItem`][agents.items.MessageOutputItem] indicates a message from the LLM. The raw item is the message generated.
-   [`HandoffCallItem`][agents.items.HandoffCallItem] indicates that the LLM called the handoff tool. The raw item is the tool call item from the LLM.
-   [`HandoffOutputItem`][agents.items.HandoffOutputItem] indicates that a handoff occurred. The raw item is the tool response to the handoff tool call. You can also access the source/target agents from the item.
-   [`ToolCallItem`][agents.items.ToolCallItem] indicates that the LLM invoked a tool.
-   [`ToolCallOutputItem`][agents.items.ToolCallOutputItem] indicates that a tool was called. The raw item is the tool response. You can also access the tool output from the item.
-   [`ReasoningItem`][agents.items.ReasoningItem] indicates a reasoning item from the LLM. The raw item is the reasoning generated.

## Other information

### Guardrail results

The [`input_guardrail_results`][agents.result.RunResultBase.input_guardrail_results] and [`output_guardrail_results`][agents.result.RunResultBase.output_guardrail_results] properties contain the results of the guardrails, if any. Guardrail results can sometimes contain useful information you want to log or store, so we make these available to you.

### Raw responses

The [`raw_responses`][agents.result.RunResultBase.raw_responses] property contains the [`ModelResponse`][agents.items.ModelResponse]s generated by the LLM.

### Original input

The [`input`][agents.result.RunResultBase.input] property contains the original input you provided to the `run` method. In most cases you won't need this, but it's available in case you do.

================
File: docs/running_agents.md
================
# Running agents

You can run agents via the [`Runner`][agents.run.Runner] class. You have 3 options:

1. [`Runner.run()`][agents.run.Runner.run], which runs async and returns a [`RunResult`][agents.result.RunResult].
2. [`Runner.run_sync()`][agents.run.Runner.run_sync], which is a sync method and just runs `.run()` under the hood.
3. [`Runner.run_streamed()`][agents.run.Runner.run_streamed], which runs async and returns a [`RunResultStreaming`][agents.result.RunResultStreaming]. It calls the LLM in streaming mode, and streams those events to you as they are received.

```python
from agents import Agent, Runner

async def main():
    agent = Agent(name="Assistant", instructions="You are a helpful assistant")

    result = await Runner.run(agent, "Write a haiku about recursion in programming.")
    print(result.final_output)
    # Code within the code,
    # Functions calling themselves,
    # Infinite loop's dance
```

Read more in the [results guide](results.md).

## The agent loop

When you use the run method in `Runner`, you pass in a starting agent and input. The input can either be a string (which is considered a user message), or a list of input items, which are the items in the OpenAI Responses API.

The runner then runs a loop:

1. We call the LLM for the current agent, with the current input.
2. The LLM produces its output.
    1. If the LLM returns a `final_output`, the loop ends and we return the result.
    2. If the LLM does a handoff, we update the current agent and input, and re-run the loop.
    3. If the LLM produces tool calls, we run those tool calls, append the results, and re-run the loop.
3. If we exceed the `max_turns` passed, we raise a [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded] exception.

!!! note

    The rule for whether the LLM output is considered as a "final output" is that it produces text output with the desired type, and there are no tool calls.

## Streaming

Streaming allows you to additionally receive streaming events as the LLM runs. Once the stream is done, the [`RunResultStreaming`][agents.result.RunResultStreaming] will contain the complete information about the run, including all the new outputs produced. You can call `.stream_events()` for the streaming events. Read more in the [streaming guide](streaming.md).

## Run config

The `run_config` parameter lets you configure some global settings for the agent run:

-   [`model`][agents.run.RunConfig.model]: Allows setting a global LLM model to use, irrespective of what `model` each Agent has.
-   [`model_provider`][agents.run.RunConfig.model_provider]: A model provider for looking up model names, which defaults to OpenAI.
-   [`model_settings`][agents.run.RunConfig.model_settings]: Overrides agent-specific settings. For example, you can set a global `temperature` or `top_p`.
-   [`input_guardrails`][agents.run.RunConfig.input_guardrails], [`output_guardrails`][agents.run.RunConfig.output_guardrails]: A list of input or output guardrails to include on all runs.
-   [`handoff_input_filter`][agents.run.RunConfig.handoff_input_filter]: A global input filter to apply to all handoffs, if the handoff doesn't already have one. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] for more details.
-   [`tracing_disabled`][agents.run.RunConfig.tracing_disabled]: Allows you to disable [tracing](tracing.md) for the entire run.
-   [`trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data]: Configures whether traces will include potentially sensitive data, such as LLM and tool call inputs/outputs.
-   [`workflow_name`][agents.run.RunConfig.workflow_name], [`trace_id`][agents.run.RunConfig.trace_id], [`group_id`][agents.run.RunConfig.group_id]: Sets the tracing workflow name, trace ID and trace group ID for the run. We recommend at least setting `workflow_name`. The group ID is an optional field that lets you link traces across multiple runs.
-   [`trace_metadata`][agents.run.RunConfig.trace_metadata]: Metadata to include on all traces.

## Conversations/chat threads

Calling any of the run methods can result in one or more agents running (and hence one or more LLM calls), but it represents a single logical turn in a chat conversation. For example:

1. User turn: user enter text
2. Runner run: first agent calls LLM, runs tools, does a handoff to a second agent, second agent runs more tools, and then produces an output.

At the end of the agent run, you can choose what to show to the user. For example, you might show the user every new item generated by the agents, or just the final output. Either way, the user might then ask a followup question, in which case you can call the run method again.

### Manual conversation management

You can manually manage conversation history using the [`RunResultBase.to_input_list()`][agents.result.RunResultBase.to_input_list] method to get the inputs for the next turn:

```python
async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    thread_id = "thread_123"  # Example thread ID
    with trace(workflow_name="Conversation", group_id=thread_id):
        # First turn
        result = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
        print(result.final_output)
        # San Francisco

        # Second turn
        new_input = result.to_input_list() + [{"role": "user", "content": "What state is it in?"}]
        result = await Runner.run(agent, new_input)
        print(result.final_output)
        # California
```

### Automatic conversation management with Sessions

For a simpler approach, you can use [Sessions](sessions/index.md) to automatically handle conversation history without manually calling `.to_input_list()`:

```python
from agents import Agent, Runner, SQLiteSession

async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    # Create session instance
    session = SQLiteSession("conversation_123")

    thread_id = "thread_123"  # Example thread ID
    with trace(workflow_name="Conversation", group_id=thread_id):
        # First turn
        result = await Runner.run(agent, "What city is the Golden Gate Bridge in?", session=session)
        print(result.final_output)
        # San Francisco

        # Second turn - agent automatically remembers previous context
        result = await Runner.run(agent, "What state is it in?", session=session)
        print(result.final_output)
        # California
```

Sessions automatically:

-   Retrieves conversation history before each run
-   Stores new messages after each run
-   Maintains separate conversations for different session IDs

See the [Sessions documentation](sessions/index.md) for more details.


### Server-managed conversations

You can also let the OpenAI conversation state feature manage conversation state on the server side, instead of handling it locally with `to_input_list()` or `Sessions`. This allows you to preserve conversation history without manually resending all past messages. See the [OpenAI Conversation state guide](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses) for more details.

OpenAI provides two ways to track state across turns:

#### 1. Using `conversation_id`

You first create a conversation using the OpenAI Conversations API and then reuse its ID for every subsequent call:

```python
from agents import Agent, Runner
from openai import AsyncOpenAI

client = AsyncOpenAI()

async def main():
    # Create a server-managed conversation
    conversation = await client.conversations.create()
    conv_id = conversation.id    

    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    # First turn
    result1 = await Runner.run(agent, "What city is the Golden Gate Bridge in?", conversation_id=conv_id)
    print(result1.final_output)
    # San Francisco

    # Second turn reuses the same conversation_id
    result2 = await Runner.run(
        agent,
        "What state is it in?",
        conversation_id=conv_id,
    )
    print(result2.final_output)
    # California
```

#### 2. Using `previous_response_id`

Another option is **response chaining**, where each turn links explicitly to the response ID from the previous turn.

```python
from agents import Agent, Runner

async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    # First turn
    result1 = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
    print(result1.final_output)
    # San Francisco

    # Second turn, chained to the previous response
    result2 = await Runner.run(
        agent,
        "What state is it in?",
        previous_response_id=result1.last_response_id,
    )
    print(result2.final_output)
    # California
```


## Long running agents & human-in-the-loop

You can use the Agents SDK [Temporal](https://temporal.io/) integration to run durable, long-running workflows, including human-in-the-loop tasks. View a demo of Temporal and the Agents SDK working in action to complete long-running tasks [in this video](https://www.youtube.com/watch?v=fFBZqzT4DD8), and [view docs here](https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents).

## Exceptions

The SDK raises exceptions in certain cases. The full list is in [`agents.exceptions`][]. As an overview:

-   [`AgentsException`][agents.exceptions.AgentsException]: This is the base class for all exceptions raised within the SDK. It serves as a generic type from which all other specific exceptions are derived.
-   [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded]: This exception is raised when the agent's run exceeds the `max_turns` limit passed to the `Runner.run`, `Runner.run_sync`, or `Runner.run_streamed` methods. It indicates that the agent could not complete its task within the specified number of interaction turns.
-   [`ModelBehaviorError`][agents.exceptions.ModelBehaviorError]: This exception occurs when the underlying model (LLM) produces unexpected or invalid outputs. This can include:
    -   Malformed JSON: When the model provides a malformed JSON structure for tool calls or in its direct output, especially if a specific `output_type` is defined.
    -   Unexpected tool-related failures: When the model fails to use tools in an expected manner
-   [`UserError`][agents.exceptions.UserError]: This exception is raised when you (the person writing code using the SDK) make an error while using the SDK. This typically results from incorrect code implementation, invalid configuration, or misuse of the SDK's API.
-   [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered], [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered]: This exception is raised when the conditions of an input guardrail or output guardrail are met, respectively. Input guardrails check incoming messages before processing, while output guardrails check the agent's final response before delivery.

================
File: docs/streaming.md
================
# Streaming

Streaming lets you subscribe to updates of the agent run as it proceeds. This can be useful for showing the end-user progress updates and partial responses.

To stream, you can call [`Runner.run_streamed()`][agents.run.Runner.run_streamed], which will give you a [`RunResultStreaming`][agents.result.RunResultStreaming]. Calling `result.stream_events()` gives you an async stream of [`StreamEvent`][agents.stream_events.StreamEvent] objects, which are described below.

## Raw response events

[`RawResponsesStreamEvent`][agents.stream_events.RawResponsesStreamEvent] are raw events passed directly from the LLM. They are in OpenAI Responses API format, which means each event has a type (like `response.created`, `response.output_text.delta`, etc) and data. These events are useful if you want to stream response messages to the user as soon as they are generated.

For example, this will output the text generated by the LLM token-by-token.

```python
import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner

async def main():
    agent = Agent(
        name="Joker",
        instructions="You are a helpful assistant.",
    )

    result = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            print(event.data.delta, end="", flush=True)


if __name__ == "__main__":
    asyncio.run(main())
```

## Run item events and agent events

[`RunItemStreamEvent`][agents.stream_events.RunItemStreamEvent]s are higher level events. They inform you when an item has been fully generated. This allows you to push progress updates at the level of "message generated", "tool ran", etc, instead of each token. Similarly, [`AgentUpdatedStreamEvent`][agents.stream_events.AgentUpdatedStreamEvent] gives you updates when the current agent changes (e.g. as the result of a handoff).

For example, this will ignore raw events and stream updates to the user.

```python
import asyncio
import random
from agents import Agent, ItemHelpers, Runner, function_tool

@function_tool
def how_many_jokes() -> int:
    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="Joker",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],
    )

    result = Runner.run_streamed(
        agent,
        input="Hello",
    )
    print("=== Run starting ===")

    async for event in result.stream_events():
        # We'll ignore the raw responses event deltas
        if event.type == "raw_response_event":
            continue
        # When the agent updates, print that
        elif event.type == "agent_updated_stream_event":
            print(f"Agent updated: {event.new_agent.name}")
            continue
        # When items are generated, print them
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}")
            else:
                pass  # Ignore other event types

    print("=== Run complete ===")


if __name__ == "__main__":
    asyncio.run(main())
```

================
File: docs/tools.md
================
# Tools

Tools let agents take actions: things like fetching data, running code, calling external APIs, and even using a computer. There are three classes of tools in the Agent SDK:

-   Hosted tools: these run on LLM servers alongside the AI models. OpenAI offers retrieval, web search and computer use as hosted tools.
-   Function calling: these allow you to use any Python function as a tool.
-   Agents as tools: this allows you to use an agent as a tool, allowing Agents to call other agents without handing off to them.

## Hosted tools

OpenAI offers a few built-in tools when using the [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel]:

-   The [`WebSearchTool`][agents.tool.WebSearchTool] lets an agent search the web.
-   The [`FileSearchTool`][agents.tool.FileSearchTool] allows retrieving information from your OpenAI Vector Stores.
-   The [`ComputerTool`][agents.tool.ComputerTool] allows automating computer use tasks.
-   The [`CodeInterpreterTool`][agents.tool.CodeInterpreterTool] lets the LLM execute code in a sandboxed environment.
-   The [`HostedMCPTool`][agents.tool.HostedMCPTool] exposes a remote MCP server's tools to the model.
-   The [`ImageGenerationTool`][agents.tool.ImageGenerationTool] generates images from a prompt.
-   The [`LocalShellTool`][agents.tool.LocalShellTool] runs shell commands on your machine.

```python
from agents import Agent, FileSearchTool, Runner, WebSearchTool

agent = Agent(
    name="Assistant",
    tools=[
        WebSearchTool(),
        FileSearchTool(
            max_num_results=3,
            vector_store_ids=["VECTOR_STORE_ID"],
        ),
    ],
)

async def main():
    result = await Runner.run(agent, "Which coffee shop should I go to, taking into account my preferences and the weather today in SF?")
    print(result.final_output)
```

## Function tools

You can use any Python function as a tool. The Agents SDK will setup the tool automatically:

-   The name of the tool will be the name of the Python function (or you can provide a name)
-   Tool description will be taken from the docstring of the function (or you can provide a description)
-   The schema for the function inputs is automatically created from the function's arguments
-   Descriptions for each input are taken from the docstring of the function, unless disabled

We use Python's `inspect` module to extract the function signature, along with [`griffe`](https://mkdocstrings.github.io/griffe/) to parse docstrings and `pydantic` for schema creation.

```python
import json

from typing_extensions import TypedDict, Any

from agents import Agent, FunctionTool, RunContextWrapper, function_tool


class Location(TypedDict):
    lat: float
    long: float

@function_tool  # (1)!
async def fetch_weather(location: Location) -> str:
    # (2)!
    """Fetch the weather for a given location.

    Args:
        location: The location to fetch the weather for.
    """
    # In real life, we'd fetch the weather from a weather API
    return "sunny"


@function_tool(name_override="fetch_data")  # (3)!
def read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -> str:
    """Read the contents of a file.

    Args:
        path: The path to the file to read.
        directory: The directory to read the file from.
    """
    # In real life, we'd read the file from the file system
    return "<file contents>"


agent = Agent(
    name="Assistant",
    tools=[fetch_weather, read_file],  # (4)!
)

for tool in agent.tools:
    if isinstance(tool, FunctionTool):
        print(tool.name)
        print(tool.description)
        print(json.dumps(tool.params_json_schema, indent=2))
        print()

```

1.  You can use any Python types as arguments to your functions, and the function can be sync or async.
2.  Docstrings, if present, are used to capture descriptions and argument descriptions
3.  Functions can optionally take the `context` (must be the first argument). You can also set overrides, like the name of the tool, description, which docstring style to use, etc.
4.  You can pass the decorated functions to the list of tools.

??? note "Expand to see output"

    ```
    fetch_weather
    Fetch the weather for a given location.
    {
    "$defs": {
      "Location": {
        "properties": {
          "lat": {
            "title": "Lat",
            "type": "number"
          },
          "long": {
            "title": "Long",
            "type": "number"
          }
        },
        "required": [
          "lat",
          "long"
        ],
        "title": "Location",
        "type": "object"
      }
    },
    "properties": {
      "location": {
        "$ref": "#/$defs/Location",
        "description": "The location to fetch the weather for."
      }
    },
    "required": [
      "location"
    ],
    "title": "fetch_weather_args",
    "type": "object"
    }

    fetch_data
    Read the contents of a file.
    {
    "properties": {
      "path": {
        "description": "The path to the file to read.",
        "title": "Path",
        "type": "string"
      },
      "directory": {
        "anyOf": [
          {
            "type": "string"
          },
          {
            "type": "null"
          }
        ],
        "default": null,
        "description": "The directory to read the file from.",
        "title": "Directory"
      }
    },
    "required": [
      "path"
    ],
    "title": "fetch_data_args",
    "type": "object"
    }
    ```

### Returning images or files from function tools

In addition to returning text outputs, you can return one or many images or files as the output of a function tool. To do so, you can return any of:

-   Images: [`ToolOutputImage`][agents.tool.ToolOutputImage] (or the TypedDict version, [`ToolOutputImageDict`][agents.tool.ToolOutputImageDict])
-   Files: [`ToolOutputFileContent`][agents.tool.ToolOutputFileContent] (or the TypedDict version, [`ToolOutputFileContentDict`][agents.tool.ToolOutputFileContentDict])
-   Text: either a string or stringable objects, or [`ToolOutputText`][agents.tool.ToolOutputText] (or the TypedDict version, [`ToolOutputTextDict`][agents.tool.ToolOutputTextDict])

### Custom function tools

Sometimes, you don't want to use a Python function as a tool. You can directly create a [`FunctionTool`][agents.tool.FunctionTool] if you prefer. You'll need to provide:

-   `name`
-   `description`
-   `params_json_schema`, which is the JSON schema for the arguments
-   `on_invoke_tool`, which is an async function that receives a [`ToolContext`][agents.tool_context.ToolContext] and the arguments as a JSON string, and must return the tool output as a string.

```python
from typing import Any

from pydantic import BaseModel

from agents import RunContextWrapper, FunctionTool



def do_some_work(data: str) -> str:
    return "done"


class FunctionArgs(BaseModel):
    username: str
    age: int


async def run_function(ctx: RunContextWrapper[Any], args: str) -> str:
    parsed = FunctionArgs.model_validate_json(args)
    return do_some_work(data=f"{parsed.username} is {parsed.age} years old")


tool = FunctionTool(
    name="process_user",
    description="Processes extracted user data",
    params_json_schema=FunctionArgs.model_json_schema(),
    on_invoke_tool=run_function,
)
```

### Automatic argument and docstring parsing

As mentioned before, we automatically parse the function signature to extract the schema for the tool, and we parse the docstring to extract descriptions for the tool and for individual arguments. Some notes on that:

1. The signature parsing is done via the `inspect` module. We use type annotations to understand the types for the arguments, and dynamically build a Pydantic model to represent the overall schema. It supports most types, including Python primitives, Pydantic models, TypedDicts, and more.
2. We use `griffe` to parse docstrings. Supported docstring formats are `google`, `sphinx` and `numpy`. We attempt to automatically detect the docstring format, but this is best-effort and you can explicitly set it when calling `function_tool`. You can also disable docstring parsing by setting `use_docstring_info` to `False`.

The code for the schema extraction lives in [`agents.function_schema`][].

## Agents as tools

In some workflows, you may want a central agent to orchestrate a network of specialized agents, instead of handing off control. You can do this by modeling agents as tools.

```python
from agents import Agent, Runner
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You translate the user's message to Spanish",
)

french_agent = Agent(
    name="French agent",
    instructions="You translate the user's message to French",
)

orchestrator_agent = Agent(
    name="orchestrator_agent",
    instructions=(
        "You are a translation agent. You use the tools given to you to translate."
        "If asked for multiple translations, you call the relevant tools."
    ),
    tools=[
        spanish_agent.as_tool(
            tool_name="translate_to_spanish",
            tool_description="Translate the user's message to Spanish",
        ),
        french_agent.as_tool(
            tool_name="translate_to_french",
            tool_description="Translate the user's message to French",
        ),
    ],
)

async def main():
    result = await Runner.run(orchestrator_agent, input="Say 'Hello, how are you?' in Spanish.")
    print(result.final_output)
```

### Customizing tool-agents

The `agent.as_tool` function is a convenience method to make it easy to turn an agent into a tool. It doesn't support all configuration though; for example, you can't set `max_turns`. For advanced use cases, use `Runner.run` directly in your tool implementation:

```python
@function_tool
async def run_my_agent() -> str:
    """A tool that runs the agent with custom configs"""

    agent = Agent(name="My agent", instructions="...")

    result = await Runner.run(
        agent,
        input="...",
        max_turns=5,
        run_config=...
    )

    return str(result.final_output)
```

### Custom output extraction

In certain cases, you might want to modify the output of the tool-agents before returning it to the central agent. This may be useful if you want to:

-   Extract a specific piece of information (e.g., a JSON payload) from the sub-agent's chat history.
-   Convert or reformat the agentâ€™s final answer (e.g., transform Markdown into plain text or CSV).
-   Validate the output or provide a fallback value when the agentâ€™s response is missing or malformed.

You can do this by supplying the `custom_output_extractor` argument to the `as_tool` method:

```python
async def extract_json_payload(run_result: RunResult) -> str:
    # Scan the agentâ€™s outputs in reverse order until we find a JSON-like message from a tool call.
    for item in reversed(run_result.new_items):
        if isinstance(item, ToolCallOutputItem) and item.output.strip().startswith("{"):
            return item.output.strip()
    # Fallback to an empty JSON object if nothing was found
    return "{}"


json_tool = data_agent.as_tool(
    tool_name="get_data_json",
    tool_description="Run the data agent and return only its JSON payload",
    custom_output_extractor=extract_json_payload,
)
```

### Conditional tool enabling

You can conditionally enable or disable agent tools at runtime using the `is_enabled` parameter. This allows you to dynamically filter which tools are available to the LLM based on context, user preferences, or runtime conditions.

```python
import asyncio
from agents import Agent, AgentBase, Runner, RunContextWrapper
from pydantic import BaseModel

class LanguageContext(BaseModel):
    language_preference: str = "french_spanish"

def french_enabled(ctx: RunContextWrapper[LanguageContext], agent: AgentBase) -> bool:
    """Enable French for French+Spanish preference."""
    return ctx.context.language_preference == "french_spanish"

# Create specialized agents
spanish_agent = Agent(
    name="spanish_agent",
    instructions="You respond in Spanish. Always reply to the user's question in Spanish.",
)

french_agent = Agent(
    name="french_agent",
    instructions="You respond in French. Always reply to the user's question in French.",
)

# Create orchestrator with conditional tools
orchestrator = Agent(
    name="orchestrator",
    instructions=(
        "You are a multilingual assistant. You use the tools given to you to respond to users. "
        "You must call ALL available tools to provide responses in different languages. "
        "You never respond in languages yourself, you always use the provided tools."
    ),
    tools=[
        spanish_agent.as_tool(
            tool_name="respond_spanish",
            tool_description="Respond to the user's question in Spanish",
            is_enabled=True,  # Always enabled
        ),
        french_agent.as_tool(
            tool_name="respond_french",
            tool_description="Respond to the user's question in French",
            is_enabled=french_enabled,
        ),
    ],
)

async def main():
    context = RunContextWrapper(LanguageContext(language_preference="french_spanish"))
    result = await Runner.run(orchestrator, "How are you?", context=context.context)
    print(result.final_output)

asyncio.run(main())
```

The `is_enabled` parameter accepts:

-   **Boolean values**: `True` (always enabled) or `False` (always disabled)
-   **Callable functions**: Functions that take `(context, agent)` and return a boolean
-   **Async functions**: Async functions for complex conditional logic

Disabled tools are completely hidden from the LLM at runtime, making this useful for:

-   Feature gating based on user permissions
-   Environment-specific tool availability (dev vs prod)
-   A/B testing different tool configurations
-   Dynamic tool filtering based on runtime state

## Handling errors in function tools

When you create a function tool via `@function_tool`, you can pass a `failure_error_function`. This is a function that provides an error response to the LLM in case the tool call crashes.

-   By default (i.e. if you don't pass anything), it runs a `default_tool_error_function` which tells the LLM an error occurred.
-   If you pass your own error function, it runs that instead, and sends the response to the LLM.
-   If you explicitly pass `None`, then any tool call errors will be re-raised for you to handle. This could be a `ModelBehaviorError` if the model produced invalid JSON, or a `UserError` if your code crashed, etc.

```python
from agents import function_tool, RunContextWrapper
from typing import Any

def my_custom_error_function(context: RunContextWrapper[Any], error: Exception) -> str:
    """A custom function to provide a user-friendly error message."""
    print(f"A tool call failed with the following error: {error}")
    return "An internal server error occurred. Please try again later."

@function_tool(failure_error_function=my_custom_error_function)
def get_user_profile(user_id: str) -> str:
    """Fetches a user profile from a mock API.
     This function demonstrates a 'flaky' or failing API call.
    """
    if user_id == "user_123":
        return "User profile for user_123 successfully retrieved."
    else:
        raise ValueError(f"Could not retrieve profile for user_id: {user_id}. API returned an error.")

```

If you are manually creating a `FunctionTool` object, then you must handle errors inside the `on_invoke_tool` function.

================
File: docs/tracing.md
================
# Tracing

The Agents SDK includes built-in tracing, collecting a comprehensive record of events during an agent run: LLM generations, tool calls, handoffs, guardrails, and even custom events that occur. Using the [Traces dashboard](https://platform.openai.com/traces), you can debug, visualize, and monitor your workflows during development and in production.

!!!note

    Tracing is enabled by default. There are two ways to disable tracing:

    1. You can globally disable tracing by setting the env var `OPENAI_AGENTS_DISABLE_TRACING=1`
    2. You can disable tracing for a single run by setting [`agents.run.RunConfig.tracing_disabled`][] to `True`

***For organizations operating under a Zero Data Retention (ZDR) policy using OpenAI's APIs, tracing is unavailable.***

## Traces and spans

-   **Traces** represent a single end-to-end operation of a "workflow". They're composed of Spans. Traces have the following properties:
    -   `workflow_name`: This is the logical workflow or app. For example "Code generation" or "Customer service".
    -   `trace_id`: A unique ID for the trace. Automatically generated if you don't pass one. Must have the format `trace_<32_alphanumeric>`.
    -   `group_id`: Optional group ID, to link multiple traces from the same conversation. For example, you might use a chat thread ID.
    -   `disabled`: If True, the trace will not be recorded.
    -   `metadata`: Optional metadata for the trace.
-   **Spans** represent operations that have a start and end time. Spans have:
    -   `started_at` and `ended_at` timestamps.
    -   `trace_id`, to represent the trace they belong to
    -   `parent_id`, which points to the parent Span of this Span (if any)
    -   `span_data`, which is information about the Span. For example, `AgentSpanData` contains information about the Agent, `GenerationSpanData` contains information about the LLM generation, etc.

## Default tracing

By default, the SDK traces the following:

-   The entire `Runner.{run, run_sync, run_streamed}()` is wrapped in a `trace()`.
-   Each time an agent runs, it is wrapped in `agent_span()`
-   LLM generations are wrapped in `generation_span()`
-   Function tool calls are each wrapped in `function_span()`
-   Guardrails are wrapped in `guardrail_span()`
-   Handoffs are wrapped in `handoff_span()`
-   Audio inputs (speech-to-text) are wrapped in a `transcription_span()`
-   Audio outputs (text-to-speech) are wrapped in a `speech_span()`
-   Related audio spans may be parented under a `speech_group_span()`

By default, the trace is named "Agent workflow". You can set this name if you use `trace`, or you can configure the name and other properties with the [`RunConfig`][agents.run.RunConfig].

In addition, you can set up [custom trace processors](#custom-tracing-processors) to push traces to other destinations (as a replacement, or secondary destination).

## Higher level traces

Sometimes, you might want multiple calls to `run()` to be part of a single trace. You can do this by wrapping the entire code in a `trace()`.

```python
from agents import Agent, Runner, trace

async def main():
    agent = Agent(name="Joke generator", instructions="Tell funny jokes.")

    with trace("Joke workflow"): # (1)!
        first_result = await Runner.run(agent, "Tell me a joke")
        second_result = await Runner.run(agent, f"Rate this joke: {first_result.final_output}")
        print(f"Joke: {first_result.final_output}")
        print(f"Rating: {second_result.final_output}")
```

1. Because the two calls to `Runner.run` are wrapped in a `with trace()`, the individual runs will be part of the overall trace rather than creating two traces.

## Creating traces

You can use the [`trace()`][agents.tracing.trace] function to create a trace. Traces need to be started and finished. You have two options to do so:

1. **Recommended**: use the trace as a context manager, i.e. `with trace(...) as my_trace`. This will automatically start and end the trace at the right time.
2. You can also manually call [`trace.start()`][agents.tracing.Trace.start] and [`trace.finish()`][agents.tracing.Trace.finish].

The current trace is tracked via a Python [`contextvar`](https://docs.python.org/3/library/contextvars.html). This means that it works with concurrency automatically. If you manually start/end a trace, you'll need to pass `mark_as_current` and `reset_current` to `start()`/`finish()` to update the current trace.

## Creating spans

You can use the various [`*_span()`][agents.tracing.create] methods to create a span. In general, you don't need to manually create spans. A [`custom_span()`][agents.tracing.custom_span] function is available for tracking custom span information.

Spans are automatically part of the current trace, and are nested under the nearest current span, which is tracked via a Python [`contextvar`](https://docs.python.org/3/library/contextvars.html).

## Sensitive data

Certain spans may capture potentially sensitive data.

The `generation_span()` stores the inputs/outputs of the LLM generation, and `function_span()` stores the inputs/outputs of function calls. These may contain sensitive data, so you can disable capturing that data via [`RunConfig.trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data].

Similarly, Audio spans include base64-encoded PCM data for input and output audio by default. You can disable capturing this audio data by configuring [`VoicePipelineConfig.trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data].

## Custom tracing processors

The high level architecture for tracing is:

-   At initialization, we create a global [`TraceProvider`][agents.tracing.setup.TraceProvider], which is responsible for creating traces.
-   We configure the `TraceProvider` with a [`BatchTraceProcessor`][agents.tracing.processors.BatchTraceProcessor] that sends traces/spans in batches to a [`BackendSpanExporter`][agents.tracing.processors.BackendSpanExporter], which exports the spans and traces to the OpenAI backend in batches.

To customize this default setup, to send traces to alternative or additional backends or modifying exporter behavior, you have two options:

1. [`add_trace_processor()`][agents.tracing.add_trace_processor] lets you add an **additional** trace processor that will receive traces and spans as they are ready. This lets you do your own processing in addition to sending traces to OpenAI's backend.
2. [`set_trace_processors()`][agents.tracing.set_trace_processors] lets you **replace** the default processors with your own trace processors. This means traces will not be sent to the OpenAI backend unless you include a `TracingProcessor` that does so.


## Tracing with Non-OpenAI Models

You can use an OpenAI API key with non-OpenAI Models to enable free tracing in the OpenAI Traces dashboard without needing to disable tracing.

```python
import os
from agents import set_tracing_export_api_key, Agent, Runner
from agents.extensions.models.litellm_model import LitellmModel

tracing_api_key = os.environ["OPENAI_API_KEY"]
set_tracing_export_api_key(tracing_api_key)

model = LitellmModel(
    model="your-model-name",
    api_key="your-api-key",
)

agent = Agent(
    name="Assistant",
    model=model,
)
```

## Notes
- View free traces at Openai Traces dashboard.


## External tracing processors list

-   [Weights & Biases](https://weave-docs.wandb.ai/guides/integrations/openai_agents)
-   [Arize-Phoenix](https://docs.arize.com/phoenix/tracing/integrations-tracing/openai-agents-sdk)
-   [Future AGI](https://docs.futureagi.com/future-agi/products/observability/auto-instrumentation/openai_agents)
-   [MLflow (self-hosted/OSS)](https://mlflow.org/docs/latest/tracing/integrations/openai-agent)
-   [MLflow (Databricks hosted)](https://docs.databricks.com/aws/en/mlflow/mlflow-tracing#-automatic-tracing)
-   [Braintrust](https://braintrust.dev/docs/guides/traces/integrations#openai-agents-sdk)
-   [Pydantic Logfire](https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents)
-   [AgentOps](https://docs.agentops.ai/v1/integrations/agentssdk)
-   [Scorecard](https://docs.scorecard.io/docs/documentation/features/tracing#openai-agents-sdk-integration)
-   [Keywords AI](https://docs.keywordsai.co/integration/development-frameworks/openai-agent)
-   [LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
-   [Maxim AI](https://www.getmaxim.ai/docs/observe/integrations/openai-agents-sdk)
-   [Comet Opik](https://www.comet.com/docs/opik/tracing/integrations/openai_agents)
-   [Langfuse](https://langfuse.com/docs/integrations/openaiagentssdk/openai-agents)
-   [Langtrace](https://docs.langtrace.ai/supported-integrations/llm-frameworks/openai-agents-sdk)
-   [Okahu-Monocle](https://github.com/monocle2ai/monocle)
-   [Galileo](https://v2docs.galileo.ai/integrations/openai-agent-integration#openai-agent-integration)
-   [Portkey AI](https://portkey.ai/docs/integrations/agents/openai-agents)
-   [LangDB AI](https://docs.langdb.ai/getting-started/working-with-agent-frameworks/working-with-openai-agents-sdk)
-   [Agenta](https://docs.agenta.ai/observability/integrations/openai-agents)

================
File: docs/usage.md
================
# Usage

The Agents SDK automatically tracks token usage for every run. You can access it from the run context and use it to monitor costs, enforce limits, or record analytics.

## What is tracked

- **requests**: number of LLM API calls made
- **input_tokens**: total input tokens sent
- **output_tokens**: total output tokens received
- **total_tokens**: input + output
- **details**:
  - `input_tokens_details.cached_tokens`
  - `output_tokens_details.reasoning_tokens`

## Accessing usage from a run

After `Runner.run(...)`, access usage via `result.context_wrapper.usage`.

```python
result = await Runner.run(agent, "What's the weather in Tokyo?")
usage = result.context_wrapper.usage

print("Requests:", usage.requests)
print("Input tokens:", usage.input_tokens)
print("Output tokens:", usage.output_tokens)
print("Total tokens:", usage.total_tokens)
```

Usage is aggregated across all model calls during the run (including tool calls and handoffs).

### Enabling usage with LiteLLM models

LiteLLM providers do not report usage metrics by default. When you are using [`LitellmModel`](models/litellm.md), pass `ModelSettings(include_usage=True)` to your agent so that LiteLLM responses populate `result.context_wrapper.usage`.

```python
from agents import Agent, ModelSettings, Runner
from agents.extensions.models.litellm_model import LitellmModel

agent = Agent(
    name="Assistant",
    model=LitellmModel(model="your/model", api_key="..."),
    model_settings=ModelSettings(include_usage=True),
)

result = await Runner.run(agent, "What's the weather in Tokyo?")
print(result.context_wrapper.usage.total_tokens)
```

## Accessing usage with sessions

When you use a `Session` (e.g., `SQLiteSession`), each call to `Runner.run(...)` returns usage for that specific run. Sessions maintain conversation history for context, but each run's usage is independent.

```python
session = SQLiteSession("my_conversation")

first = await Runner.run(agent, "Hi!", session=session)
print(first.context_wrapper.usage.total_tokens)  # Usage for first run

second = await Runner.run(agent, "Can you elaborate?", session=session)
print(second.context_wrapper.usage.total_tokens)  # Usage for second run
```

Note that while sessions preserve conversation context between runs, the usage metrics returned by each `Runner.run()` call represent only that particular execution. In sessions, previous messages may be re-fed as input to each run, which affects the input token count in consequent turns.

## Using usage in hooks

If you're using `RunHooks`, the `context` object passed to each hook contains `usage`. This lets you log usage at key lifecycle moments.

```python
class MyHooks(RunHooks):
    async def on_agent_end(self, context: RunContextWrapper, agent: Agent, output: Any) -> None:
        u = context.usage
        print(f"{agent.name} â†’ {u.requests} requests, {u.total_tokens} total tokens")
```

## API Reference

For detailed API documentation, see:

-   [`Usage`][agents.usage.Usage] - Usage tracking data structure
-   [`RunContextWrapper`][agents.run.RunContextWrapper] - Access usage from run context
-   [`RunHooks`][agents.run.RunHooks] - Hook into usage tracking lifecycle

================
File: docs/visualization.md
================
# Agent Visualization

Agent visualization allows you to generate a structured graphical representation of agents and their relationships using **Graphviz**. This is useful for understanding how agents, tools, and handoffs interact within an application.

## Installation

Install the optional `viz` dependency group:

```bash
pip install "openai-agents[viz]"
```

## Generating a Graph

You can generate an agent visualization using the `draw_graph` function. This function creates a directed graph where:

- **Agents** are represented as yellow boxes.
- **MCP Servers** are represented as grey boxes.
- **Tools** are represented as green ellipses.
- **Handoffs** are directed edges from one agent to another.

### Example Usage

```python
import os

from agents import Agent, function_tool
from agents.mcp.server import MCPServerStdio
from agents.extensions.visualization import draw_graph

@function_tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
)

current_dir = os.path.dirname(os.path.abspath(__file__))
samples_dir = os.path.join(current_dir, "sample_files")
mcp_server = MCPServerStdio(
    name="Filesystem Server, via npx",
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", samples_dir],
    },
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    tools=[get_weather],
    mcp_servers=[mcp_server],
)

draw_graph(triage_agent)
```

![Agent Graph](./assets/images/graph.png)

This generates a graph that visually represents the structure of the **triage agent** and its connections to sub-agents and tools.


## Understanding the Visualization

The generated graph includes:

- A **start node** (`__start__`) indicating the entry point.
- Agents represented as **rectangles** with yellow fill.
- Tools represented as **ellipses** with green fill.
- MCP Servers represented as **rectangles** with grey fill.
- Directed edges indicating interactions:
  - **Solid arrows** for agent-to-agent handoffs.
  - **Dotted arrows** for tool invocations.
  - **Dashed arrows** for MCP server invocations.
- An **end node** (`__end__`) indicating where execution terminates.

**Note:** MCP servers are rendered in recent versions of the
`agents` package (verified in **v0.2.8**). If you donâ€™t see MCP boxes
in your visualization, upgrade to the latest release.

## Customizing the Graph

### Showing the Graph
By default, `draw_graph` displays the graph inline. To show the graph in a separate window, write the following:

```python
draw_graph(triage_agent).view()
```

### Saving the Graph
By default, `draw_graph` displays the graph inline. To save it as a file, specify a filename:

```python
draw_graph(triage_agent, filename="agent_graph")
```

This will generate `agent_graph.png` in the working directory.




================================================================
End of Codebase
================================================================
